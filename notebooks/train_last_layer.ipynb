{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "43700d4d",
   "metadata": {},
   "source": [
    "# Notebook to train last layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "afdf04e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import zipfile\n",
    "import tarfile\n",
    "import urllib\n",
    "import requests\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import h5py as h5\n",
    "import argparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5c813369",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import awkward as ak\n",
    "import uproot\n",
    "import vector\n",
    "vector.register_awkward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ec0649fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c1e2d6a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/billyli/miniforge_x86_new/envs/weaver/lib/python3.7/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8bb1463f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PosixPath('/Users/billyli/scope/JetClass/minimal/HToCC_120.root'),\n",
       " PosixPath('/Users/billyli/scope/JetClass/minimal/WToQQ_120.root'),\n",
       " PosixPath('/Users/billyli/scope/JetClass/minimal/ZToQQ_120.root'),\n",
       " PosixPath('/Users/billyli/scope/JetClass/minimal/HToBB_120.root'),\n",
       " PosixPath('/Users/billyli/scope/JetClass/minimal/HToGG_120.root'),\n",
       " PosixPath('/Users/billyli/scope/JetClass/minimal/HToWW2Q1L_120.root'),\n",
       " PosixPath('/Users/billyli/scope/JetClass/minimal/TTBarLep_120.root'),\n",
       " PosixPath('/Users/billyli/scope/JetClass/minimal/TTBar_120.root'),\n",
       " PosixPath('/Users/billyli/scope/JetClass/minimal/ZJetsToNuNu_120.root'),\n",
       " PosixPath('/Users/billyli/scope/JetClass/minimal/HToWW4Q_120.root')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "root_dir = Path(\"/Users/billyli/scope/JetClass/minimal\")\n",
    "list(root_dir.glob('*.root'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "61b64e01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['HToCC', '120.root']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f = list(root_dir.glob('*.root'))[0]\n",
    "f.name.split('_')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b035971",
   "metadata": {},
   "source": [
    "## Get Jet label from Root Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "09836d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_size = 1e6\n",
    "data_fraction = 3e-2\n",
    "val_fraction = 1e-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c8e68549",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_to_numpy(tree, feature):\n",
    "    return tree[feature].arrays().to_numpy().astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1357ec6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "jet_classes = []\n",
    "for f in root_dir.glob('*.root'):\n",
    "    process = f.name.split('_')[0]\n",
    "    tree = uproot.open(f)['tree']\n",
    "\n",
    "    n_entries = tree.num_entries\n",
    "    n_keep = int(n_entries * data_fraction)\n",
    "\n",
    "    jet_label_f = tree.arrays(\n",
    "        filter_name=['label_*'],\n",
    "        entry_stop=n_keep,  # only read first fraction\n",
    "    )\n",
    "\n",
    "    label_list = ['label_QCD', 'label_Hbb', 'label_Hcc', 'label_Hgg', 'label_H4q', 'label_Hqql', 'label_Zqq', 'label_Wqq', 'label_Tbqq', 'label_Tbl']\n",
    "    jet_class = np.stack([jet_label_f[n].to_numpy().astype('int') for n in label_list], axis=1)\n",
    "    jet_classes.append(jet_class)\n",
    "y = np.concatenate(jet_classes, axis=0)\n",
    "y_tensor = torch.tensor(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52a888e5",
   "metadata": {},
   "source": [
    "## Load RAV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ea520fbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 1])\n"
     ]
    }
   ],
   "source": [
    "rav = h5.File('/Users/billyli/scope/particle_transformer/notebooks/rav_sdmass_frac_3e-2.h5', 'r')[\"RAV_jet_sdmass\"][:]\n",
    "rav_tensor = torch.tensor(rav, dtype=torch.float32)\n",
    "print(rav_tensor.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e8e5e2f",
   "metadata": {},
   "source": [
    "## Find the hidden state's orthogonal component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6c2e4af7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([30000, 1, 128])\n"
     ]
    }
   ],
   "source": [
    "hidden = h5.File(\"/Users/billyli/scope/weaver-core/frac_3e-2.h5\", 'r')[\"hidden_11\"][:]\n",
    "hidden_tensor = torch.tensor(hidden, dtype=torch.float32)\n",
    "print(hidden_tensor.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "429660f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def perpendicular_component(hidden_tensor, raw_tensor, eps=1e-12):\n",
    "    \"\"\"\n",
    "    hidden_tensor: (30000, 128, 1)  vectors v\n",
    "    raw_tensor:    either (30000, 128, 1) per-sample u\n",
    "                   or     (128, 1) / (128,) shared u\n",
    "    returns:       (30000, 128, 1)  v_perp\n",
    "    \"\"\"\n",
    "    v = hidden_tensor.squeeze(-1)  # (30000, 128)\n",
    "    u = raw_tensor.squeeze(-1)     # (30000,128) or (128,)\n",
    "\n",
    "    # Broadcast u to match v if needed\n",
    "    if u.dim() == 1:\n",
    "        u = u.unsqueeze(0).expand_as(v)  # (1,128) -> (30000,128)\n",
    "    elif u.shape != v.shape:\n",
    "        u = u.expand_as(v)\n",
    "\n",
    "    # Compute projection scalar coeff alpha = (v·u)/(u·u)\n",
    "    alpha = (v * u).sum(dim=-1, keepdim=True) / ((u * u).sum(dim=-1, keepdim=True) + eps)  # (30000,1)\n",
    "\n",
    "    proj = alpha * u             # (30000,128)\n",
    "    v_perp = v - proj            # (30000,128)\n",
    "    return v_perp.permute(0, 2, 1)  # (30000,128,1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "67da95b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "hidden_perp = perpendicular_component(hidden_tensor, rav_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3db5bb90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(hidden_perp.squeeze(-1) @ rav_tensor.squeeze(-1) > 1e-4).sum() # should be close to 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bcffced",
   "metadata": {},
   "source": [
    "## Define the last layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "89decf41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class PerpClassifier(nn.Module):\n",
    "    def __init__(self, in_dim=128, num_classes=10, for_inference=False):\n",
    "        super().__init__()\n",
    "        self.norm = nn.LayerNorm(in_dim)\n",
    "        self.fc = nn.Linear(in_dim, num_classes)\n",
    "        self.for_inference = for_inference\n",
    "\n",
    "    def forward(self, cls_tokens, rav_tensor=None, return_hidden=False):\n",
    "        \"\"\"\n",
    "        cls_tokens: (N, C) or (1, N, C) like your snippet\n",
    "        raw_tensor: (N, C) or (C,) direction(s) to project out\n",
    "        \"\"\"\n",
    "        # Match your snippet's first lines\n",
    "        if cls_tokens.dim() == 3:\n",
    "            # (1, N, C) -> (N, C)\n",
    "            x_cls = self.norm(cls_tokens).squeeze(0)\n",
    "        else:\n",
    "            x_cls = self.norm(cls_tokens)\n",
    "\n",
    "        # Keep track if you want\n",
    "        hiddens = [x_cls.unsqueeze(0)]\n",
    "\n",
    "        # Use perpendicular component if provided\n",
    "        if rav_tensor is not None:\n",
    "            x_used = perpendicular_component(x_cls, rav_tensor)   # (N, C)\n",
    "        else:\n",
    "            x_used = x_cls\n",
    "        \n",
    "        x_used = x_used.squeeze(-1)\n",
    "        logits = self.fc(x_used)                         # (N, 10)\n",
    "\n",
    "        if self.for_inference:\n",
    "            probs = F.softmax(logits, dim=1)\n",
    "            return (probs, hiddens) if return_hidden else probs\n",
    "        else:\n",
    "            return (logits, hiddens) if return_hidden else logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "689d4c6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PerpClassifier(\n",
       "  (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "  (fc): Linear(in_features=128, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = PerpClassifier(in_dim=128, num_classes=10, for_inference=False)\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6867b23",
   "metadata": {},
   "source": [
    "## Import Weaver modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ecd2074d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from weaver.utils.dataset import SimpleIterDataset\n",
    "from weaver.train import train_load, model_setup, optim\n",
    "from weaver.utils.logger import _logger, _configLogger"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "656e7bc8",
   "metadata": {},
   "source": [
    "## Mimic Args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "64528f96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_StoreTrueAction(option_strings=['--hidden-states'], dest='hidden_states', nargs=0, const=True, default=False, type=None, choices=None, help='let ParT output hidden states with the logits', metavar=None)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--regression-mode', action='store_true', default=False,\n",
    "                    help='run in regression mode if this flag is set; otherwise run in classification mode')\n",
    "parser.add_argument('-c', '--data-config', type=str,\n",
    "                    help='data config YAML file')\n",
    "parser.add_argument('--extra-selection', type=str, default=None,\n",
    "                    help='Additional selection requirement, will modify `selection` to `(selection) & (extra)` on-the-fly')\n",
    "parser.add_argument('--extra-test-selection', type=str, default=None,\n",
    "                    help='Additional test-time selection requirement, will modify `test_time_selection` to `(test_time_selection) & (extra)` on-the-fly')\n",
    "parser.add_argument('-i', '--data-train', nargs='*', default=[],\n",
    "                    help='training files; supported syntax:'\n",
    "                         ' (a) plain list, `--data-train /path/to/a/* /path/to/b/*`;'\n",
    "                         ' (b) (named) groups [Recommended], `--data-train a:/path/to/a/* b:/path/to/b/*`,'\n",
    "                         ' the file splitting (for each dataloader worker) will be performed per group,'\n",
    "                         ' and then mixed together, to ensure a uniform mixing from all groups for each worker.'\n",
    "                    )\n",
    "parser.add_argument('-l', '--data-val', nargs='*', default=[],\n",
    "                    help='validation files; when not set, will use training files and split by `--train-val-split`')\n",
    "parser.add_argument('-t', '--data-test', nargs='*', default=[],\n",
    "                    help='testing files; supported syntax:'\n",
    "                         ' (a) plain list, `--data-test /path/to/a/* /path/to/b/*`;'\n",
    "                         ' (b) keyword-based, `--data-test a:/path/to/a/* b:/path/to/b/*`, will produce output_a, output_b;'\n",
    "                         ' (c) split output per N input files, `--data-test a%%10:/path/to/a/*`, will split per 10 input files')\n",
    "parser.add_argument('--data-fraction', type=float, default=1,\n",
    "                    help='fraction of events to load from each file; for training, the events are randomly selected for each epoch')\n",
    "parser.add_argument('--file-fraction', type=float, default=1,\n",
    "                    help='fraction of files to load; for training, the files are randomly selected for each epoch')\n",
    "parser.add_argument('--fetch-by-files', action='store_true', default=False,\n",
    "                    help='When enabled, will load all events from a small number (set by ``--fetch-step``) of files for each data fetching. '\n",
    "                         'Otherwise (default), load a small fraction of events from all files each time, which helps reduce variations in the sample composition.')\n",
    "parser.add_argument('--fetch-step', type=float, default=0.01,\n",
    "                    help='fraction of events to load each time from every file (when ``--fetch-by-files`` is disabled); '\n",
    "                         'Or: number of files to load each time (when ``--fetch-by-files`` is enabled). Shuffling & sampling is done within these events, so set a large enough value.')\n",
    "parser.add_argument('--in-memory', action='store_true', default=False,\n",
    "                    help='load the whole dataset (and perform the preprocessing) only once and keep it in memory for the entire run')\n",
    "parser.add_argument('--train-val-split', type=float, default=0.8,\n",
    "                    help='training/validation split fraction')\n",
    "parser.add_argument('--no-remake-weights', action='store_true', default=False,\n",
    "                    help='do not remake weights for sampling (reweighting), use existing ones in the previous auto-generated data config YAML file')\n",
    "parser.add_argument('--demo', action='store_true', default=False,\n",
    "                    help='quickly test the setup by running over only a small number of events')\n",
    "parser.add_argument('--lr-finder', type=str, default=None,\n",
    "                    help='run learning rate finder instead of the actual training; format: ``start_lr, end_lr, num_iters``')\n",
    "parser.add_argument('--tensorboard', type=str, default=None,\n",
    "                    help='create a tensorboard summary writer with the given comment')\n",
    "parser.add_argument('--tensorboard-custom-fn', type=str, default=None,\n",
    "                    help='the path of the python script containing a user-specified function `get_tensorboard_custom_fn`, '\n",
    "                         'to display custom information per mini-batch or per epoch, during the training, validation or test.')\n",
    "parser.add_argument('-n', '--network-config', type=str,\n",
    "                    help='network architecture configuration file; the path must be relative to the current dir')\n",
    "parser.add_argument('-o', '--network-option', nargs=2, action='append', default=[],\n",
    "                    help='options to pass to the model class constructor, e.g., `--network-option use_counts False`')\n",
    "parser.add_argument('-m', '--model-prefix', type=str, default='models/{auto}/network',\n",
    "                    help='path to save or load the model; for training, this will be used as a prefix, so model snapshots '\n",
    "                         'will saved to `{model_prefix}_epoch-%%d_state.pt` after each epoch, and the one with the best '\n",
    "                         'validation metric to `{model_prefix}_best_epoch_state.pt`; for testing, this should be the full path '\n",
    "                         'including the suffix, otherwise the one with the best validation metric will be used; '\n",
    "                         'for training, `{auto}` can be used as part of the path to auto-generate a name, '\n",
    "                         'based on the timestamp and network configuration')\n",
    "parser.add_argument('--load-model-weights', type=str, default=None,\n",
    "                    help='initialize model with pre-trained weights')\n",
    "parser.add_argument('--exclude-model-weights', type=str, default=None,\n",
    "                    help='comma-separated regex to exclude matched weights from being loaded, e.g., `a.fc..+,b.fc..+`')\n",
    "parser.add_argument('--freeze-model-weights', type=str, default=None,\n",
    "                    help='comma-separated regex to freeze matched weights from being updated in the training, e.g., `a.fc..+,b.fc..+`')\n",
    "parser.add_argument('--num-epochs', type=int, default=20,\n",
    "                    help='number of epochs')\n",
    "parser.add_argument('--steps-per-epoch', type=int, default=None,\n",
    "                    help='number of steps (iterations) per epochs; '\n",
    "                         'if neither of `--steps-per-epoch` or `--samples-per-epoch` is set, each epoch will run over all loaded samples')\n",
    "parser.add_argument('--steps-per-epoch-val', type=int, default=None,\n",
    "                    help='number of steps (iterations) per epochs for validation; '\n",
    "                         'if neither of `--steps-per-epoch-val` or `--samples-per-epoch-val` is set, each epoch will run over all loaded samples')\n",
    "parser.add_argument('--samples-per-epoch', type=int, default=None,\n",
    "                    help='number of samples per epochs; '\n",
    "                         'if neither of `--steps-per-epoch` or `--samples-per-epoch` is set, each epoch will run over all loaded samples')\n",
    "parser.add_argument('--samples-per-epoch-val', type=int, default=None,\n",
    "                    help='number of samples per epochs for validation; '\n",
    "                         'if neither of `--steps-per-epoch-val` or `--samples-per-epoch-val` is set, each epoch will run over all loaded samples')\n",
    "parser.add_argument('--optimizer', type=str, default='ranger', choices=['adam', 'adamW', 'radam', 'ranger'],  # TODO: add more\n",
    "                    help='optimizer for the training')\n",
    "parser.add_argument('--optimizer-option', nargs=2, action='append', default=[],\n",
    "                    help='options to pass to the optimizer class constructor, e.g., `--optimizer-option weight_decay 1e-4`')\n",
    "parser.add_argument('--lr-scheduler', type=str, default='flat+decay',\n",
    "                    choices=['none', 'steps', 'flat+decay', 'flat+linear', 'flat+cos', 'one-cycle'],\n",
    "                    help='learning rate scheduler')\n",
    "parser.add_argument('--warmup-steps', type=int, default=0,\n",
    "                    help='number of warm-up steps, only valid for `flat+linear` and `flat+cos` lr schedulers')\n",
    "parser.add_argument('--load-epoch', type=int, default=None,\n",
    "                    help='used to resume interrupted training, load model and optimizer state saved in the `epoch-%%d_state.pt` and `epoch-%%d_optimizer.pt` files')\n",
    "parser.add_argument('--start-lr', type=float, default=5e-3,\n",
    "                    help='start learning rate')\n",
    "parser.add_argument('--batch-size', type=int, default=128,\n",
    "                    help='batch size')\n",
    "parser.add_argument('--use-amp', action='store_true', default=False,\n",
    "                    help='use mixed precision training (fp16)')\n",
    "parser.add_argument('--gpus', type=str, default='0',\n",
    "                    help='device for the training/testing; to use CPU, set to empty string (\"\"); to use multiple gpu, set it as a comma separated list, e.g., `1,2,3,4`')\n",
    "parser.add_argument('--predict-gpus', type=str, default=None,\n",
    "                    help='device for the testing; to use CPU, set to empty string (\"\"); to use multiple gpu, set it as a comma separated list, e.g., `1,2,3,4`; if not set, use the same as `--gpus`')\n",
    "parser.add_argument('--num-workers', type=int, default=1,\n",
    "                    help='number of threads to load the dataset; memory consumption and disk access load increases (~linearly) with this numbers')\n",
    "parser.add_argument('--predict', action='store_true', default=False,\n",
    "                    help='run prediction instead of training')\n",
    "parser.add_argument('--predict-output', type=str,\n",
    "                    help='path to save the prediction output, support `.root` and `.parquet` format')\n",
    "parser.add_argument('--export-onnx', type=str, default=None,\n",
    "                    help='export the PyTorch model to ONNX model and save it at the given path (path must ends w/ .onnx); '\n",
    "                         'needs to set `--data-config`, `--network-config`, and `--model-prefix` (requires the full model path)')\n",
    "parser.add_argument('--onnx-opset', type=int, default=15,\n",
    "                    help='ONNX opset version.')\n",
    "parser.add_argument('--io-test', action='store_true', default=False,\n",
    "                    help='test throughput of the dataloader')\n",
    "parser.add_argument('--copy-inputs', action='store_true', default=False,\n",
    "                    help='copy input files to the current dir (can help to speed up dataloading when running over remote files, e.g., from EOS)')\n",
    "parser.add_argument('--log', type=str, default='',\n",
    "                    help='path to the log file; `{auto}` can be used as part of the path to auto-generate a name, based on the timestamp and network configuration')\n",
    "parser.add_argument('--print', action='store_true', default=False,\n",
    "                    help='do not run training/prediction but only print model information, e.g., FLOPs and number of parameters of a model')\n",
    "parser.add_argument('--profile', action='store_true', default=False,\n",
    "                    help='run the profiler')\n",
    "parser.add_argument('--backend', type=str, choices=['gloo', 'nccl', 'mpi'], default=None,\n",
    "                    help='backend for distributed training')\n",
    "parser.add_argument('--cross-validation', type=str, default=None,\n",
    "                    help='enable k-fold cross validation; input format: `variable_name%%k`')\n",
    "parser.add_argument('--disable-mps', action='store_true', default=False,\n",
    "                    help='disable using mps device if it does not work for you')\n",
    "parser.add_argument('--hidden-states-out', type=str, default=\"hidden_states_out.h5\",\n",
    "                    help='path to save hidden states as h5 file')\n",
    "parser.add_argument('--hidden-states', action='store_true', default=False,\n",
    "                    help='let ParT output hidden states with the logits')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "89d06666",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = parser.parse_args([\n",
    "    \"--predict\",\n",
    "    \"--data-test\", \"/Users/billyli/scope/JetClass/minimal/*.root\",\n",
    "    \"--data-config\", \"/Users/billyli/scope/particle_transformer/data/JetClass/JetClass_full.yaml\",\n",
    "    \"--data-fraction\", f\"{data_fraction}\",\n",
    "    \"--network-config\", \"/Users/billyli/scope/particle_transformer/networks/ParT_w_hidden_states.py\",\n",
    "    \"--use-amp\",\n",
    "    \"--model-prefix\", \"/Users/billyli/scope/particle_transformer/models/ParT_full.pt\",\n",
    "    \"--batch-size\", \"64\",\n",
    "    \"--predict-output\", \"/Users/billyli/scope/particle_transformer/tmp/test_output.root\",\n",
    "    \"--gpus\", \"\",\n",
    "    \"--num-workers\", \"0\",\n",
    "    \"--disable-mps\",\n",
    "    \"--hidden-states\"\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75c5a8f6",
   "metadata": {},
   "source": [
    "## Deivice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c2c8ad88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# device\n",
    "if args.gpus:\n",
    "    # distributed training\n",
    "    if args.backend is not None:\n",
    "        local_rank = args.local_rank\n",
    "        torch.cuda.set_device(local_rank)\n",
    "        gpus = [local_rank]\n",
    "        dev = torch.device(local_rank)\n",
    "        torch.distributed.init_process_group(backend=args.backend)\n",
    "        _logger.info(f'Using distributed PyTorch with {args.backend} backend')\n",
    "    else:\n",
    "        gpus = [int(i) for i in args.gpus.split(',')]\n",
    "        dev = torch.device(gpus[0])\n",
    "else:\n",
    "    gpus = None\n",
    "    dev = torch.device('cpu')\n",
    "    if not args.disable_mps:\n",
    "        try:\n",
    "            if torch.backends.mps.is_available():\n",
    "                dev = torch.device('mps')\n",
    "        except AttributeError:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1ccd79f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "args.local_rank = None if args.backend is None else int(os.environ.get(\"LOCAL_RANK\", \"0\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8e5367b",
   "metadata": {},
   "source": [
    "## Prepare Dataloader for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6129ad01",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader, random_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9f587edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: X: (N, 128), y_onehot: (N, 10)\n",
    "y_idx = y_tensor.argmax(dim=1).long()  # (N,)\n",
    "\n",
    "# Wrap into a dataset\n",
    "dataset = TensorDataset(hidden_tensor, y_idx)\n",
    "\n",
    "# Split 9:1\n",
    "N = len(dataset)\n",
    "n_train = int(0.9 * N)\n",
    "n_val   = N - n_train\n",
    "train_set, val_set = random_split(dataset, [n_train, n_val])\n",
    "\n",
    "# Build DataLoaders\n",
    "train_loader = DataLoader(train_set, batch_size=args.batch_size, shuffle=True)\n",
    "val_loader   = DataLoader(val_set,   batch_size=args.batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe79f6d4",
   "metadata": {},
   "source": [
    "## Config Optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "befe3b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39abeeca",
   "metadata": {},
   "source": [
    "## Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "27b376ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02a756dc",
   "metadata": {},
   "source": [
    "## Train the last layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ba3904f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1da4d33b",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def eval_epoch(loader, rav_tensor, device='cpu'):\n",
    "    model.eval()\n",
    "    total, correct, total_loss = 0, 0, 0.0\n",
    "    for xb, yb in loader:\n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "        logits = model(xb, rav_tensor=rav_tensor)\n",
    "        loss = criterion(logits, yb)\n",
    "        preds = logits.argmax(dim=1)\n",
    "        correct += (preds == yb).sum().item()\n",
    "        total += yb.size(0)\n",
    "        total_loss += loss.item() * yb.size(0)\n",
    "    return total_loss/total, correct/total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d13f8917",
   "metadata": {},
   "outputs": [],
   "source": [
    "eppoch = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e055adee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01 | train loss 1.9288 acc 0.3576 | val loss 1.4760 acc 0.6284\n",
      "Epoch 02 | train loss 1.1843 acc 0.7583 | val loss 0.9501 acc 0.8138\n",
      "Epoch 03 | train loss 0.8074 acc 0.8286 | val loss 0.6951 acc 0.8384\n",
      "Epoch 04 | train loss 0.6273 acc 0.8420 | val loss 0.5728 acc 0.8453\n",
      "Epoch 05 | train loss 0.5383 acc 0.8479 | val loss 0.5096 acc 0.8503\n",
      "Epoch 06 | train loss 0.4906 acc 0.8515 | val loss 0.4741 acc 0.8526\n",
      "Epoch 07 | train loss 0.4630 acc 0.8533 | val loss 0.4529 acc 0.8539\n",
      "Epoch 08 | train loss 0.4462 acc 0.8541 | val loss 0.4397 acc 0.8549\n",
      "Epoch 09 | train loss 0.4354 acc 0.8553 | val loss 0.4310 acc 0.8554\n",
      "Epoch 10 | train loss 0.4282 acc 0.8555 | val loss 0.4251 acc 0.8558\n",
      "Epoch 11 | train loss 0.4233 acc 0.8564 | val loss 0.4210 acc 0.8563\n",
      "Epoch 12 | train loss 0.4199 acc 0.8564 | val loss 0.4180 acc 0.8564\n",
      "Epoch 13 | train loss 0.4173 acc 0.8563 | val loss 0.4158 acc 0.8561\n",
      "Epoch 14 | train loss 0.4154 acc 0.8568 | val loss 0.4141 acc 0.8574\n",
      "Epoch 15 | train loss 0.4138 acc 0.8571 | val loss 0.4129 acc 0.8570\n",
      "Epoch 16 | train loss 0.4127 acc 0.8573 | val loss 0.4118 acc 0.8576\n",
      "Epoch 17 | train loss 0.4118 acc 0.8569 | val loss 0.4110 acc 0.8571\n",
      "Epoch 18 | train loss 0.4111 acc 0.8572 | val loss 0.4103 acc 0.8577\n",
      "Epoch 19 | train loss 0.4105 acc 0.8576 | val loss 0.4098 acc 0.8580\n",
      "Epoch 20 | train loss 0.4100 acc 0.8576 | val loss 0.4094 acc 0.8575\n",
      "Epoch 21 | train loss 0.4096 acc 0.8577 | val loss 0.4090 acc 0.8577\n",
      "Epoch 22 | train loss 0.4093 acc 0.8579 | val loss 0.4087 acc 0.8575\n",
      "Epoch 23 | train loss 0.4090 acc 0.8577 | val loss 0.4085 acc 0.8569\n",
      "Epoch 24 | train loss 0.4087 acc 0.8573 | val loss 0.4082 acc 0.8576\n",
      "Epoch 25 | train loss 0.4085 acc 0.8581 | val loss 0.4079 acc 0.8579\n",
      "Epoch 26 | train loss 0.4082 acc 0.8579 | val loss 0.4078 acc 0.8576\n",
      "Epoch 27 | train loss 0.4081 acc 0.8579 | val loss 0.4078 acc 0.8576\n",
      "Epoch 28 | train loss 0.4080 acc 0.8577 | val loss 0.4075 acc 0.8583\n",
      "Epoch 29 | train loss 0.4079 acc 0.8583 | val loss 0.4074 acc 0.8585\n",
      "Epoch 30 | train loss 0.4077 acc 0.8581 | val loss 0.4072 acc 0.8579\n",
      "Epoch 31 | train loss 0.4076 acc 0.8582 | val loss 0.4073 acc 0.8585\n",
      "Epoch 32 | train loss 0.4076 acc 0.8583 | val loss 0.4070 acc 0.8580\n",
      "Epoch 33 | train loss 0.4074 acc 0.8584 | val loss 0.4070 acc 0.8583\n",
      "Epoch 34 | train loss 0.4074 acc 0.8583 | val loss 0.4069 acc 0.8584\n",
      "Epoch 35 | train loss 0.4073 acc 0.8583 | val loss 0.4069 acc 0.8585\n",
      "Epoch 36 | train loss 0.4072 acc 0.8583 | val loss 0.4067 acc 0.8583\n",
      "Epoch 37 | train loss 0.4072 acc 0.8581 | val loss 0.4068 acc 0.8583\n",
      "Epoch 38 | train loss 0.4071 acc 0.8583 | val loss 0.4066 acc 0.8582\n",
      "Epoch 39 | train loss 0.4070 acc 0.8580 | val loss 0.4066 acc 0.8587\n",
      "Epoch 40 | train loss 0.4069 acc 0.8581 | val loss 0.4066 acc 0.8583\n",
      "Epoch 41 | train loss 0.4069 acc 0.8580 | val loss 0.4065 acc 0.8581\n",
      "Epoch 42 | train loss 0.4069 acc 0.8585 | val loss 0.4065 acc 0.8577\n",
      "Epoch 43 | train loss 0.4069 acc 0.8577 | val loss 0.4064 acc 0.8582\n",
      "Epoch 44 | train loss 0.4068 acc 0.8580 | val loss 0.4064 acc 0.8581\n",
      "Epoch 45 | train loss 0.4068 acc 0.8582 | val loss 0.4063 acc 0.8584\n",
      "Epoch 46 | train loss 0.4067 acc 0.8580 | val loss 0.4062 acc 0.8582\n",
      "Epoch 47 | train loss 0.4066 acc 0.8576 | val loss 0.4062 acc 0.8587\n",
      "Epoch 48 | train loss 0.4066 acc 0.8584 | val loss 0.4063 acc 0.8581\n",
      "Epoch 49 | train loss 0.4066 acc 0.8579 | val loss 0.4061 acc 0.8582\n",
      "Epoch 50 | train loss 0.4065 acc 0.8574 | val loss 0.4061 acc 0.8585\n",
      "Epoch 51 | train loss 0.4065 acc 0.8587 | val loss 0.4061 acc 0.8584\n",
      "Epoch 52 | train loss 0.4065 acc 0.8578 | val loss 0.4060 acc 0.8587\n",
      "Epoch 53 | train loss 0.4065 acc 0.8581 | val loss 0.4060 acc 0.8580\n",
      "Epoch 54 | train loss 0.4064 acc 0.8580 | val loss 0.4060 acc 0.8587\n",
      "Epoch 55 | train loss 0.4064 acc 0.8585 | val loss 0.4060 acc 0.8577\n",
      "Epoch 56 | train loss 0.4064 acc 0.8574 | val loss 0.4059 acc 0.8579\n",
      "Epoch 57 | train loss 0.4063 acc 0.8583 | val loss 0.4060 acc 0.8580\n",
      "Epoch 58 | train loss 0.4064 acc 0.8578 | val loss 0.4059 acc 0.8580\n",
      "Epoch 59 | train loss 0.4063 acc 0.8585 | val loss 0.4059 acc 0.8579\n",
      "Epoch 60 | train loss 0.4063 acc 0.8578 | val loss 0.4059 acc 0.8587\n",
      "Epoch 61 | train loss 0.4062 acc 0.8579 | val loss 0.4058 acc 0.8580\n",
      "Epoch 62 | train loss 0.4062 acc 0.8577 | val loss 0.4058 acc 0.8579\n",
      "Epoch 63 | train loss 0.4062 acc 0.8575 | val loss 0.4059 acc 0.8582\n",
      "Epoch 64 | train loss 0.4062 acc 0.8576 | val loss 0.4057 acc 0.8578\n",
      "Epoch 65 | train loss 0.4062 acc 0.8583 | val loss 0.4057 acc 0.8580\n",
      "Epoch 66 | train loss 0.4061 acc 0.8576 | val loss 0.4057 acc 0.8582\n",
      "Epoch 67 | train loss 0.4062 acc 0.8579 | val loss 0.4057 acc 0.8584\n",
      "Epoch 68 | train loss 0.4061 acc 0.8577 | val loss 0.4057 acc 0.8582\n",
      "Epoch 69 | train loss 0.4061 acc 0.8579 | val loss 0.4056 acc 0.8585\n",
      "Epoch 70 | train loss 0.4061 acc 0.8581 | val loss 0.4056 acc 0.8584\n",
      "Epoch 71 | train loss 0.4060 acc 0.8583 | val loss 0.4056 acc 0.8579\n",
      "Epoch 72 | train loss 0.4060 acc 0.8579 | val loss 0.4056 acc 0.8579\n",
      "Epoch 73 | train loss 0.4060 acc 0.8581 | val loss 0.4056 acc 0.8579\n",
      "Epoch 74 | train loss 0.4060 acc 0.8582 | val loss 0.4056 acc 0.8575\n",
      "Epoch 75 | train loss 0.4060 acc 0.8575 | val loss 0.4055 acc 0.8581\n",
      "Epoch 76 | train loss 0.4059 acc 0.8581 | val loss 0.4056 acc 0.8582\n",
      "Epoch 77 | train loss 0.4059 acc 0.8578 | val loss 0.4055 acc 0.8581\n",
      "Epoch 78 | train loss 0.4060 acc 0.8582 | val loss 0.4055 acc 0.8583\n",
      "Epoch 79 | train loss 0.4059 acc 0.8580 | val loss 0.4054 acc 0.8578\n",
      "Epoch 80 | train loss 0.4059 acc 0.8580 | val loss 0.4054 acc 0.8583\n",
      "Epoch 81 | train loss 0.4058 acc 0.8580 | val loss 0.4056 acc 0.8576\n",
      "Epoch 82 | train loss 0.4058 acc 0.8584 | val loss 0.4055 acc 0.8579\n",
      "Epoch 83 | train loss 0.4059 acc 0.8571 | val loss 0.4054 acc 0.8581\n",
      "Epoch 84 | train loss 0.4058 acc 0.8580 | val loss 0.4055 acc 0.8582\n",
      "Epoch 85 | train loss 0.4059 acc 0.8580 | val loss 0.4054 acc 0.8583\n",
      "Epoch 86 | train loss 0.4058 acc 0.8577 | val loss 0.4054 acc 0.8585\n",
      "Epoch 87 | train loss 0.4058 acc 0.8584 | val loss 0.4053 acc 0.8579\n",
      "Epoch 88 | train loss 0.4058 acc 0.8576 | val loss 0.4054 acc 0.8586\n",
      "Epoch 89 | train loss 0.4058 acc 0.8579 | val loss 0.4053 acc 0.8584\n",
      "Epoch 90 | train loss 0.4057 acc 0.8579 | val loss 0.4054 acc 0.8580\n",
      "Epoch 91 | train loss 0.4057 acc 0.8585 | val loss 0.4053 acc 0.8582\n",
      "Epoch 92 | train loss 0.4058 acc 0.8582 | val loss 0.4053 acc 0.8581\n",
      "Epoch 93 | train loss 0.4058 acc 0.8580 | val loss 0.4053 acc 0.8580\n",
      "Epoch 94 | train loss 0.4058 acc 0.8579 | val loss 0.4053 acc 0.8576\n",
      "Epoch 95 | train loss 0.4057 acc 0.8580 | val loss 0.4053 acc 0.8582\n",
      "Epoch 96 | train loss 0.4057 acc 0.8576 | val loss 0.4053 acc 0.8583\n",
      "Epoch 97 | train loss 0.4057 acc 0.8580 | val loss 0.4052 acc 0.8584\n",
      "Epoch 98 | train loss 0.4057 acc 0.8580 | val loss 0.4052 acc 0.8581\n",
      "Epoch 99 | train loss 0.4058 acc 0.8580 | val loss 0.4053 acc 0.8584\n",
      "Epoch 100 | train loss 0.4056 acc 0.8582 | val loss 0.4052 acc 0.8587\n",
      "Epoch 101 | train loss 0.4057 acc 0.8579 | val loss 0.4052 acc 0.8582\n",
      "Epoch 102 | train loss 0.4056 acc 0.8578 | val loss 0.4052 acc 0.8580\n",
      "Epoch 103 | train loss 0.4056 acc 0.8574 | val loss 0.4052 acc 0.8581\n",
      "Epoch 104 | train loss 0.4056 acc 0.8581 | val loss 0.4052 acc 0.8582\n",
      "Epoch 105 | train loss 0.4056 acc 0.8575 | val loss 0.4052 acc 0.8581\n",
      "Epoch 106 | train loss 0.4056 acc 0.8580 | val loss 0.4051 acc 0.8584\n",
      "Epoch 107 | train loss 0.4056 acc 0.8586 | val loss 0.4052 acc 0.8580\n",
      "Epoch 108 | train loss 0.4056 acc 0.8579 | val loss 0.4051 acc 0.8583\n",
      "Epoch 109 | train loss 0.4055 acc 0.8576 | val loss 0.4051 acc 0.8579\n",
      "Epoch 110 | train loss 0.4056 acc 0.8579 | val loss 0.4051 acc 0.8580\n",
      "Epoch 111 | train loss 0.4055 acc 0.8581 | val loss 0.4052 acc 0.8579\n",
      "Epoch 112 | train loss 0.4056 acc 0.8582 | val loss 0.4051 acc 0.8582\n",
      "Epoch 113 | train loss 0.4055 acc 0.8577 | val loss 0.4051 acc 0.8579\n",
      "Epoch 114 | train loss 0.4055 acc 0.8581 | val loss 0.4051 acc 0.8580\n",
      "Epoch 115 | train loss 0.4055 acc 0.8579 | val loss 0.4051 acc 0.8584\n",
      "Epoch 116 | train loss 0.4055 acc 0.8579 | val loss 0.4051 acc 0.8579\n",
      "Epoch 117 | train loss 0.4055 acc 0.8576 | val loss 0.4051 acc 0.8577\n",
      "Epoch 118 | train loss 0.4054 acc 0.8577 | val loss 0.4052 acc 0.8584\n",
      "Epoch 119 | train loss 0.4055 acc 0.8580 | val loss 0.4050 acc 0.8582\n",
      "Epoch 120 | train loss 0.4054 acc 0.8581 | val loss 0.4050 acc 0.8580\n",
      "Epoch 121 | train loss 0.4055 acc 0.8580 | val loss 0.4050 acc 0.8585\n",
      "Epoch 122 | train loss 0.4054 acc 0.8582 | val loss 0.4050 acc 0.8580\n",
      "Epoch 123 | train loss 0.4054 acc 0.8575 | val loss 0.4050 acc 0.8583\n",
      "Epoch 124 | train loss 0.4054 acc 0.8579 | val loss 0.4050 acc 0.8577\n",
      "Epoch 125 | train loss 0.4054 acc 0.8580 | val loss 0.4050 acc 0.8580\n",
      "Epoch 126 | train loss 0.4054 acc 0.8583 | val loss 0.4050 acc 0.8583\n",
      "Epoch 127 | train loss 0.4054 acc 0.8581 | val loss 0.4049 acc 0.8581\n",
      "Epoch 128 | train loss 0.4054 acc 0.8578 | val loss 0.4050 acc 0.8581\n",
      "Epoch 129 | train loss 0.4054 acc 0.8580 | val loss 0.4050 acc 0.8583\n",
      "Epoch 130 | train loss 0.4054 acc 0.8582 | val loss 0.4050 acc 0.8585\n",
      "Epoch 131 | train loss 0.4054 acc 0.8579 | val loss 0.4049 acc 0.8583\n",
      "Epoch 132 | train loss 0.4053 acc 0.8583 | val loss 0.4050 acc 0.8585\n",
      "Epoch 133 | train loss 0.4054 acc 0.8580 | val loss 0.4049 acc 0.8586\n",
      "Epoch 134 | train loss 0.4054 acc 0.8579 | val loss 0.4049 acc 0.8581\n",
      "Epoch 135 | train loss 0.4053 acc 0.8580 | val loss 0.4050 acc 0.8579\n",
      "Epoch 136 | train loss 0.4054 acc 0.8578 | val loss 0.4049 acc 0.8579\n",
      "Epoch 137 | train loss 0.4054 acc 0.8579 | val loss 0.4049 acc 0.8584\n",
      "Epoch 138 | train loss 0.4053 acc 0.8579 | val loss 0.4050 acc 0.8579\n",
      "Epoch 139 | train loss 0.4053 acc 0.8581 | val loss 0.4049 acc 0.8580\n",
      "Epoch 140 | train loss 0.4053 acc 0.8584 | val loss 0.4049 acc 0.8578\n",
      "Epoch 141 | train loss 0.4053 acc 0.8580 | val loss 0.4048 acc 0.8581\n",
      "Epoch 142 | train loss 0.4053 acc 0.8576 | val loss 0.4048 acc 0.8583\n",
      "Epoch 143 | train loss 0.4053 acc 0.8580 | val loss 0.4048 acc 0.8580\n",
      "Epoch 144 | train loss 0.4053 acc 0.8582 | val loss 0.4048 acc 0.8576\n",
      "Epoch 145 | train loss 0.4052 acc 0.8579 | val loss 0.4049 acc 0.8581\n",
      "Epoch 146 | train loss 0.4053 acc 0.8580 | val loss 0.4048 acc 0.8586\n",
      "Epoch 147 | train loss 0.4053 acc 0.8575 | val loss 0.4049 acc 0.8586\n",
      "Epoch 148 | train loss 0.4053 acc 0.8583 | val loss 0.4048 acc 0.8580\n",
      "Epoch 149 | train loss 0.4053 acc 0.8582 | val loss 0.4049 acc 0.8582\n",
      "Epoch 150 | train loss 0.4052 acc 0.8584 | val loss 0.4048 acc 0.8579\n",
      "Epoch 151 | train loss 0.4053 acc 0.8580 | val loss 0.4048 acc 0.8583\n",
      "Epoch 152 | train loss 0.4052 acc 0.8579 | val loss 0.4048 acc 0.8579\n",
      "Epoch 153 | train loss 0.4053 acc 0.8576 | val loss 0.4048 acc 0.8587\n",
      "Epoch 154 | train loss 0.4052 acc 0.8578 | val loss 0.4048 acc 0.8578\n",
      "Epoch 155 | train loss 0.4052 acc 0.8580 | val loss 0.4048 acc 0.8579\n",
      "Epoch 156 | train loss 0.4052 acc 0.8577 | val loss 0.4048 acc 0.8581\n",
      "Epoch 157 | train loss 0.4052 acc 0.8581 | val loss 0.4048 acc 0.8580\n",
      "Epoch 158 | train loss 0.4052 acc 0.8579 | val loss 0.4048 acc 0.8582\n",
      "Epoch 159 | train loss 0.4053 acc 0.8580 | val loss 0.4047 acc 0.8585\n",
      "Epoch 160 | train loss 0.4052 acc 0.8576 | val loss 0.4048 acc 0.8583\n",
      "Epoch 161 | train loss 0.4052 acc 0.8583 | val loss 0.4048 acc 0.8578\n",
      "Epoch 162 | train loss 0.4052 acc 0.8581 | val loss 0.4048 acc 0.8581\n",
      "Epoch 163 | train loss 0.4052 acc 0.8579 | val loss 0.4047 acc 0.8581\n",
      "Epoch 164 | train loss 0.4052 acc 0.8581 | val loss 0.4047 acc 0.8579\n",
      "Epoch 165 | train loss 0.4052 acc 0.8581 | val loss 0.4047 acc 0.8577\n",
      "Epoch 166 | train loss 0.4052 acc 0.8579 | val loss 0.4047 acc 0.8582\n",
      "Epoch 167 | train loss 0.4052 acc 0.8579 | val loss 0.4047 acc 0.8581\n",
      "Epoch 168 | train loss 0.4051 acc 0.8580 | val loss 0.4047 acc 0.8579\n",
      "Epoch 169 | train loss 0.4052 acc 0.8580 | val loss 0.4047 acc 0.8580\n",
      "Epoch 170 | train loss 0.4051 acc 0.8583 | val loss 0.4047 acc 0.8583\n",
      "Epoch 171 | train loss 0.4051 acc 0.8581 | val loss 0.4047 acc 0.8584\n",
      "Epoch 172 | train loss 0.4051 acc 0.8577 | val loss 0.4048 acc 0.8586\n",
      "Epoch 173 | train loss 0.4052 acc 0.8578 | val loss 0.4047 acc 0.8585\n",
      "Epoch 174 | train loss 0.4051 acc 0.8581 | val loss 0.4046 acc 0.8578\n",
      "Epoch 175 | train loss 0.4051 acc 0.8582 | val loss 0.4046 acc 0.8580\n",
      "Epoch 176 | train loss 0.4051 acc 0.8586 | val loss 0.4047 acc 0.8585\n",
      "Epoch 177 | train loss 0.4051 acc 0.8583 | val loss 0.4047 acc 0.8580\n",
      "Epoch 178 | train loss 0.4051 acc 0.8581 | val loss 0.4046 acc 0.8580\n",
      "Epoch 179 | train loss 0.4051 acc 0.8580 | val loss 0.4046 acc 0.8581\n",
      "Epoch 180 | train loss 0.4051 acc 0.8577 | val loss 0.4047 acc 0.8580\n",
      "Epoch 181 | train loss 0.4051 acc 0.8579 | val loss 0.4046 acc 0.8583\n",
      "Epoch 182 | train loss 0.4051 acc 0.8581 | val loss 0.4046 acc 0.8579\n",
      "Epoch 183 | train loss 0.4051 acc 0.8579 | val loss 0.4046 acc 0.8580\n",
      "Epoch 184 | train loss 0.4051 acc 0.8581 | val loss 0.4046 acc 0.8583\n",
      "Epoch 185 | train loss 0.4051 acc 0.8580 | val loss 0.4046 acc 0.8583\n",
      "Epoch 186 | train loss 0.4050 acc 0.8580 | val loss 0.4046 acc 0.8578\n",
      "Epoch 187 | train loss 0.4051 acc 0.8577 | val loss 0.4046 acc 0.8582\n",
      "Epoch 188 | train loss 0.4050 acc 0.8581 | val loss 0.4047 acc 0.8583\n",
      "Epoch 189 | train loss 0.4050 acc 0.8581 | val loss 0.4047 acc 0.8584\n",
      "Epoch 190 | train loss 0.4051 acc 0.8580 | val loss 0.4046 acc 0.8584\n",
      "Epoch 191 | train loss 0.4050 acc 0.8581 | val loss 0.4046 acc 0.8580\n",
      "Epoch 192 | train loss 0.4050 acc 0.8583 | val loss 0.4046 acc 0.8583\n",
      "Epoch 193 | train loss 0.4050 acc 0.8580 | val loss 0.4046 acc 0.8582\n",
      "Epoch 194 | train loss 0.4050 acc 0.8579 | val loss 0.4046 acc 0.8580\n",
      "Epoch 195 | train loss 0.4050 acc 0.8582 | val loss 0.4046 acc 0.8580\n",
      "Epoch 196 | train loss 0.4050 acc 0.8583 | val loss 0.4046 acc 0.8578\n",
      "Epoch 197 | train loss 0.4050 acc 0.8577 | val loss 0.4046 acc 0.8582\n",
      "Epoch 198 | train loss 0.4050 acc 0.8579 | val loss 0.4046 acc 0.8583\n",
      "Epoch 199 | train loss 0.4050 acc 0.8581 | val loss 0.4045 acc 0.8581\n",
      "Epoch 200 | train loss 0.4050 acc 0.8580 | val loss 0.4046 acc 0.8580\n",
      "Epoch 201 | train loss 0.4050 acc 0.8579 | val loss 0.4046 acc 0.8585\n",
      "Epoch 202 | train loss 0.4050 acc 0.8584 | val loss 0.4045 acc 0.8580\n",
      "Epoch 203 | train loss 0.4049 acc 0.8581 | val loss 0.4046 acc 0.8581\n",
      "Epoch 204 | train loss 0.4050 acc 0.8584 | val loss 0.4045 acc 0.8581\n",
      "Epoch 205 | train loss 0.4050 acc 0.8575 | val loss 0.4045 acc 0.8579\n",
      "Epoch 206 | train loss 0.4050 acc 0.8581 | val loss 0.4045 acc 0.8581\n",
      "Epoch 207 | train loss 0.4049 acc 0.8580 | val loss 0.4045 acc 0.8579\n",
      "Epoch 208 | train loss 0.4050 acc 0.8582 | val loss 0.4045 acc 0.8580\n",
      "Epoch 209 | train loss 0.4050 acc 0.8581 | val loss 0.4045 acc 0.8586\n",
      "Epoch 210 | train loss 0.4049 acc 0.8579 | val loss 0.4046 acc 0.8579\n",
      "Epoch 211 | train loss 0.4049 acc 0.8583 | val loss 0.4045 acc 0.8579\n",
      "Epoch 212 | train loss 0.4050 acc 0.8577 | val loss 0.4045 acc 0.8580\n",
      "Epoch 213 | train loss 0.4050 acc 0.8576 | val loss 0.4045 acc 0.8582\n",
      "Epoch 214 | train loss 0.4050 acc 0.8582 | val loss 0.4045 acc 0.8584\n",
      "Epoch 215 | train loss 0.4049 acc 0.8580 | val loss 0.4045 acc 0.8584\n",
      "Epoch 216 | train loss 0.4050 acc 0.8581 | val loss 0.4045 acc 0.8582\n",
      "Epoch 217 | train loss 0.4050 acc 0.8577 | val loss 0.4046 acc 0.8583\n",
      "Epoch 218 | train loss 0.4049 acc 0.8579 | val loss 0.4045 acc 0.8581\n",
      "Epoch 219 | train loss 0.4050 acc 0.8584 | val loss 0.4045 acc 0.8582\n",
      "Epoch 220 | train loss 0.4049 acc 0.8579 | val loss 0.4045 acc 0.8583\n",
      "Epoch 221 | train loss 0.4049 acc 0.8581 | val loss 0.4045 acc 0.8586\n",
      "Epoch 222 | train loss 0.4049 acc 0.8581 | val loss 0.4045 acc 0.8580\n",
      "Epoch 223 | train loss 0.4048 acc 0.8581 | val loss 0.4045 acc 0.8585\n",
      "Epoch 224 | train loss 0.4049 acc 0.8583 | val loss 0.4045 acc 0.8580\n",
      "Epoch 225 | train loss 0.4049 acc 0.8578 | val loss 0.4045 acc 0.8576\n",
      "Epoch 226 | train loss 0.4049 acc 0.8578 | val loss 0.4046 acc 0.8582\n",
      "Epoch 227 | train loss 0.4049 acc 0.8583 | val loss 0.4044 acc 0.8579\n",
      "Epoch 228 | train loss 0.4049 acc 0.8581 | val loss 0.4045 acc 0.8576\n",
      "Epoch 229 | train loss 0.4049 acc 0.8581 | val loss 0.4045 acc 0.8577\n",
      "Epoch 230 | train loss 0.4049 acc 0.8584 | val loss 0.4044 acc 0.8580\n",
      "Epoch 231 | train loss 0.4049 acc 0.8580 | val loss 0.4045 acc 0.8583\n",
      "Epoch 232 | train loss 0.4049 acc 0.8577 | val loss 0.4044 acc 0.8584\n",
      "Epoch 233 | train loss 0.4049 acc 0.8580 | val loss 0.4045 acc 0.8580\n",
      "Epoch 234 | train loss 0.4048 acc 0.8583 | val loss 0.4045 acc 0.8581\n",
      "Epoch 235 | train loss 0.4049 acc 0.8580 | val loss 0.4044 acc 0.8583\n",
      "Epoch 236 | train loss 0.4049 acc 0.8582 | val loss 0.4044 acc 0.8582\n",
      "Epoch 237 | train loss 0.4048 acc 0.8581 | val loss 0.4044 acc 0.8586\n",
      "Epoch 238 | train loss 0.4049 acc 0.8581 | val loss 0.4044 acc 0.8581\n",
      "Epoch 239 | train loss 0.4049 acc 0.8583 | val loss 0.4044 acc 0.8587\n",
      "Epoch 240 | train loss 0.4048 acc 0.8583 | val loss 0.4044 acc 0.8582\n",
      "Epoch 241 | train loss 0.4048 acc 0.8579 | val loss 0.4045 acc 0.8580\n",
      "Epoch 242 | train loss 0.4048 acc 0.8582 | val loss 0.4044 acc 0.8579\n",
      "Epoch 243 | train loss 0.4049 acc 0.8584 | val loss 0.4045 acc 0.8581\n",
      "Epoch 244 | train loss 0.4048 acc 0.8581 | val loss 0.4044 acc 0.8580\n",
      "Epoch 245 | train loss 0.4048 acc 0.8580 | val loss 0.4044 acc 0.8580\n",
      "Epoch 246 | train loss 0.4048 acc 0.8578 | val loss 0.4044 acc 0.8588\n",
      "Epoch 247 | train loss 0.4049 acc 0.8583 | val loss 0.4044 acc 0.8581\n",
      "Epoch 248 | train loss 0.4048 acc 0.8586 | val loss 0.4044 acc 0.8582\n",
      "Epoch 249 | train loss 0.4048 acc 0.8580 | val loss 0.4044 acc 0.8586\n",
      "Epoch 250 | train loss 0.4048 acc 0.8589 | val loss 0.4045 acc 0.8579\n",
      "Epoch 251 | train loss 0.4048 acc 0.8579 | val loss 0.4044 acc 0.8581\n",
      "Epoch 252 | train loss 0.4048 acc 0.8583 | val loss 0.4044 acc 0.8581\n",
      "Epoch 253 | train loss 0.4048 acc 0.8580 | val loss 0.4044 acc 0.8581\n",
      "Epoch 254 | train loss 0.4049 acc 0.8576 | val loss 0.4044 acc 0.8583\n",
      "Epoch 255 | train loss 0.4048 acc 0.8580 | val loss 0.4044 acc 0.8582\n",
      "Epoch 256 | train loss 0.4048 acc 0.8585 | val loss 0.4044 acc 0.8584\n",
      "Epoch 257 | train loss 0.4047 acc 0.8581 | val loss 0.4044 acc 0.8581\n",
      "Epoch 258 | train loss 0.4048 acc 0.8580 | val loss 0.4044 acc 0.8580\n",
      "Epoch 259 | train loss 0.4048 acc 0.8582 | val loss 0.4044 acc 0.8576\n",
      "Epoch 260 | train loss 0.4048 acc 0.8580 | val loss 0.4044 acc 0.8583\n",
      "Epoch 261 | train loss 0.4047 acc 0.8580 | val loss 0.4045 acc 0.8583\n",
      "Epoch 262 | train loss 0.4048 acc 0.8580 | val loss 0.4043 acc 0.8586\n",
      "Epoch 263 | train loss 0.4048 acc 0.8580 | val loss 0.4044 acc 0.8580\n",
      "Epoch 264 | train loss 0.4048 acc 0.8586 | val loss 0.4043 acc 0.8584\n",
      "Epoch 265 | train loss 0.4047 acc 0.8581 | val loss 0.4044 acc 0.8587\n",
      "Epoch 266 | train loss 0.4048 acc 0.8584 | val loss 0.4043 acc 0.8583\n",
      "Epoch 267 | train loss 0.4047 acc 0.8580 | val loss 0.4044 acc 0.8587\n",
      "Epoch 268 | train loss 0.4048 acc 0.8579 | val loss 0.4043 acc 0.8587\n",
      "Epoch 269 | train loss 0.4048 acc 0.8580 | val loss 0.4043 acc 0.8578\n",
      "Epoch 270 | train loss 0.4047 acc 0.8580 | val loss 0.4044 acc 0.8584\n",
      "Epoch 271 | train loss 0.4049 acc 0.8579 | val loss 0.4043 acc 0.8587\n",
      "Epoch 272 | train loss 0.4048 acc 0.8583 | val loss 0.4043 acc 0.8586\n",
      "Epoch 273 | train loss 0.4047 acc 0.8582 | val loss 0.4043 acc 0.8580\n",
      "Epoch 274 | train loss 0.4048 acc 0.8584 | val loss 0.4043 acc 0.8583\n",
      "Epoch 275 | train loss 0.4048 acc 0.8583 | val loss 0.4043 acc 0.8585\n",
      "Epoch 276 | train loss 0.4047 acc 0.8581 | val loss 0.4043 acc 0.8584\n",
      "Epoch 277 | train loss 0.4048 acc 0.8584 | val loss 0.4043 acc 0.8589\n",
      "Epoch 278 | train loss 0.4048 acc 0.8579 | val loss 0.4043 acc 0.8583\n",
      "Epoch 279 | train loss 0.4047 acc 0.8581 | val loss 0.4043 acc 0.8583\n",
      "Epoch 280 | train loss 0.4048 acc 0.8583 | val loss 0.4043 acc 0.8583\n",
      "Epoch 281 | train loss 0.4048 acc 0.8583 | val loss 0.4044 acc 0.8588\n",
      "Epoch 282 | train loss 0.4047 acc 0.8582 | val loss 0.4043 acc 0.8585\n",
      "Epoch 283 | train loss 0.4047 acc 0.8581 | val loss 0.4043 acc 0.8581\n",
      "Epoch 284 | train loss 0.4047 acc 0.8581 | val loss 0.4043 acc 0.8581\n",
      "Epoch 285 | train loss 0.4048 acc 0.8579 | val loss 0.4043 acc 0.8580\n",
      "Epoch 286 | train loss 0.4047 acc 0.8579 | val loss 0.4043 acc 0.8580\n",
      "Epoch 287 | train loss 0.4047 acc 0.8580 | val loss 0.4043 acc 0.8582\n",
      "Epoch 288 | train loss 0.4047 acc 0.8580 | val loss 0.4043 acc 0.8581\n",
      "Epoch 289 | train loss 0.4047 acc 0.8581 | val loss 0.4043 acc 0.8582\n",
      "Epoch 290 | train loss 0.4047 acc 0.8583 | val loss 0.4042 acc 0.8585\n",
      "Epoch 291 | train loss 0.4047 acc 0.8581 | val loss 0.4042 acc 0.8581\n",
      "Epoch 292 | train loss 0.4047 acc 0.8584 | val loss 0.4043 acc 0.8584\n",
      "Epoch 293 | train loss 0.4047 acc 0.8582 | val loss 0.4043 acc 0.8585\n",
      "Epoch 294 | train loss 0.4047 acc 0.8582 | val loss 0.4043 acc 0.8580\n",
      "Epoch 295 | train loss 0.4047 acc 0.8582 | val loss 0.4042 acc 0.8586\n",
      "Epoch 296 | train loss 0.4047 acc 0.8579 | val loss 0.4043 acc 0.8583\n",
      "Epoch 297 | train loss 0.4047 acc 0.8580 | val loss 0.4042 acc 0.8586\n",
      "Epoch 298 | train loss 0.4046 acc 0.8583 | val loss 0.4043 acc 0.8578\n",
      "Epoch 299 | train loss 0.4047 acc 0.8581 | val loss 0.4043 acc 0.8582\n",
      "Epoch 300 | train loss 0.4046 acc 0.8583 | val loss 0.4043 acc 0.8584\n",
      "Epoch 301 | train loss 0.4047 acc 0.8578 | val loss 0.4042 acc 0.8585\n",
      "Epoch 302 | train loss 0.4047 acc 0.8581 | val loss 0.4042 acc 0.8583\n",
      "Epoch 303 | train loss 0.4047 acc 0.8581 | val loss 0.4042 acc 0.8582\n",
      "Epoch 304 | train loss 0.4047 acc 0.8580 | val loss 0.4043 acc 0.8585\n",
      "Epoch 305 | train loss 0.4047 acc 0.8584 | val loss 0.4042 acc 0.8582\n",
      "Epoch 306 | train loss 0.4047 acc 0.8582 | val loss 0.4043 acc 0.8588\n",
      "Epoch 307 | train loss 0.4047 acc 0.8582 | val loss 0.4042 acc 0.8587\n",
      "Epoch 308 | train loss 0.4047 acc 0.8578 | val loss 0.4042 acc 0.8581\n",
      "Epoch 309 | train loss 0.4046 acc 0.8578 | val loss 0.4042 acc 0.8586\n",
      "Epoch 310 | train loss 0.4046 acc 0.8584 | val loss 0.4042 acc 0.8581\n",
      "Epoch 311 | train loss 0.4047 acc 0.8579 | val loss 0.4043 acc 0.8583\n",
      "Epoch 312 | train loss 0.4047 acc 0.8581 | val loss 0.4042 acc 0.8584\n",
      "Epoch 313 | train loss 0.4047 acc 0.8579 | val loss 0.4042 acc 0.8585\n",
      "Epoch 314 | train loss 0.4046 acc 0.8579 | val loss 0.4043 acc 0.8584\n",
      "Epoch 315 | train loss 0.4047 acc 0.8583 | val loss 0.4042 acc 0.8582\n",
      "Epoch 316 | train loss 0.4046 acc 0.8580 | val loss 0.4043 acc 0.8580\n",
      "Epoch 317 | train loss 0.4046 acc 0.8579 | val loss 0.4042 acc 0.8586\n",
      "Epoch 318 | train loss 0.4047 acc 0.8585 | val loss 0.4042 acc 0.8583\n",
      "Epoch 319 | train loss 0.4047 acc 0.8579 | val loss 0.4042 acc 0.8584\n",
      "Epoch 320 | train loss 0.4046 acc 0.8582 | val loss 0.4042 acc 0.8581\n",
      "Epoch 321 | train loss 0.4046 acc 0.8583 | val loss 0.4042 acc 0.8589\n",
      "Epoch 322 | train loss 0.4046 acc 0.8584 | val loss 0.4042 acc 0.8583\n",
      "Epoch 323 | train loss 0.4046 acc 0.8582 | val loss 0.4042 acc 0.8586\n",
      "Epoch 324 | train loss 0.4046 acc 0.8580 | val loss 0.4043 acc 0.8585\n",
      "Epoch 325 | train loss 0.4046 acc 0.8581 | val loss 0.4042 acc 0.8588\n",
      "Epoch 326 | train loss 0.4046 acc 0.8576 | val loss 0.4042 acc 0.8586\n",
      "Epoch 327 | train loss 0.4046 acc 0.8584 | val loss 0.4042 acc 0.8582\n",
      "Epoch 328 | train loss 0.4046 acc 0.8585 | val loss 0.4042 acc 0.8587\n",
      "Epoch 329 | train loss 0.4046 acc 0.8580 | val loss 0.4042 acc 0.8583\n",
      "Epoch 330 | train loss 0.4047 acc 0.8581 | val loss 0.4041 acc 0.8584\n",
      "Epoch 331 | train loss 0.4046 acc 0.8583 | val loss 0.4042 acc 0.8583\n",
      "Epoch 332 | train loss 0.4046 acc 0.8579 | val loss 0.4042 acc 0.8586\n",
      "Epoch 333 | train loss 0.4046 acc 0.8583 | val loss 0.4041 acc 0.8586\n",
      "Epoch 334 | train loss 0.4046 acc 0.8581 | val loss 0.4042 acc 0.8587\n",
      "Epoch 335 | train loss 0.4046 acc 0.8582 | val loss 0.4042 acc 0.8584\n",
      "Epoch 336 | train loss 0.4046 acc 0.8582 | val loss 0.4041 acc 0.8586\n",
      "Epoch 337 | train loss 0.4046 acc 0.8577 | val loss 0.4042 acc 0.8583\n",
      "Epoch 338 | train loss 0.4046 acc 0.8583 | val loss 0.4042 acc 0.8575\n",
      "Epoch 339 | train loss 0.4046 acc 0.8581 | val loss 0.4041 acc 0.8584\n",
      "Epoch 340 | train loss 0.4046 acc 0.8582 | val loss 0.4042 acc 0.8585\n",
      "Epoch 341 | train loss 0.4046 acc 0.8583 | val loss 0.4042 acc 0.8583\n",
      "Epoch 342 | train loss 0.4046 acc 0.8581 | val loss 0.4042 acc 0.8586\n",
      "Epoch 343 | train loss 0.4045 acc 0.8583 | val loss 0.4041 acc 0.8583\n",
      "Epoch 344 | train loss 0.4046 acc 0.8583 | val loss 0.4041 acc 0.8582\n",
      "Epoch 345 | train loss 0.4046 acc 0.8585 | val loss 0.4042 acc 0.8589\n",
      "Epoch 346 | train loss 0.4046 acc 0.8587 | val loss 0.4042 acc 0.8591\n",
      "Epoch 347 | train loss 0.4046 acc 0.8580 | val loss 0.4041 acc 0.8584\n",
      "Epoch 348 | train loss 0.4045 acc 0.8580 | val loss 0.4042 acc 0.8582\n",
      "Epoch 349 | train loss 0.4045 acc 0.8583 | val loss 0.4043 acc 0.8583\n",
      "Epoch 350 | train loss 0.4046 acc 0.8580 | val loss 0.4041 acc 0.8586\n",
      "Epoch 351 | train loss 0.4045 acc 0.8585 | val loss 0.4042 acc 0.8580\n",
      "Epoch 352 | train loss 0.4046 acc 0.8581 | val loss 0.4042 acc 0.8587\n",
      "Epoch 353 | train loss 0.4045 acc 0.8582 | val loss 0.4042 acc 0.8581\n",
      "Epoch 354 | train loss 0.4046 acc 0.8584 | val loss 0.4041 acc 0.8584\n",
      "Epoch 355 | train loss 0.4045 acc 0.8587 | val loss 0.4042 acc 0.8581\n",
      "Epoch 356 | train loss 0.4046 acc 0.8577 | val loss 0.4041 acc 0.8587\n",
      "Epoch 357 | train loss 0.4045 acc 0.8588 | val loss 0.4041 acc 0.8584\n",
      "Epoch 358 | train loss 0.4045 acc 0.8581 | val loss 0.4041 acc 0.8587\n",
      "Epoch 359 | train loss 0.4046 acc 0.8583 | val loss 0.4041 acc 0.8584\n",
      "Epoch 360 | train loss 0.4045 acc 0.8586 | val loss 0.4042 acc 0.8580\n",
      "Epoch 361 | train loss 0.4045 acc 0.8579 | val loss 0.4042 acc 0.8579\n",
      "Epoch 362 | train loss 0.4046 acc 0.8580 | val loss 0.4041 acc 0.8585\n",
      "Epoch 363 | train loss 0.4045 acc 0.8585 | val loss 0.4041 acc 0.8581\n",
      "Epoch 364 | train loss 0.4045 acc 0.8584 | val loss 0.4041 acc 0.8582\n",
      "Epoch 365 | train loss 0.4046 acc 0.8582 | val loss 0.4041 acc 0.8583\n",
      "Epoch 366 | train loss 0.4045 acc 0.8579 | val loss 0.4042 acc 0.8586\n",
      "Epoch 367 | train loss 0.4046 acc 0.8584 | val loss 0.4041 acc 0.8587\n",
      "Epoch 368 | train loss 0.4045 acc 0.8584 | val loss 0.4041 acc 0.8583\n",
      "Epoch 369 | train loss 0.4045 acc 0.8577 | val loss 0.4042 acc 0.8585\n",
      "Epoch 370 | train loss 0.4045 acc 0.8584 | val loss 0.4042 acc 0.8589\n",
      "Epoch 371 | train loss 0.4046 acc 0.8584 | val loss 0.4041 acc 0.8585\n",
      "Epoch 372 | train loss 0.4046 acc 0.8583 | val loss 0.4041 acc 0.8588\n",
      "Epoch 373 | train loss 0.4045 acc 0.8584 | val loss 0.4041 acc 0.8583\n",
      "Epoch 374 | train loss 0.4045 acc 0.8580 | val loss 0.4042 acc 0.8587\n",
      "Epoch 375 | train loss 0.4045 acc 0.8587 | val loss 0.4041 acc 0.8583\n",
      "Epoch 376 | train loss 0.4045 acc 0.8584 | val loss 0.4041 acc 0.8589\n",
      "Epoch 377 | train loss 0.4045 acc 0.8587 | val loss 0.4041 acc 0.8587\n",
      "Epoch 378 | train loss 0.4045 acc 0.8582 | val loss 0.4042 acc 0.8582\n",
      "Epoch 379 | train loss 0.4045 acc 0.8580 | val loss 0.4041 acc 0.8591\n",
      "Epoch 380 | train loss 0.4045 acc 0.8590 | val loss 0.4041 acc 0.8584\n",
      "Epoch 381 | train loss 0.4045 acc 0.8580 | val loss 0.4041 acc 0.8585\n",
      "Epoch 382 | train loss 0.4045 acc 0.8579 | val loss 0.4042 acc 0.8580\n",
      "Epoch 383 | train loss 0.4045 acc 0.8579 | val loss 0.4041 acc 0.8586\n",
      "Epoch 384 | train loss 0.4045 acc 0.8578 | val loss 0.4042 acc 0.8590\n",
      "Epoch 385 | train loss 0.4045 acc 0.8584 | val loss 0.4041 acc 0.8581\n",
      "Epoch 386 | train loss 0.4045 acc 0.8585 | val loss 0.4041 acc 0.8580\n",
      "Epoch 387 | train loss 0.4045 acc 0.8586 | val loss 0.4041 acc 0.8583\n",
      "Epoch 388 | train loss 0.4045 acc 0.8581 | val loss 0.4041 acc 0.8590\n",
      "Epoch 389 | train loss 0.4045 acc 0.8580 | val loss 0.4041 acc 0.8587\n",
      "Epoch 390 | train loss 0.4045 acc 0.8588 | val loss 0.4041 acc 0.8587\n",
      "Epoch 391 | train loss 0.4045 acc 0.8582 | val loss 0.4041 acc 0.8586\n",
      "Epoch 392 | train loss 0.4045 acc 0.8586 | val loss 0.4041 acc 0.8585\n",
      "Epoch 393 | train loss 0.4045 acc 0.8579 | val loss 0.4041 acc 0.8586\n",
      "Epoch 394 | train loss 0.4045 acc 0.8584 | val loss 0.4041 acc 0.8585\n",
      "Epoch 395 | train loss 0.4045 acc 0.8581 | val loss 0.4041 acc 0.8584\n",
      "Epoch 396 | train loss 0.4045 acc 0.8587 | val loss 0.4041 acc 0.8580\n",
      "Epoch 397 | train loss 0.4045 acc 0.8579 | val loss 0.4041 acc 0.8587\n",
      "Epoch 398 | train loss 0.4045 acc 0.8583 | val loss 0.4041 acc 0.8583\n",
      "Epoch 399 | train loss 0.4045 acc 0.8584 | val loss 0.4041 acc 0.8580\n",
      "Epoch 400 | train loss 0.4045 acc 0.8580 | val loss 0.4040 acc 0.8584\n",
      "Epoch 401 | train loss 0.4045 acc 0.8578 | val loss 0.4041 acc 0.8581\n",
      "Epoch 402 | train loss 0.4045 acc 0.8586 | val loss 0.4040 acc 0.8587\n",
      "Epoch 403 | train loss 0.4044 acc 0.8585 | val loss 0.4042 acc 0.8590\n",
      "Epoch 404 | train loss 0.4044 acc 0.8585 | val loss 0.4041 acc 0.8581\n",
      "Epoch 405 | train loss 0.4045 acc 0.8580 | val loss 0.4040 acc 0.8583\n",
      "Epoch 406 | train loss 0.4045 acc 0.8583 | val loss 0.4041 acc 0.8586\n",
      "Epoch 407 | train loss 0.4045 acc 0.8584 | val loss 0.4041 acc 0.8587\n",
      "Epoch 408 | train loss 0.4045 acc 0.8586 | val loss 0.4040 acc 0.8587\n",
      "Epoch 409 | train loss 0.4044 acc 0.8584 | val loss 0.4041 acc 0.8581\n",
      "Epoch 410 | train loss 0.4045 acc 0.8577 | val loss 0.4040 acc 0.8584\n",
      "Epoch 411 | train loss 0.4045 acc 0.8584 | val loss 0.4040 acc 0.8583\n",
      "Epoch 412 | train loss 0.4044 acc 0.8583 | val loss 0.4041 acc 0.8583\n",
      "Epoch 413 | train loss 0.4045 acc 0.8586 | val loss 0.4040 acc 0.8582\n",
      "Epoch 414 | train loss 0.4045 acc 0.8584 | val loss 0.4040 acc 0.8587\n",
      "Epoch 415 | train loss 0.4044 acc 0.8583 | val loss 0.4041 acc 0.8583\n",
      "Epoch 416 | train loss 0.4045 acc 0.8582 | val loss 0.4040 acc 0.8581\n",
      "Epoch 417 | train loss 0.4044 acc 0.8580 | val loss 0.4041 acc 0.8585\n",
      "Epoch 418 | train loss 0.4044 acc 0.8581 | val loss 0.4040 acc 0.8589\n",
      "Epoch 419 | train loss 0.4044 acc 0.8584 | val loss 0.4040 acc 0.8590\n",
      "Epoch 420 | train loss 0.4045 acc 0.8583 | val loss 0.4040 acc 0.8584\n",
      "Epoch 421 | train loss 0.4045 acc 0.8584 | val loss 0.4040 acc 0.8581\n",
      "Epoch 422 | train loss 0.4045 acc 0.8586 | val loss 0.4040 acc 0.8585\n",
      "Epoch 423 | train loss 0.4045 acc 0.8585 | val loss 0.4040 acc 0.8585\n",
      "Epoch 424 | train loss 0.4044 acc 0.8585 | val loss 0.4041 acc 0.8583\n",
      "Epoch 425 | train loss 0.4045 acc 0.8581 | val loss 0.4040 acc 0.8586\n",
      "Epoch 426 | train loss 0.4044 acc 0.8587 | val loss 0.4040 acc 0.8584\n",
      "Epoch 427 | train loss 0.4044 acc 0.8581 | val loss 0.4041 acc 0.8581\n",
      "Epoch 428 | train loss 0.4044 acc 0.8579 | val loss 0.4040 acc 0.8588\n",
      "Epoch 429 | train loss 0.4045 acc 0.8583 | val loss 0.4040 acc 0.8587\n",
      "Epoch 430 | train loss 0.4044 acc 0.8582 | val loss 0.4040 acc 0.8587\n",
      "Epoch 431 | train loss 0.4044 acc 0.8584 | val loss 0.4040 acc 0.8587\n",
      "Epoch 432 | train loss 0.4044 acc 0.8585 | val loss 0.4040 acc 0.8586\n",
      "Epoch 433 | train loss 0.4045 acc 0.8583 | val loss 0.4040 acc 0.8584\n",
      "Epoch 434 | train loss 0.4044 acc 0.8584 | val loss 0.4040 acc 0.8579\n",
      "Epoch 435 | train loss 0.4044 acc 0.8583 | val loss 0.4040 acc 0.8587\n",
      "Epoch 436 | train loss 0.4043 acc 0.8583 | val loss 0.4040 acc 0.8580\n",
      "Epoch 437 | train loss 0.4045 acc 0.8585 | val loss 0.4040 acc 0.8586\n",
      "Epoch 438 | train loss 0.4044 acc 0.8584 | val loss 0.4040 acc 0.8585\n",
      "Epoch 439 | train loss 0.4043 acc 0.8588 | val loss 0.4041 acc 0.8580\n",
      "Epoch 440 | train loss 0.4044 acc 0.8583 | val loss 0.4041 acc 0.8585\n",
      "Epoch 441 | train loss 0.4044 acc 0.8581 | val loss 0.4040 acc 0.8586\n",
      "Epoch 442 | train loss 0.4044 acc 0.8585 | val loss 0.4040 acc 0.8581\n",
      "Epoch 443 | train loss 0.4044 acc 0.8584 | val loss 0.4040 acc 0.8586\n",
      "Epoch 444 | train loss 0.4044 acc 0.8587 | val loss 0.4040 acc 0.8588\n",
      "Epoch 445 | train loss 0.4044 acc 0.8583 | val loss 0.4040 acc 0.8588\n",
      "Epoch 446 | train loss 0.4044 acc 0.8580 | val loss 0.4040 acc 0.8590\n",
      "Epoch 447 | train loss 0.4044 acc 0.8583 | val loss 0.4040 acc 0.8589\n",
      "Epoch 448 | train loss 0.4044 acc 0.8587 | val loss 0.4040 acc 0.8587\n",
      "Epoch 449 | train loss 0.4044 acc 0.8582 | val loss 0.4040 acc 0.8586\n",
      "Epoch 450 | train loss 0.4044 acc 0.8583 | val loss 0.4040 acc 0.8586\n",
      "Epoch 451 | train loss 0.4044 acc 0.8582 | val loss 0.4040 acc 0.8586\n",
      "Epoch 452 | train loss 0.4044 acc 0.8588 | val loss 0.4040 acc 0.8587\n",
      "Epoch 453 | train loss 0.4044 acc 0.8586 | val loss 0.4040 acc 0.8584\n",
      "Epoch 454 | train loss 0.4044 acc 0.8583 | val loss 0.4040 acc 0.8586\n",
      "Epoch 455 | train loss 0.4044 acc 0.8584 | val loss 0.4040 acc 0.8584\n",
      "Epoch 456 | train loss 0.4044 acc 0.8584 | val loss 0.4040 acc 0.8585\n",
      "Epoch 457 | train loss 0.4044 acc 0.8585 | val loss 0.4040 acc 0.8586\n",
      "Epoch 458 | train loss 0.4043 acc 0.8587 | val loss 0.4040 acc 0.8586\n",
      "Epoch 459 | train loss 0.4044 acc 0.8584 | val loss 0.4039 acc 0.8586\n",
      "Epoch 460 | train loss 0.4044 acc 0.8587 | val loss 0.4040 acc 0.8580\n",
      "Epoch 461 | train loss 0.4044 acc 0.8583 | val loss 0.4040 acc 0.8588\n",
      "Epoch 462 | train loss 0.4044 acc 0.8586 | val loss 0.4040 acc 0.8585\n",
      "Epoch 463 | train loss 0.4044 acc 0.8587 | val loss 0.4040 acc 0.8589\n",
      "Epoch 464 | train loss 0.4044 acc 0.8586 | val loss 0.4040 acc 0.8580\n",
      "Epoch 465 | train loss 0.4044 acc 0.8580 | val loss 0.4039 acc 0.8584\n",
      "Epoch 466 | train loss 0.4044 acc 0.8585 | val loss 0.4040 acc 0.8584\n",
      "Epoch 467 | train loss 0.4044 acc 0.8581 | val loss 0.4040 acc 0.8587\n",
      "Epoch 468 | train loss 0.4044 acc 0.8583 | val loss 0.4039 acc 0.8581\n",
      "Epoch 469 | train loss 0.4043 acc 0.8584 | val loss 0.4039 acc 0.8587\n",
      "Epoch 470 | train loss 0.4044 acc 0.8581 | val loss 0.4039 acc 0.8588\n",
      "Epoch 471 | train loss 0.4044 acc 0.8582 | val loss 0.4040 acc 0.8585\n",
      "Epoch 472 | train loss 0.4044 acc 0.8583 | val loss 0.4039 acc 0.8585\n",
      "Epoch 473 | train loss 0.4044 acc 0.8582 | val loss 0.4039 acc 0.8589\n",
      "Epoch 474 | train loss 0.4043 acc 0.8588 | val loss 0.4042 acc 0.8583\n",
      "Epoch 475 | train loss 0.4044 acc 0.8583 | val loss 0.4039 acc 0.8589\n",
      "Epoch 476 | train loss 0.4043 acc 0.8587 | val loss 0.4040 acc 0.8592\n",
      "Epoch 477 | train loss 0.4043 acc 0.8583 | val loss 0.4040 acc 0.8585\n",
      "Epoch 478 | train loss 0.4044 acc 0.8587 | val loss 0.4040 acc 0.8583\n",
      "Epoch 479 | train loss 0.4044 acc 0.8581 | val loss 0.4039 acc 0.8585\n",
      "Epoch 480 | train loss 0.4044 acc 0.8585 | val loss 0.4039 acc 0.8587\n",
      "Epoch 481 | train loss 0.4044 acc 0.8586 | val loss 0.4039 acc 0.8588\n",
      "Epoch 482 | train loss 0.4043 acc 0.8590 | val loss 0.4040 acc 0.8583\n",
      "Epoch 483 | train loss 0.4043 acc 0.8583 | val loss 0.4039 acc 0.8587\n",
      "Epoch 484 | train loss 0.4043 acc 0.8584 | val loss 0.4039 acc 0.8581\n",
      "Epoch 485 | train loss 0.4044 acc 0.8579 | val loss 0.4040 acc 0.8588\n",
      "Epoch 486 | train loss 0.4044 acc 0.8587 | val loss 0.4039 acc 0.8587\n",
      "Epoch 487 | train loss 0.4043 acc 0.8587 | val loss 0.4039 acc 0.8583\n",
      "Epoch 488 | train loss 0.4044 acc 0.8583 | val loss 0.4039 acc 0.8587\n",
      "Epoch 489 | train loss 0.4043 acc 0.8587 | val loss 0.4039 acc 0.8585\n",
      "Epoch 490 | train loss 0.4044 acc 0.8583 | val loss 0.4039 acc 0.8589\n",
      "Epoch 491 | train loss 0.4043 acc 0.8589 | val loss 0.4039 acc 0.8583\n",
      "Epoch 492 | train loss 0.4043 acc 0.8585 | val loss 0.4040 acc 0.8584\n",
      "Epoch 493 | train loss 0.4043 acc 0.8583 | val loss 0.4039 acc 0.8587\n",
      "Epoch 494 | train loss 0.4044 acc 0.8591 | val loss 0.4039 acc 0.8587\n",
      "Epoch 495 | train loss 0.4043 acc 0.8585 | val loss 0.4039 acc 0.8589\n",
      "Epoch 496 | train loss 0.4043 acc 0.8586 | val loss 0.4039 acc 0.8590\n",
      "Epoch 497 | train loss 0.4044 acc 0.8581 | val loss 0.4040 acc 0.8589\n",
      "Epoch 498 | train loss 0.4043 acc 0.8587 | val loss 0.4040 acc 0.8587\n",
      "Epoch 499 | train loss 0.4043 acc 0.8585 | val loss 0.4040 acc 0.8587\n",
      "Epoch 500 | train loss 0.4043 acc 0.8586 | val loss 0.4040 acc 0.8584\n",
      "Epoch 501 | train loss 0.4043 acc 0.8587 | val loss 0.4040 acc 0.8584\n",
      "Epoch 502 | train loss 0.4042 acc 0.8584 | val loss 0.4041 acc 0.8578\n",
      "Epoch 503 | train loss 0.4044 acc 0.8577 | val loss 0.4039 acc 0.8581\n",
      "Epoch 504 | train loss 0.4044 acc 0.8584 | val loss 0.4039 acc 0.8584\n",
      "Epoch 505 | train loss 0.4043 acc 0.8585 | val loss 0.4039 acc 0.8582\n",
      "Epoch 506 | train loss 0.4043 acc 0.8583 | val loss 0.4039 acc 0.8586\n",
      "Epoch 507 | train loss 0.4044 acc 0.8586 | val loss 0.4039 acc 0.8587\n",
      "Epoch 508 | train loss 0.4043 acc 0.8587 | val loss 0.4039 acc 0.8582\n",
      "Epoch 509 | train loss 0.4044 acc 0.8583 | val loss 0.4039 acc 0.8584\n",
      "Epoch 510 | train loss 0.4043 acc 0.8589 | val loss 0.4039 acc 0.8585\n",
      "Epoch 511 | train loss 0.4044 acc 0.8587 | val loss 0.4039 acc 0.8586\n",
      "Epoch 512 | train loss 0.4043 acc 0.8586 | val loss 0.4039 acc 0.8588\n",
      "Epoch 513 | train loss 0.4043 acc 0.8584 | val loss 0.4039 acc 0.8587\n",
      "Epoch 514 | train loss 0.4043 acc 0.8586 | val loss 0.4039 acc 0.8584\n",
      "Epoch 515 | train loss 0.4043 acc 0.8585 | val loss 0.4039 acc 0.8586\n",
      "Epoch 516 | train loss 0.4043 acc 0.8580 | val loss 0.4039 acc 0.8590\n",
      "Epoch 517 | train loss 0.4044 acc 0.8587 | val loss 0.4039 acc 0.8583\n",
      "Epoch 518 | train loss 0.4043 acc 0.8583 | val loss 0.4039 acc 0.8589\n",
      "Epoch 519 | train loss 0.4043 acc 0.8586 | val loss 0.4039 acc 0.8586\n",
      "Epoch 520 | train loss 0.4043 acc 0.8581 | val loss 0.4039 acc 0.8583\n",
      "Epoch 521 | train loss 0.4043 acc 0.8584 | val loss 0.4039 acc 0.8586\n",
      "Epoch 522 | train loss 0.4043 acc 0.8583 | val loss 0.4039 acc 0.8590\n",
      "Epoch 523 | train loss 0.4043 acc 0.8581 | val loss 0.4039 acc 0.8585\n",
      "Epoch 524 | train loss 0.4043 acc 0.8586 | val loss 0.4039 acc 0.8586\n",
      "Epoch 525 | train loss 0.4042 acc 0.8582 | val loss 0.4040 acc 0.8581\n",
      "Epoch 526 | train loss 0.4043 acc 0.8584 | val loss 0.4040 acc 0.8581\n",
      "Epoch 527 | train loss 0.4043 acc 0.8582 | val loss 0.4039 acc 0.8584\n",
      "Epoch 528 | train loss 0.4043 acc 0.8584 | val loss 0.4039 acc 0.8585\n",
      "Epoch 529 | train loss 0.4043 acc 0.8584 | val loss 0.4039 acc 0.8588\n",
      "Epoch 530 | train loss 0.4042 acc 0.8584 | val loss 0.4039 acc 0.8580\n",
      "Epoch 531 | train loss 0.4043 acc 0.8583 | val loss 0.4039 acc 0.8586\n",
      "Epoch 532 | train loss 0.4043 acc 0.8589 | val loss 0.4040 acc 0.8580\n",
      "Epoch 533 | train loss 0.4044 acc 0.8581 | val loss 0.4038 acc 0.8587\n",
      "Epoch 534 | train loss 0.4043 acc 0.8581 | val loss 0.4039 acc 0.8586\n",
      "Epoch 535 | train loss 0.4042 acc 0.8583 | val loss 0.4039 acc 0.8586\n",
      "Epoch 536 | train loss 0.4043 acc 0.8582 | val loss 0.4039 acc 0.8587\n",
      "Epoch 537 | train loss 0.4043 acc 0.8582 | val loss 0.4039 acc 0.8589\n",
      "Epoch 538 | train loss 0.4043 acc 0.8584 | val loss 0.4039 acc 0.8583\n",
      "Epoch 539 | train loss 0.4043 acc 0.8581 | val loss 0.4039 acc 0.8584\n",
      "Epoch 540 | train loss 0.4043 acc 0.8587 | val loss 0.4040 acc 0.8585\n",
      "Epoch 541 | train loss 0.4043 acc 0.8586 | val loss 0.4039 acc 0.8584\n",
      "Epoch 542 | train loss 0.4043 acc 0.8581 | val loss 0.4039 acc 0.8586\n",
      "Epoch 543 | train loss 0.4043 acc 0.8585 | val loss 0.4039 acc 0.8584\n",
      "Epoch 544 | train loss 0.4043 acc 0.8585 | val loss 0.4038 acc 0.8586\n",
      "Epoch 545 | train loss 0.4043 acc 0.8588 | val loss 0.4039 acc 0.8587\n",
      "Epoch 546 | train loss 0.4042 acc 0.8585 | val loss 0.4039 acc 0.8584\n",
      "Epoch 547 | train loss 0.4043 acc 0.8584 | val loss 0.4039 acc 0.8586\n",
      "Epoch 548 | train loss 0.4043 acc 0.8582 | val loss 0.4039 acc 0.8586\n",
      "Epoch 549 | train loss 0.4043 acc 0.8589 | val loss 0.4038 acc 0.8582\n",
      "Epoch 550 | train loss 0.4043 acc 0.8582 | val loss 0.4038 acc 0.8587\n",
      "Epoch 551 | train loss 0.4043 acc 0.8586 | val loss 0.4039 acc 0.8587\n",
      "Epoch 552 | train loss 0.4043 acc 0.8584 | val loss 0.4039 acc 0.8588\n",
      "Epoch 553 | train loss 0.4043 acc 0.8583 | val loss 0.4038 acc 0.8586\n",
      "Epoch 554 | train loss 0.4042 acc 0.8585 | val loss 0.4039 acc 0.8584\n",
      "Epoch 555 | train loss 0.4043 acc 0.8587 | val loss 0.4039 acc 0.8590\n",
      "Epoch 556 | train loss 0.4043 acc 0.8590 | val loss 0.4038 acc 0.8586\n",
      "Epoch 557 | train loss 0.4043 acc 0.8586 | val loss 0.4038 acc 0.8584\n",
      "Epoch 558 | train loss 0.4043 acc 0.8582 | val loss 0.4038 acc 0.8590\n",
      "Epoch 559 | train loss 0.4043 acc 0.8584 | val loss 0.4038 acc 0.8587\n",
      "Epoch 560 | train loss 0.4043 acc 0.8588 | val loss 0.4039 acc 0.8585\n",
      "Epoch 561 | train loss 0.4043 acc 0.8586 | val loss 0.4038 acc 0.8585\n",
      "Epoch 562 | train loss 0.4043 acc 0.8590 | val loss 0.4038 acc 0.8586\n",
      "Epoch 563 | train loss 0.4043 acc 0.8588 | val loss 0.4038 acc 0.8583\n",
      "Epoch 564 | train loss 0.4043 acc 0.8586 | val loss 0.4039 acc 0.8585\n",
      "Epoch 565 | train loss 0.4043 acc 0.8585 | val loss 0.4038 acc 0.8586\n",
      "Epoch 566 | train loss 0.4043 acc 0.8582 | val loss 0.4038 acc 0.8587\n",
      "Epoch 567 | train loss 0.4042 acc 0.8584 | val loss 0.4039 acc 0.8584\n",
      "Epoch 568 | train loss 0.4043 acc 0.8586 | val loss 0.4038 acc 0.8589\n",
      "Epoch 569 | train loss 0.4042 acc 0.8587 | val loss 0.4038 acc 0.8589\n",
      "Epoch 570 | train loss 0.4043 acc 0.8587 | val loss 0.4038 acc 0.8586\n",
      "Epoch 571 | train loss 0.4042 acc 0.8585 | val loss 0.4040 acc 0.8582\n",
      "Epoch 572 | train loss 0.4043 acc 0.8581 | val loss 0.4038 acc 0.8589\n",
      "Epoch 573 | train loss 0.4042 acc 0.8586 | val loss 0.4040 acc 0.8579\n",
      "Epoch 574 | train loss 0.4043 acc 0.8580 | val loss 0.4039 acc 0.8585\n",
      "Epoch 575 | train loss 0.4042 acc 0.8583 | val loss 0.4039 acc 0.8589\n",
      "Epoch 576 | train loss 0.4042 acc 0.8589 | val loss 0.4038 acc 0.8583\n",
      "Epoch 577 | train loss 0.4043 acc 0.8582 | val loss 0.4039 acc 0.8588\n",
      "Epoch 578 | train loss 0.4042 acc 0.8584 | val loss 0.4038 acc 0.8590\n",
      "Epoch 579 | train loss 0.4043 acc 0.8584 | val loss 0.4038 acc 0.8589\n",
      "Epoch 580 | train loss 0.4042 acc 0.8588 | val loss 0.4039 acc 0.8587\n",
      "Epoch 581 | train loss 0.4043 acc 0.8578 | val loss 0.4038 acc 0.8583\n",
      "Epoch 582 | train loss 0.4042 acc 0.8588 | val loss 0.4038 acc 0.8587\n",
      "Epoch 583 | train loss 0.4042 acc 0.8583 | val loss 0.4038 acc 0.8586\n",
      "Epoch 584 | train loss 0.4042 acc 0.8588 | val loss 0.4039 acc 0.8591\n",
      "Epoch 585 | train loss 0.4042 acc 0.8587 | val loss 0.4039 acc 0.8584\n",
      "Epoch 586 | train loss 0.4042 acc 0.8585 | val loss 0.4039 acc 0.8584\n",
      "Epoch 587 | train loss 0.4043 acc 0.8584 | val loss 0.4038 acc 0.8584\n",
      "Epoch 588 | train loss 0.4043 acc 0.8586 | val loss 0.4039 acc 0.8582\n",
      "Epoch 589 | train loss 0.4042 acc 0.8585 | val loss 0.4038 acc 0.8584\n",
      "Epoch 590 | train loss 0.4042 acc 0.8587 | val loss 0.4039 acc 0.8586\n",
      "Epoch 591 | train loss 0.4042 acc 0.8587 | val loss 0.4038 acc 0.8583\n",
      "Epoch 592 | train loss 0.4041 acc 0.8583 | val loss 0.4040 acc 0.8585\n",
      "Epoch 593 | train loss 0.4043 acc 0.8586 | val loss 0.4039 acc 0.8589\n",
      "Epoch 594 | train loss 0.4043 acc 0.8582 | val loss 0.4039 acc 0.8589\n",
      "Epoch 595 | train loss 0.4042 acc 0.8587 | val loss 0.4038 acc 0.8584\n",
      "Epoch 596 | train loss 0.4042 acc 0.8580 | val loss 0.4038 acc 0.8583\n",
      "Epoch 597 | train loss 0.4043 acc 0.8586 | val loss 0.4038 acc 0.8588\n",
      "Epoch 598 | train loss 0.4042 acc 0.8584 | val loss 0.4038 acc 0.8582\n",
      "Epoch 599 | train loss 0.4042 acc 0.8583 | val loss 0.4038 acc 0.8586\n",
      "Epoch 600 | train loss 0.4042 acc 0.8581 | val loss 0.4038 acc 0.8584\n",
      "Epoch 601 | train loss 0.4042 acc 0.8584 | val loss 0.4038 acc 0.8588\n",
      "Epoch 602 | train loss 0.4043 acc 0.8593 | val loss 0.4038 acc 0.8585\n",
      "Epoch 603 | train loss 0.4042 acc 0.8586 | val loss 0.4038 acc 0.8586\n",
      "Epoch 604 | train loss 0.4042 acc 0.8586 | val loss 0.4039 acc 0.8583\n",
      "Epoch 605 | train loss 0.4043 acc 0.8582 | val loss 0.4038 acc 0.8587\n",
      "Epoch 606 | train loss 0.4042 acc 0.8583 | val loss 0.4038 acc 0.8588\n",
      "Epoch 607 | train loss 0.4043 acc 0.8584 | val loss 0.4038 acc 0.8589\n",
      "Epoch 608 | train loss 0.4042 acc 0.8584 | val loss 0.4039 acc 0.8586\n",
      "Epoch 609 | train loss 0.4042 acc 0.8589 | val loss 0.4039 acc 0.8584\n",
      "Epoch 610 | train loss 0.4042 acc 0.8588 | val loss 0.4038 acc 0.8589\n",
      "Epoch 611 | train loss 0.4042 acc 0.8585 | val loss 0.4039 acc 0.8586\n",
      "Epoch 612 | train loss 0.4042 acc 0.8585 | val loss 0.4038 acc 0.8585\n",
      "Epoch 613 | train loss 0.4042 acc 0.8579 | val loss 0.4038 acc 0.8587\n",
      "Epoch 614 | train loss 0.4042 acc 0.8584 | val loss 0.4038 acc 0.8585\n",
      "Epoch 615 | train loss 0.4042 acc 0.8580 | val loss 0.4038 acc 0.8588\n",
      "Epoch 616 | train loss 0.4042 acc 0.8585 | val loss 0.4038 acc 0.8590\n",
      "Epoch 617 | train loss 0.4042 acc 0.8588 | val loss 0.4038 acc 0.8589\n",
      "Epoch 618 | train loss 0.4041 acc 0.8583 | val loss 0.4038 acc 0.8587\n",
      "Epoch 619 | train loss 0.4042 acc 0.8584 | val loss 0.4038 acc 0.8585\n",
      "Epoch 620 | train loss 0.4042 acc 0.8584 | val loss 0.4038 acc 0.8587\n",
      "Epoch 621 | train loss 0.4042 acc 0.8585 | val loss 0.4039 acc 0.8586\n",
      "Epoch 622 | train loss 0.4042 acc 0.8587 | val loss 0.4038 acc 0.8584\n",
      "Epoch 623 | train loss 0.4042 acc 0.8581 | val loss 0.4039 acc 0.8592\n",
      "Epoch 624 | train loss 0.4042 acc 0.8586 | val loss 0.4038 acc 0.8586\n",
      "Epoch 625 | train loss 0.4042 acc 0.8589 | val loss 0.4038 acc 0.8586\n",
      "Epoch 626 | train loss 0.4042 acc 0.8585 | val loss 0.4038 acc 0.8586\n",
      "Epoch 627 | train loss 0.4042 acc 0.8581 | val loss 0.4038 acc 0.8583\n",
      "Epoch 628 | train loss 0.4042 acc 0.8583 | val loss 0.4038 acc 0.8585\n",
      "Epoch 629 | train loss 0.4041 acc 0.8583 | val loss 0.4039 acc 0.8590\n",
      "Epoch 630 | train loss 0.4041 acc 0.8583 | val loss 0.4039 acc 0.8585\n",
      "Epoch 631 | train loss 0.4042 acc 0.8580 | val loss 0.4038 acc 0.8591\n",
      "Epoch 632 | train loss 0.4042 acc 0.8585 | val loss 0.4038 acc 0.8581\n",
      "Epoch 633 | train loss 0.4042 acc 0.8586 | val loss 0.4038 acc 0.8583\n",
      "Epoch 634 | train loss 0.4042 acc 0.8581 | val loss 0.4038 acc 0.8586\n",
      "Epoch 635 | train loss 0.4042 acc 0.8586 | val loss 0.4038 acc 0.8587\n",
      "Epoch 636 | train loss 0.4042 acc 0.8583 | val loss 0.4038 acc 0.8584\n",
      "Epoch 637 | train loss 0.4042 acc 0.8584 | val loss 0.4038 acc 0.8586\n",
      "Epoch 638 | train loss 0.4042 acc 0.8586 | val loss 0.4038 acc 0.8588\n",
      "Epoch 639 | train loss 0.4042 acc 0.8588 | val loss 0.4038 acc 0.8590\n",
      "Epoch 640 | train loss 0.4042 acc 0.8588 | val loss 0.4038 acc 0.8585\n",
      "Epoch 641 | train loss 0.4042 acc 0.8581 | val loss 0.4038 acc 0.8586\n",
      "Epoch 642 | train loss 0.4042 acc 0.8585 | val loss 0.4038 acc 0.8586\n",
      "Epoch 643 | train loss 0.4042 acc 0.8587 | val loss 0.4038 acc 0.8586\n",
      "Epoch 644 | train loss 0.4042 acc 0.8588 | val loss 0.4038 acc 0.8583\n",
      "Epoch 645 | train loss 0.4041 acc 0.8584 | val loss 0.4039 acc 0.8578\n",
      "Epoch 646 | train loss 0.4042 acc 0.8583 | val loss 0.4038 acc 0.8590\n",
      "Epoch 647 | train loss 0.4042 acc 0.8588 | val loss 0.4038 acc 0.8587\n",
      "Epoch 648 | train loss 0.4042 acc 0.8583 | val loss 0.4038 acc 0.8584\n",
      "Epoch 649 | train loss 0.4042 acc 0.8587 | val loss 0.4038 acc 0.8586\n",
      "Epoch 650 | train loss 0.4042 acc 0.8588 | val loss 0.4038 acc 0.8585\n",
      "Epoch 651 | train loss 0.4042 acc 0.8591 | val loss 0.4038 acc 0.8587\n",
      "Epoch 652 | train loss 0.4041 acc 0.8587 | val loss 0.4038 acc 0.8589\n",
      "Epoch 653 | train loss 0.4042 acc 0.8587 | val loss 0.4037 acc 0.8588\n",
      "Epoch 654 | train loss 0.4041 acc 0.8583 | val loss 0.4038 acc 0.8584\n",
      "Epoch 655 | train loss 0.4041 acc 0.8583 | val loss 0.4038 acc 0.8589\n",
      "Epoch 656 | train loss 0.4042 acc 0.8587 | val loss 0.4038 acc 0.8587\n",
      "Epoch 657 | train loss 0.4042 acc 0.8589 | val loss 0.4038 acc 0.8587\n",
      "Epoch 658 | train loss 0.4042 acc 0.8587 | val loss 0.4038 acc 0.8581\n",
      "Epoch 659 | train loss 0.4042 acc 0.8581 | val loss 0.4038 acc 0.8585\n",
      "Epoch 660 | train loss 0.4042 acc 0.8587 | val loss 0.4038 acc 0.8584\n",
      "Epoch 661 | train loss 0.4042 acc 0.8587 | val loss 0.4038 acc 0.8587\n",
      "Epoch 662 | train loss 0.4042 acc 0.8585 | val loss 0.4038 acc 0.8589\n",
      "Epoch 663 | train loss 0.4042 acc 0.8587 | val loss 0.4037 acc 0.8589\n",
      "Epoch 664 | train loss 0.4042 acc 0.8591 | val loss 0.4037 acc 0.8590\n",
      "Epoch 665 | train loss 0.4042 acc 0.8587 | val loss 0.4038 acc 0.8586\n",
      "Epoch 666 | train loss 0.4042 acc 0.8584 | val loss 0.4037 acc 0.8589\n",
      "Epoch 667 | train loss 0.4041 acc 0.8586 | val loss 0.4038 acc 0.8582\n",
      "Epoch 668 | train loss 0.4042 acc 0.8589 | val loss 0.4037 acc 0.8589\n",
      "Epoch 669 | train loss 0.4042 acc 0.8584 | val loss 0.4037 acc 0.8587\n",
      "Epoch 670 | train loss 0.4042 acc 0.8587 | val loss 0.4038 acc 0.8586\n",
      "Epoch 671 | train loss 0.4042 acc 0.8587 | val loss 0.4037 acc 0.8587\n",
      "Epoch 672 | train loss 0.4041 acc 0.8583 | val loss 0.4039 acc 0.8582\n",
      "Epoch 673 | train loss 0.4042 acc 0.8584 | val loss 0.4037 acc 0.8586\n",
      "Epoch 674 | train loss 0.4042 acc 0.8582 | val loss 0.4037 acc 0.8590\n",
      "Epoch 675 | train loss 0.4042 acc 0.8585 | val loss 0.4038 acc 0.8590\n",
      "Epoch 676 | train loss 0.4042 acc 0.8585 | val loss 0.4037 acc 0.8585\n",
      "Epoch 677 | train loss 0.4042 acc 0.8583 | val loss 0.4038 acc 0.8586\n",
      "Epoch 678 | train loss 0.4041 acc 0.8585 | val loss 0.4038 acc 0.8584\n",
      "Epoch 679 | train loss 0.4042 acc 0.8584 | val loss 0.4038 acc 0.8584\n",
      "Epoch 680 | train loss 0.4041 acc 0.8583 | val loss 0.4037 acc 0.8590\n",
      "Epoch 681 | train loss 0.4042 acc 0.8587 | val loss 0.4038 acc 0.8589\n",
      "Epoch 682 | train loss 0.4042 acc 0.8584 | val loss 0.4038 acc 0.8587\n",
      "Epoch 683 | train loss 0.4042 acc 0.8584 | val loss 0.4037 acc 0.8586\n",
      "Epoch 684 | train loss 0.4041 acc 0.8584 | val loss 0.4037 acc 0.8587\n",
      "Epoch 685 | train loss 0.4041 acc 0.8585 | val loss 0.4038 acc 0.8585\n",
      "Epoch 686 | train loss 0.4042 acc 0.8585 | val loss 0.4037 acc 0.8586\n",
      "Epoch 687 | train loss 0.4042 acc 0.8586 | val loss 0.4037 acc 0.8586\n",
      "Epoch 688 | train loss 0.4042 acc 0.8585 | val loss 0.4037 acc 0.8587\n",
      "Epoch 689 | train loss 0.4041 acc 0.8586 | val loss 0.4038 acc 0.8584\n",
      "Epoch 690 | train loss 0.4042 acc 0.8584 | val loss 0.4037 acc 0.8587\n",
      "Epoch 691 | train loss 0.4042 acc 0.8581 | val loss 0.4038 acc 0.8586\n",
      "Epoch 692 | train loss 0.4041 acc 0.8583 | val loss 0.4038 acc 0.8584\n",
      "Epoch 693 | train loss 0.4042 acc 0.8585 | val loss 0.4037 acc 0.8585\n",
      "Epoch 694 | train loss 0.4042 acc 0.8586 | val loss 0.4037 acc 0.8586\n",
      "Epoch 695 | train loss 0.4041 acc 0.8587 | val loss 0.4037 acc 0.8586\n",
      "Epoch 696 | train loss 0.4041 acc 0.8586 | val loss 0.4038 acc 0.8582\n",
      "Epoch 697 | train loss 0.4042 acc 0.8583 | val loss 0.4037 acc 0.8585\n",
      "Epoch 698 | train loss 0.4041 acc 0.8586 | val loss 0.4038 acc 0.8586\n",
      "Epoch 699 | train loss 0.4042 acc 0.8589 | val loss 0.4037 acc 0.8589\n",
      "Epoch 700 | train loss 0.4042 acc 0.8583 | val loss 0.4038 acc 0.8586\n",
      "Epoch 701 | train loss 0.4042 acc 0.8584 | val loss 0.4037 acc 0.8589\n",
      "Epoch 702 | train loss 0.4042 acc 0.8586 | val loss 0.4037 acc 0.8586\n",
      "Epoch 703 | train loss 0.4042 acc 0.8589 | val loss 0.4037 acc 0.8587\n",
      "Epoch 704 | train loss 0.4041 acc 0.8590 | val loss 0.4038 acc 0.8587\n",
      "Epoch 705 | train loss 0.4042 acc 0.8585 | val loss 0.4037 acc 0.8586\n",
      "Epoch 706 | train loss 0.4041 acc 0.8588 | val loss 0.4038 acc 0.8587\n",
      "Epoch 707 | train loss 0.4042 acc 0.8583 | val loss 0.4037 acc 0.8588\n",
      "Epoch 708 | train loss 0.4042 acc 0.8586 | val loss 0.4037 acc 0.8583\n",
      "Epoch 709 | train loss 0.4041 acc 0.8584 | val loss 0.4037 acc 0.8590\n",
      "Epoch 710 | train loss 0.4041 acc 0.8586 | val loss 0.4038 acc 0.8586\n",
      "Epoch 711 | train loss 0.4041 acc 0.8587 | val loss 0.4037 acc 0.8588\n",
      "Epoch 712 | train loss 0.4041 acc 0.8581 | val loss 0.4038 acc 0.8586\n",
      "Epoch 713 | train loss 0.4041 acc 0.8587 | val loss 0.4037 acc 0.8588\n",
      "Epoch 714 | train loss 0.4041 acc 0.8586 | val loss 0.4037 acc 0.8589\n",
      "Epoch 715 | train loss 0.4041 acc 0.8586 | val loss 0.4037 acc 0.8587\n",
      "Epoch 716 | train loss 0.4041 acc 0.8585 | val loss 0.4037 acc 0.8584\n",
      "Epoch 717 | train loss 0.4041 acc 0.8585 | val loss 0.4037 acc 0.8587\n",
      "Epoch 718 | train loss 0.4041 acc 0.8584 | val loss 0.4037 acc 0.8584\n",
      "Epoch 719 | train loss 0.4041 acc 0.8586 | val loss 0.4037 acc 0.8588\n",
      "Epoch 720 | train loss 0.4041 acc 0.8585 | val loss 0.4037 acc 0.8586\n",
      "Epoch 721 | train loss 0.4041 acc 0.8589 | val loss 0.4038 acc 0.8583\n",
      "Epoch 722 | train loss 0.4041 acc 0.8587 | val loss 0.4037 acc 0.8588\n",
      "Epoch 723 | train loss 0.4041 acc 0.8582 | val loss 0.4037 acc 0.8586\n",
      "Epoch 724 | train loss 0.4041 acc 0.8590 | val loss 0.4037 acc 0.8584\n",
      "Epoch 725 | train loss 0.4041 acc 0.8587 | val loss 0.4037 acc 0.8586\n",
      "Epoch 726 | train loss 0.4041 acc 0.8584 | val loss 0.4037 acc 0.8591\n",
      "Epoch 727 | train loss 0.4041 acc 0.8584 | val loss 0.4037 acc 0.8587\n",
      "Epoch 728 | train loss 0.4041 acc 0.8580 | val loss 0.4038 acc 0.8586\n",
      "Epoch 729 | train loss 0.4042 acc 0.8579 | val loss 0.4037 acc 0.8588\n",
      "Epoch 730 | train loss 0.4041 acc 0.8587 | val loss 0.4037 acc 0.8587\n",
      "Epoch 731 | train loss 0.4041 acc 0.8586 | val loss 0.4038 acc 0.8584\n",
      "Epoch 732 | train loss 0.4041 acc 0.8587 | val loss 0.4037 acc 0.8587\n",
      "Epoch 733 | train loss 0.4041 acc 0.8586 | val loss 0.4037 acc 0.8587\n",
      "Epoch 734 | train loss 0.4041 acc 0.8585 | val loss 0.4037 acc 0.8587\n",
      "Epoch 735 | train loss 0.4041 acc 0.8588 | val loss 0.4037 acc 0.8589\n",
      "Epoch 736 | train loss 0.4041 acc 0.8587 | val loss 0.4037 acc 0.8584\n",
      "Epoch 737 | train loss 0.4041 acc 0.8586 | val loss 0.4037 acc 0.8590\n",
      "Epoch 738 | train loss 0.4041 acc 0.8588 | val loss 0.4037 acc 0.8586\n",
      "Epoch 739 | train loss 0.4041 acc 0.8589 | val loss 0.4037 acc 0.8587\n",
      "Epoch 740 | train loss 0.4041 acc 0.8589 | val loss 0.4037 acc 0.8589\n",
      "Epoch 741 | train loss 0.4041 acc 0.8584 | val loss 0.4037 acc 0.8582\n",
      "Epoch 742 | train loss 0.4041 acc 0.8579 | val loss 0.4037 acc 0.8588\n",
      "Epoch 743 | train loss 0.4041 acc 0.8585 | val loss 0.4037 acc 0.8588\n",
      "Epoch 744 | train loss 0.4041 acc 0.8587 | val loss 0.4037 acc 0.8589\n",
      "Epoch 745 | train loss 0.4042 acc 0.8585 | val loss 0.4037 acc 0.8591\n",
      "Epoch 746 | train loss 0.4041 acc 0.8582 | val loss 0.4038 acc 0.8588\n",
      "Epoch 747 | train loss 0.4042 acc 0.8585 | val loss 0.4037 acc 0.8589\n",
      "Epoch 748 | train loss 0.4041 acc 0.8586 | val loss 0.4037 acc 0.8586\n",
      "Epoch 749 | train loss 0.4041 acc 0.8586 | val loss 0.4037 acc 0.8588\n",
      "Epoch 750 | train loss 0.4041 acc 0.8586 | val loss 0.4037 acc 0.8587\n",
      "Epoch 751 | train loss 0.4041 acc 0.8584 | val loss 0.4037 acc 0.8589\n",
      "Epoch 752 | train loss 0.4040 acc 0.8586 | val loss 0.4038 acc 0.8583\n",
      "Epoch 753 | train loss 0.4041 acc 0.8584 | val loss 0.4037 acc 0.8588\n",
      "Epoch 754 | train loss 0.4041 acc 0.8587 | val loss 0.4037 acc 0.8587\n",
      "Epoch 755 | train loss 0.4041 acc 0.8587 | val loss 0.4037 acc 0.8585\n",
      "Epoch 756 | train loss 0.4041 acc 0.8580 | val loss 0.4037 acc 0.8585\n",
      "Epoch 757 | train loss 0.4041 acc 0.8589 | val loss 0.4037 acc 0.8587\n",
      "Epoch 758 | train loss 0.4041 acc 0.8580 | val loss 0.4037 acc 0.8586\n",
      "Epoch 759 | train loss 0.4041 acc 0.8587 | val loss 0.4037 acc 0.8592\n",
      "Epoch 760 | train loss 0.4041 acc 0.8586 | val loss 0.4037 acc 0.8584\n",
      "Epoch 761 | train loss 0.4041 acc 0.8586 | val loss 0.4037 acc 0.8582\n",
      "Epoch 762 | train loss 0.4042 acc 0.8587 | val loss 0.4037 acc 0.8585\n",
      "Epoch 763 | train loss 0.4041 acc 0.8586 | val loss 0.4037 acc 0.8589\n",
      "Epoch 764 | train loss 0.4041 acc 0.8591 | val loss 0.4037 acc 0.8583\n",
      "Epoch 765 | train loss 0.4041 acc 0.8584 | val loss 0.4037 acc 0.8585\n",
      "Epoch 766 | train loss 0.4041 acc 0.8586 | val loss 0.4037 acc 0.8584\n",
      "Epoch 767 | train loss 0.4041 acc 0.8586 | val loss 0.4037 acc 0.8584\n",
      "Epoch 768 | train loss 0.4041 acc 0.8584 | val loss 0.4037 acc 0.8585\n",
      "Epoch 769 | train loss 0.4041 acc 0.8587 | val loss 0.4037 acc 0.8590\n",
      "Epoch 770 | train loss 0.4041 acc 0.8584 | val loss 0.4037 acc 0.8588\n",
      "Epoch 771 | train loss 0.4041 acc 0.8585 | val loss 0.4037 acc 0.8587\n",
      "Epoch 772 | train loss 0.4041 acc 0.8589 | val loss 0.4037 acc 0.8585\n",
      "Epoch 773 | train loss 0.4041 acc 0.8585 | val loss 0.4037 acc 0.8590\n",
      "Epoch 774 | train loss 0.4041 acc 0.8583 | val loss 0.4037 acc 0.8586\n",
      "Epoch 775 | train loss 0.4041 acc 0.8585 | val loss 0.4037 acc 0.8584\n",
      "Epoch 776 | train loss 0.4042 acc 0.8582 | val loss 0.4037 acc 0.8586\n",
      "Epoch 777 | train loss 0.4041 acc 0.8587 | val loss 0.4037 acc 0.8584\n",
      "Epoch 778 | train loss 0.4041 acc 0.8587 | val loss 0.4037 acc 0.8589\n",
      "Epoch 779 | train loss 0.4041 acc 0.8585 | val loss 0.4037 acc 0.8588\n",
      "Epoch 780 | train loss 0.4040 acc 0.8583 | val loss 0.4038 acc 0.8589\n",
      "Epoch 781 | train loss 0.4041 acc 0.8586 | val loss 0.4037 acc 0.8583\n",
      "Epoch 782 | train loss 0.4041 acc 0.8588 | val loss 0.4037 acc 0.8584\n",
      "Epoch 783 | train loss 0.4041 acc 0.8584 | val loss 0.4037 acc 0.8587\n",
      "Epoch 784 | train loss 0.4040 acc 0.8590 | val loss 0.4037 acc 0.8587\n",
      "Epoch 785 | train loss 0.4041 acc 0.8587 | val loss 0.4037 acc 0.8589\n",
      "Epoch 786 | train loss 0.4041 acc 0.8583 | val loss 0.4037 acc 0.8589\n",
      "Epoch 787 | train loss 0.4040 acc 0.8586 | val loss 0.4038 acc 0.8591\n",
      "Epoch 788 | train loss 0.4040 acc 0.8585 | val loss 0.4038 acc 0.8584\n",
      "Epoch 789 | train loss 0.4041 acc 0.8581 | val loss 0.4037 acc 0.8588\n",
      "Epoch 790 | train loss 0.4041 acc 0.8583 | val loss 0.4037 acc 0.8588\n",
      "Epoch 791 | train loss 0.4041 acc 0.8583 | val loss 0.4038 acc 0.8585\n",
      "Epoch 792 | train loss 0.4041 acc 0.8586 | val loss 0.4037 acc 0.8588\n",
      "Epoch 793 | train loss 0.4041 acc 0.8585 | val loss 0.4037 acc 0.8586\n",
      "Epoch 794 | train loss 0.4041 acc 0.8583 | val loss 0.4037 acc 0.8584\n",
      "Epoch 795 | train loss 0.4041 acc 0.8583 | val loss 0.4038 acc 0.8587\n",
      "Epoch 796 | train loss 0.4041 acc 0.8589 | val loss 0.4037 acc 0.8586\n",
      "Epoch 797 | train loss 0.4041 acc 0.8579 | val loss 0.4037 acc 0.8589\n",
      "Epoch 798 | train loss 0.4041 acc 0.8590 | val loss 0.4037 acc 0.8586\n",
      "Epoch 799 | train loss 0.4041 acc 0.8586 | val loss 0.4037 acc 0.8585\n",
      "Epoch 800 | train loss 0.4041 acc 0.8583 | val loss 0.4037 acc 0.8589\n",
      "Epoch 801 | train loss 0.4040 acc 0.8587 | val loss 0.4038 acc 0.8587\n",
      "Epoch 802 | train loss 0.4041 acc 0.8587 | val loss 0.4037 acc 0.8584\n",
      "Epoch 803 | train loss 0.4041 acc 0.8584 | val loss 0.4037 acc 0.8590\n",
      "Epoch 804 | train loss 0.4041 acc 0.8586 | val loss 0.4037 acc 0.8589\n",
      "Epoch 805 | train loss 0.4041 acc 0.8586 | val loss 0.4037 acc 0.8590\n",
      "Epoch 806 | train loss 0.4041 acc 0.8585 | val loss 0.4037 acc 0.8589\n",
      "Epoch 807 | train loss 0.4041 acc 0.8586 | val loss 0.4037 acc 0.8589\n",
      "Epoch 808 | train loss 0.4041 acc 0.8587 | val loss 0.4037 acc 0.8586\n",
      "Epoch 809 | train loss 0.4041 acc 0.8585 | val loss 0.4037 acc 0.8585\n",
      "Epoch 810 | train loss 0.4040 acc 0.8580 | val loss 0.4036 acc 0.8589\n",
      "Epoch 811 | train loss 0.4041 acc 0.8585 | val loss 0.4037 acc 0.8588\n",
      "Epoch 812 | train loss 0.4041 acc 0.8585 | val loss 0.4036 acc 0.8588\n",
      "Epoch 813 | train loss 0.4041 acc 0.8585 | val loss 0.4037 acc 0.8586\n",
      "Epoch 814 | train loss 0.4041 acc 0.8587 | val loss 0.4037 acc 0.8587\n",
      "Epoch 815 | train loss 0.4041 acc 0.8585 | val loss 0.4037 acc 0.8585\n",
      "Epoch 816 | train loss 0.4041 acc 0.8586 | val loss 0.4037 acc 0.8587\n",
      "Epoch 817 | train loss 0.4041 acc 0.8582 | val loss 0.4038 acc 0.8587\n",
      "Epoch 818 | train loss 0.4041 acc 0.8583 | val loss 0.4036 acc 0.8589\n",
      "Epoch 819 | train loss 0.4040 acc 0.8589 | val loss 0.4037 acc 0.8583\n",
      "Epoch 820 | train loss 0.4040 acc 0.8586 | val loss 0.4038 acc 0.8579\n",
      "Epoch 821 | train loss 0.4041 acc 0.8584 | val loss 0.4037 acc 0.8587\n",
      "Epoch 822 | train loss 0.4041 acc 0.8585 | val loss 0.4036 acc 0.8588\n",
      "Epoch 823 | train loss 0.4040 acc 0.8584 | val loss 0.4037 acc 0.8590\n",
      "Epoch 824 | train loss 0.4041 acc 0.8590 | val loss 0.4037 acc 0.8586\n",
      "Epoch 825 | train loss 0.4040 acc 0.8586 | val loss 0.4037 acc 0.8589\n",
      "Epoch 826 | train loss 0.4040 acc 0.8586 | val loss 0.4036 acc 0.8586\n",
      "Epoch 827 | train loss 0.4040 acc 0.8586 | val loss 0.4037 acc 0.8583\n",
      "Epoch 828 | train loss 0.4041 acc 0.8585 | val loss 0.4036 acc 0.8591\n",
      "Epoch 829 | train loss 0.4040 acc 0.8587 | val loss 0.4037 acc 0.8585\n",
      "Epoch 830 | train loss 0.4041 acc 0.8584 | val loss 0.4037 acc 0.8586\n",
      "Epoch 831 | train loss 0.4041 acc 0.8583 | val loss 0.4036 acc 0.8585\n",
      "Epoch 832 | train loss 0.4040 acc 0.8587 | val loss 0.4037 acc 0.8586\n",
      "Epoch 833 | train loss 0.4040 acc 0.8584 | val loss 0.4036 acc 0.8587\n",
      "Epoch 834 | train loss 0.4041 acc 0.8586 | val loss 0.4037 acc 0.8588\n",
      "Epoch 835 | train loss 0.4040 acc 0.8584 | val loss 0.4037 acc 0.8589\n",
      "Epoch 836 | train loss 0.4041 acc 0.8585 | val loss 0.4036 acc 0.8588\n",
      "Epoch 837 | train loss 0.4041 acc 0.8590 | val loss 0.4036 acc 0.8587\n",
      "Epoch 838 | train loss 0.4041 acc 0.8586 | val loss 0.4037 acc 0.8587\n",
      "Epoch 839 | train loss 0.4040 acc 0.8584 | val loss 0.4037 acc 0.8588\n",
      "Epoch 840 | train loss 0.4040 acc 0.8587 | val loss 0.4037 acc 0.8587\n",
      "Epoch 841 | train loss 0.4040 acc 0.8586 | val loss 0.4037 acc 0.8589\n",
      "Epoch 842 | train loss 0.4041 acc 0.8585 | val loss 0.4036 acc 0.8589\n",
      "Epoch 843 | train loss 0.4041 acc 0.8588 | val loss 0.4037 acc 0.8586\n",
      "Epoch 844 | train loss 0.4041 acc 0.8592 | val loss 0.4036 acc 0.8589\n",
      "Epoch 845 | train loss 0.4041 acc 0.8586 | val loss 0.4036 acc 0.8590\n",
      "Epoch 846 | train loss 0.4040 acc 0.8589 | val loss 0.4037 acc 0.8584\n",
      "Epoch 847 | train loss 0.4041 acc 0.8586 | val loss 0.4036 acc 0.8587\n",
      "Epoch 848 | train loss 0.4041 acc 0.8586 | val loss 0.4036 acc 0.8586\n",
      "Epoch 849 | train loss 0.4040 acc 0.8588 | val loss 0.4036 acc 0.8584\n",
      "Epoch 850 | train loss 0.4041 acc 0.8584 | val loss 0.4036 acc 0.8589\n",
      "Epoch 851 | train loss 0.4040 acc 0.8586 | val loss 0.4036 acc 0.8587\n",
      "Epoch 852 | train loss 0.4040 acc 0.8587 | val loss 0.4036 acc 0.8590\n",
      "Epoch 853 | train loss 0.4039 acc 0.8584 | val loss 0.4038 acc 0.8582\n",
      "Epoch 854 | train loss 0.4041 acc 0.8585 | val loss 0.4036 acc 0.8584\n",
      "Epoch 855 | train loss 0.4040 acc 0.8590 | val loss 0.4036 acc 0.8586\n",
      "Epoch 856 | train loss 0.4040 acc 0.8581 | val loss 0.4037 acc 0.8587\n",
      "Epoch 857 | train loss 0.4041 acc 0.8588 | val loss 0.4036 acc 0.8587\n",
      "Epoch 858 | train loss 0.4040 acc 0.8584 | val loss 0.4036 acc 0.8589\n",
      "Epoch 859 | train loss 0.4040 acc 0.8586 | val loss 0.4037 acc 0.8588\n",
      "Epoch 860 | train loss 0.4041 acc 0.8591 | val loss 0.4036 acc 0.8589\n",
      "Epoch 861 | train loss 0.4041 acc 0.8584 | val loss 0.4036 acc 0.8588\n",
      "Epoch 862 | train loss 0.4041 acc 0.8582 | val loss 0.4037 acc 0.8584\n",
      "Epoch 863 | train loss 0.4040 acc 0.8586 | val loss 0.4036 acc 0.8589\n",
      "Epoch 864 | train loss 0.4041 acc 0.8591 | val loss 0.4036 acc 0.8589\n",
      "Epoch 865 | train loss 0.4040 acc 0.8584 | val loss 0.4037 acc 0.8583\n",
      "Epoch 866 | train loss 0.4040 acc 0.8589 | val loss 0.4036 acc 0.8586\n",
      "Epoch 867 | train loss 0.4041 acc 0.8592 | val loss 0.4036 acc 0.8586\n",
      "Epoch 868 | train loss 0.4041 acc 0.8583 | val loss 0.4036 acc 0.8588\n",
      "Epoch 869 | train loss 0.4041 acc 0.8589 | val loss 0.4036 acc 0.8588\n",
      "Epoch 870 | train loss 0.4040 acc 0.8588 | val loss 0.4036 acc 0.8586\n",
      "Epoch 871 | train loss 0.4040 acc 0.8583 | val loss 0.4036 acc 0.8589\n",
      "Epoch 872 | train loss 0.4040 acc 0.8590 | val loss 0.4036 acc 0.8587\n",
      "Epoch 873 | train loss 0.4040 acc 0.8585 | val loss 0.4037 acc 0.8587\n",
      "Epoch 874 | train loss 0.4040 acc 0.8586 | val loss 0.4036 acc 0.8589\n",
      "Epoch 875 | train loss 0.4040 acc 0.8589 | val loss 0.4037 acc 0.8589\n",
      "Epoch 876 | train loss 0.4040 acc 0.8586 | val loss 0.4036 acc 0.8591\n",
      "Epoch 877 | train loss 0.4040 acc 0.8587 | val loss 0.4037 acc 0.8584\n",
      "Epoch 878 | train loss 0.4040 acc 0.8589 | val loss 0.4037 acc 0.8586\n",
      "Epoch 879 | train loss 0.4041 acc 0.8587 | val loss 0.4036 acc 0.8590\n",
      "Epoch 880 | train loss 0.4040 acc 0.8589 | val loss 0.4036 acc 0.8589\n",
      "Epoch 881 | train loss 0.4040 acc 0.8585 | val loss 0.4037 acc 0.8584\n",
      "Epoch 882 | train loss 0.4040 acc 0.8586 | val loss 0.4037 acc 0.8589\n",
      "Epoch 883 | train loss 0.4041 acc 0.8590 | val loss 0.4036 acc 0.8587\n",
      "Epoch 884 | train loss 0.4040 acc 0.8584 | val loss 0.4036 acc 0.8588\n",
      "Epoch 885 | train loss 0.4040 acc 0.8585 | val loss 0.4036 acc 0.8587\n",
      "Epoch 886 | train loss 0.4040 acc 0.8590 | val loss 0.4037 acc 0.8583\n",
      "Epoch 887 | train loss 0.4040 acc 0.8581 | val loss 0.4036 acc 0.8587\n",
      "Epoch 888 | train loss 0.4040 acc 0.8587 | val loss 0.4036 acc 0.8591\n",
      "Epoch 889 | train loss 0.4040 acc 0.8587 | val loss 0.4038 acc 0.8587\n",
      "Epoch 890 | train loss 0.4040 acc 0.8586 | val loss 0.4036 acc 0.8589\n",
      "Epoch 891 | train loss 0.4040 acc 0.8589 | val loss 0.4037 acc 0.8582\n",
      "Epoch 892 | train loss 0.4039 acc 0.8587 | val loss 0.4037 acc 0.8588\n",
      "Epoch 893 | train loss 0.4040 acc 0.8588 | val loss 0.4037 acc 0.8587\n",
      "Epoch 894 | train loss 0.4040 acc 0.8589 | val loss 0.4036 acc 0.8590\n",
      "Epoch 895 | train loss 0.4040 acc 0.8586 | val loss 0.4037 acc 0.8589\n",
      "Epoch 896 | train loss 0.4040 acc 0.8591 | val loss 0.4036 acc 0.8589\n",
      "Epoch 897 | train loss 0.4040 acc 0.8586 | val loss 0.4036 acc 0.8589\n",
      "Epoch 898 | train loss 0.4041 acc 0.8588 | val loss 0.4037 acc 0.8583\n",
      "Epoch 899 | train loss 0.4040 acc 0.8586 | val loss 0.4036 acc 0.8585\n",
      "Epoch 900 | train loss 0.4040 acc 0.8586 | val loss 0.4036 acc 0.8589\n",
      "Epoch 901 | train loss 0.4040 acc 0.8585 | val loss 0.4036 acc 0.8587\n",
      "Epoch 902 | train loss 0.4040 acc 0.8585 | val loss 0.4036 acc 0.8588\n",
      "Epoch 903 | train loss 0.4040 acc 0.8585 | val loss 0.4036 acc 0.8587\n",
      "Epoch 904 | train loss 0.4040 acc 0.8588 | val loss 0.4036 acc 0.8583\n",
      "Epoch 905 | train loss 0.4040 acc 0.8590 | val loss 0.4036 acc 0.8587\n",
      "Epoch 906 | train loss 0.4040 acc 0.8585 | val loss 0.4036 acc 0.8586\n",
      "Epoch 907 | train loss 0.4040 acc 0.8587 | val loss 0.4036 acc 0.8588\n",
      "Epoch 908 | train loss 0.4040 acc 0.8586 | val loss 0.4036 acc 0.8584\n",
      "Epoch 909 | train loss 0.4040 acc 0.8583 | val loss 0.4036 acc 0.8586\n",
      "Epoch 910 | train loss 0.4041 acc 0.8585 | val loss 0.4036 acc 0.8592\n",
      "Epoch 911 | train loss 0.4040 acc 0.8584 | val loss 0.4037 acc 0.8583\n",
      "Epoch 912 | train loss 0.4040 acc 0.8585 | val loss 0.4036 acc 0.8586\n",
      "Epoch 913 | train loss 0.4040 acc 0.8582 | val loss 0.4036 acc 0.8588\n",
      "Epoch 914 | train loss 0.4040 acc 0.8586 | val loss 0.4036 acc 0.8584\n",
      "Epoch 915 | train loss 0.4039 acc 0.8584 | val loss 0.4037 acc 0.8584\n",
      "Epoch 916 | train loss 0.4040 acc 0.8581 | val loss 0.4036 acc 0.8589\n",
      "Epoch 917 | train loss 0.4040 acc 0.8583 | val loss 0.4036 acc 0.8587\n",
      "Epoch 918 | train loss 0.4040 acc 0.8586 | val loss 0.4036 acc 0.8588\n",
      "Epoch 919 | train loss 0.4040 acc 0.8586 | val loss 0.4036 acc 0.8586\n",
      "Epoch 920 | train loss 0.4040 acc 0.8589 | val loss 0.4036 acc 0.8590\n",
      "Epoch 921 | train loss 0.4040 acc 0.8588 | val loss 0.4036 acc 0.8585\n",
      "Epoch 922 | train loss 0.4040 acc 0.8589 | val loss 0.4036 acc 0.8588\n",
      "Epoch 923 | train loss 0.4040 acc 0.8586 | val loss 0.4036 acc 0.8591\n",
      "Epoch 924 | train loss 0.4040 acc 0.8588 | val loss 0.4037 acc 0.8583\n",
      "Epoch 925 | train loss 0.4040 acc 0.8584 | val loss 0.4036 acc 0.8586\n",
      "Epoch 926 | train loss 0.4040 acc 0.8588 | val loss 0.4036 acc 0.8586\n",
      "Epoch 927 | train loss 0.4040 acc 0.8581 | val loss 0.4036 acc 0.8591\n",
      "Epoch 928 | train loss 0.4040 acc 0.8586 | val loss 0.4036 acc 0.8590\n",
      "Epoch 929 | train loss 0.4040 acc 0.8587 | val loss 0.4036 acc 0.8590\n",
      "Epoch 930 | train loss 0.4040 acc 0.8583 | val loss 0.4036 acc 0.8587\n",
      "Epoch 931 | train loss 0.4040 acc 0.8587 | val loss 0.4036 acc 0.8590\n",
      "Epoch 932 | train loss 0.4040 acc 0.8586 | val loss 0.4036 acc 0.8588\n",
      "Epoch 933 | train loss 0.4040 acc 0.8585 | val loss 0.4036 acc 0.8588\n",
      "Epoch 934 | train loss 0.4041 acc 0.8584 | val loss 0.4036 acc 0.8590\n",
      "Epoch 935 | train loss 0.4040 acc 0.8582 | val loss 0.4036 acc 0.8587\n",
      "Epoch 936 | train loss 0.4040 acc 0.8586 | val loss 0.4036 acc 0.8586\n",
      "Epoch 937 | train loss 0.4040 acc 0.8590 | val loss 0.4036 acc 0.8585\n",
      "Epoch 938 | train loss 0.4040 acc 0.8588 | val loss 0.4036 acc 0.8591\n",
      "Epoch 939 | train loss 0.4040 acc 0.8587 | val loss 0.4036 acc 0.8584\n",
      "Epoch 940 | train loss 0.4040 acc 0.8590 | val loss 0.4036 acc 0.8587\n",
      "Epoch 941 | train loss 0.4040 acc 0.8587 | val loss 0.4036 acc 0.8587\n",
      "Epoch 942 | train loss 0.4040 acc 0.8584 | val loss 0.4036 acc 0.8589\n",
      "Epoch 943 | train loss 0.4040 acc 0.8588 | val loss 0.4036 acc 0.8587\n",
      "Epoch 944 | train loss 0.4040 acc 0.8586 | val loss 0.4036 acc 0.8589\n",
      "Epoch 945 | train loss 0.4040 acc 0.8585 | val loss 0.4036 acc 0.8589\n",
      "Epoch 946 | train loss 0.4040 acc 0.8586 | val loss 0.4036 acc 0.8586\n",
      "Epoch 947 | train loss 0.4040 acc 0.8586 | val loss 0.4036 acc 0.8587\n",
      "Epoch 948 | train loss 0.4040 acc 0.8584 | val loss 0.4036 acc 0.8588\n",
      "Epoch 949 | train loss 0.4040 acc 0.8589 | val loss 0.4036 acc 0.8588\n",
      "Epoch 950 | train loss 0.4040 acc 0.8583 | val loss 0.4036 acc 0.8588\n",
      "Epoch 951 | train loss 0.4040 acc 0.8584 | val loss 0.4036 acc 0.8589\n",
      "Epoch 952 | train loss 0.4040 acc 0.8591 | val loss 0.4036 acc 0.8591\n",
      "Epoch 953 | train loss 0.4040 acc 0.8586 | val loss 0.4036 acc 0.8587\n",
      "Epoch 954 | train loss 0.4040 acc 0.8590 | val loss 0.4036 acc 0.8590\n",
      "Epoch 955 | train loss 0.4040 acc 0.8584 | val loss 0.4036 acc 0.8586\n",
      "Epoch 956 | train loss 0.4040 acc 0.8586 | val loss 0.4037 acc 0.8584\n",
      "Epoch 957 | train loss 0.4041 acc 0.8581 | val loss 0.4036 acc 0.8584\n",
      "Epoch 958 | train loss 0.4040 acc 0.8587 | val loss 0.4036 acc 0.8583\n",
      "Epoch 959 | train loss 0.4040 acc 0.8585 | val loss 0.4036 acc 0.8583\n",
      "Epoch 960 | train loss 0.4040 acc 0.8586 | val loss 0.4036 acc 0.8588\n",
      "Epoch 961 | train loss 0.4040 acc 0.8588 | val loss 0.4036 acc 0.8586\n",
      "Epoch 962 | train loss 0.4040 acc 0.8584 | val loss 0.4036 acc 0.8589\n",
      "Epoch 963 | train loss 0.4039 acc 0.8586 | val loss 0.4036 acc 0.8586\n",
      "Epoch 964 | train loss 0.4040 acc 0.8585 | val loss 0.4036 acc 0.8584\n",
      "Epoch 965 | train loss 0.4040 acc 0.8586 | val loss 0.4036 acc 0.8589\n",
      "Epoch 966 | train loss 0.4040 acc 0.8589 | val loss 0.4036 acc 0.8588\n",
      "Epoch 967 | train loss 0.4040 acc 0.8587 | val loss 0.4036 acc 0.8589\n",
      "Epoch 968 | train loss 0.4040 acc 0.8589 | val loss 0.4036 acc 0.8587\n",
      "Epoch 969 | train loss 0.4039 acc 0.8583 | val loss 0.4036 acc 0.8589\n",
      "Epoch 970 | train loss 0.4040 acc 0.8586 | val loss 0.4036 acc 0.8588\n",
      "Epoch 971 | train loss 0.4040 acc 0.8586 | val loss 0.4036 acc 0.8589\n",
      "Epoch 972 | train loss 0.4040 acc 0.8589 | val loss 0.4036 acc 0.8586\n",
      "Epoch 973 | train loss 0.4039 acc 0.8582 | val loss 0.4037 acc 0.8587\n",
      "Epoch 974 | train loss 0.4040 acc 0.8588 | val loss 0.4036 acc 0.8587\n",
      "Epoch 975 | train loss 0.4040 acc 0.8587 | val loss 0.4036 acc 0.8586\n",
      "Epoch 976 | train loss 0.4040 acc 0.8583 | val loss 0.4036 acc 0.8587\n",
      "Epoch 977 | train loss 0.4040 acc 0.8586 | val loss 0.4036 acc 0.8591\n",
      "Epoch 978 | train loss 0.4040 acc 0.8584 | val loss 0.4036 acc 0.8591\n",
      "Epoch 979 | train loss 0.4040 acc 0.8586 | val loss 0.4036 acc 0.8590\n",
      "Epoch 980 | train loss 0.4039 acc 0.8581 | val loss 0.4037 acc 0.8583\n",
      "Epoch 981 | train loss 0.4040 acc 0.8584 | val loss 0.4036 acc 0.8587\n",
      "Epoch 982 | train loss 0.4040 acc 0.8587 | val loss 0.4036 acc 0.8584\n",
      "Epoch 983 | train loss 0.4040 acc 0.8588 | val loss 0.4036 acc 0.8585\n",
      "Epoch 984 | train loss 0.4040 acc 0.8588 | val loss 0.4036 acc 0.8590\n",
      "Epoch 985 | train loss 0.4040 acc 0.8587 | val loss 0.4036 acc 0.8587\n",
      "Epoch 986 | train loss 0.4040 acc 0.8582 | val loss 0.4036 acc 0.8589\n",
      "Epoch 987 | train loss 0.4040 acc 0.8587 | val loss 0.4036 acc 0.8589\n",
      "Epoch 988 | train loss 0.4040 acc 0.8589 | val loss 0.4036 acc 0.8590\n",
      "Epoch 989 | train loss 0.4040 acc 0.8583 | val loss 0.4036 acc 0.8589\n",
      "Epoch 990 | train loss 0.4039 acc 0.8589 | val loss 0.4036 acc 0.8585\n",
      "Epoch 991 | train loss 0.4040 acc 0.8584 | val loss 0.4036 acc 0.8587\n",
      "Epoch 992 | train loss 0.4040 acc 0.8589 | val loss 0.4036 acc 0.8589\n",
      "Epoch 993 | train loss 0.4039 acc 0.8586 | val loss 0.4036 acc 0.8588\n",
      "Epoch 994 | train loss 0.4040 acc 0.8590 | val loss 0.4036 acc 0.8589\n",
      "Epoch 995 | train loss 0.4040 acc 0.8585 | val loss 0.4036 acc 0.8589\n",
      "Epoch 996 | train loss 0.4040 acc 0.8589 | val loss 0.4036 acc 0.8586\n",
      "Epoch 997 | train loss 0.4040 acc 0.8586 | val loss 0.4036 acc 0.8589\n",
      "Epoch 998 | train loss 0.4040 acc 0.8589 | val loss 0.4036 acc 0.8589\n",
      "Epoch 999 | train loss 0.4040 acc 0.8587 | val loss 0.4036 acc 0.8591\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, 1000):\n",
    "    model.train()\n",
    "    total, correct, total_loss = 0, 0, 0.0\n",
    "    for xb, yb in train_loader:\n",
    "        xb, yb = xb.to(dev), yb.to(dev)\n",
    "        xb.permute(1, 0, 2)\n",
    "        opt.zero_grad(set_to_none=True)\n",
    "        logits = model(xb, rav_tensor=rav_tensor)   # logits from perp(x)\n",
    "        loss = criterion(logits, yb)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "        preds = logits.argmax(dim=1)\n",
    "        correct += (preds == yb).sum().item()\n",
    "        total += yb.size(0)\n",
    "        total_loss += loss.item() * yb.size(0)\n",
    "\n",
    "    train_loss = total_loss/total\n",
    "    train_acc = correct/total\n",
    "    val_loss, val_acc = eval_epoch(train_loader, rav_tensor)\n",
    "\n",
    "    print(f\"Epoch {epoch:02d} | \"\n",
    "          f\"train loss {train_loss:.4f} acc {train_acc:.4f} | \"\n",
    "          f\"val loss {val_loss:.4f} acc {val_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4719ee20",
   "metadata": {},
   "source": [
    "## Evaluate training mass scultping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "24150e91",
   "metadata": {},
   "outputs": [],
   "source": [
    "qcd_sdmass = []\n",
    "for f in root_dir.glob('ZJetsToNuNu_*.root'):\n",
    "    process = f.name.split('_')[0]\n",
    "    tree = uproot.open(f)['tree']\n",
    "\n",
    "    n_entries = tree.num_entries\n",
    "    n_keep = int(n_entries * data_fraction)\n",
    "    qcd_sdmass_f = tree.arrays(\n",
    "        filter_name=['jet_sdmass'],\n",
    "        entry_stop=n_keep,  # only read first fraction\n",
    "        library='np',\n",
    "    )\n",
    "    qcd_sdmass.append(qcd_sdmass_f['jet_sdmass'])\n",
    "qcd_sdmass = np.concatenate(qcd_sdmass, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3340c74c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PosixPath('/Users/billyli/scope/JetClass/minimal/HToCC_120.root'),\n",
       " PosixPath('/Users/billyli/scope/JetClass/minimal/WToQQ_120.root'),\n",
       " PosixPath('/Users/billyli/scope/JetClass/minimal/ZToQQ_120.root'),\n",
       " PosixPath('/Users/billyli/scope/JetClass/minimal/HToBB_120.root'),\n",
       " PosixPath('/Users/billyli/scope/JetClass/minimal/HToGG_120.root'),\n",
       " PosixPath('/Users/billyli/scope/JetClass/minimal/HToWW2Q1L_120.root'),\n",
       " PosixPath('/Users/billyli/scope/JetClass/minimal/TTBarLep_120.root'),\n",
       " PosixPath('/Users/billyli/scope/JetClass/minimal/TTBar_120.root'),\n",
       " PosixPath('/Users/billyli/scope/JetClass/minimal/ZJetsToNuNu_120.root'),\n",
       " PosixPath('/Users/billyli/scope/JetClass/minimal/HToWW4Q_120.root')]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(root_dir.glob('*.root'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "80cfed20",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_tot = hidden_tensor.size()[0]\n",
    "n_qcd_size = int(n_tot/10)\n",
    "n_qcd_start = int(n_tot/10*8)\n",
    "n_qcd_end = n_qcd_start + n_qcd_size\n",
    "hidden_qcd = hidden_tensor[n_qcd_start:n_qcd_end, :, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "cc4c3d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "model.for_inference = True\n",
    "scores = model(hidden_qcd.to(dev), rav_tensor=rav_tensor.to(dev))\n",
    "Xbb = scores[:, 1]\n",
    "QCD = scores[:, 0]\n",
    "XbbvsQCD = Xbb / (Xbb + QCD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "3d5b7a49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f7fb15414d0>"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAiMAAAGdCAYAAADAAnMpAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/NK7nSAAAACXBIWXMAAA9hAAAPYQGoP6dpAAA4CklEQVR4nO3df1RVdb7/8deRACFTMOWHSKAN4S8Uw8HQO1NeuaKVV8cZl+O00tBxpsK1LB1TSjSmq6hdHC0YWWleuzWOTf78ThYNUdg0MDIqrqZSFJVw6ICaIgrKz/39w+vJk4AcEz7+eD7WOmvFZ38+e7/3p4PnxT77h82yLEsAAACGdDBdAAAAuL0RRgAAgFGEEQAAYBRhBAAAGEUYAQAARhFGAACAUYQRAABgFGEEAAAYdYfpAlqjsbFRX3/9te666y7ZbDbT5QAAgFawLEtnz55Vjx491KFD88c/boow8vXXXys4ONh0GQAA4BocO3ZMPXv2bHb5TRFG7rrrLkkXd6Zz586GqwEAAK1RWVmp4OBgx+d4c26KMHLpq5nOnTsTRgAAuMlc7RQLTmAFAABGEUYAAIBRhBEAAGDUTXHOCADg5mZZlurr69XQ0GC6FFxHbm5uuuOOO773bTcIIwCANlVbWyu73a7q6mrTpaANeHt7KzAwUB4eHte8DsIIAKDNNDY26ujRo3Jzc1OPHj3k4eHBzStvEZZlqba2VidOnNDRo0cVFhbW4o3NWkIYAQC0mdraWjU2Nio4OFje3t6my8F15uXlJXd3d3311Veqra1Vx44dr2k9nMAKAGhz1/oXM2581+P/Le8OAABgFF/TAADaXWnFeZ2uqm237fne6aEgH6922x5c43IY+eSTT/Tyyy9rz549stvt2rp1q8aPH9/imJycHM2ePVtffPGFgoODtWDBAj3xxBPXWDIA4GZWWnFesak7db6u/S7z9XJ304dzHiSQ3KBcDiNVVVUaNGiQpk2bpgkTJly1/9GjR/XII4/oySef1B/+8AdlZ2frl7/8pQIDAxUXF3dNRQMAbl6nq2p1vq5BKydF6gd+ndp8e0XHz+mZt/fpdFWtS2GkrKxMixcv1o4dO1RaWio/Pz9FRkbqmWee0ciRI1u1jpycHI0YMUKnT5+Wj4/PNe5B09555x0lJSWpuLhYYWFhWrZsmR5++OGr1vJddrtdAQEB17U2V7kcRsaMGaMxY8a0un9GRoZ69eql1NRUSVLfvn316aef6ne/+x1hBABuYz/w66QBQV1Ml9Gk4uJiDR8+XD4+Pnr55ZcVERGhuro6ffDBB0pISNCBAweM1pebm6vJkycrJSVFjz76qDZs2KDx48dr7969GjBgQItjCwsLnR466+fn19blXlWbnzOSl5en2NhYp7a4uDg988wzzY6pqalRTU2N4+fKysq2Kg/XWVnJIZ07Xd7s8hO1p1RZf67Z5YF3h2rgfcPaojRjPvvbn3Wy5GCzy7vdc58GDh/b5LKrzWcnX38F3BP2vWsE4Ozpp5+WzWZTfn6+7rzzTkd7//79NW3aNEkXA0uvXr1UUFCgyMhISVJFRYV8fX318ccfKzQ01HEkwtfXV5I0depUrV+/Xps2bVJycrKKiork7e2twYMHa/v27U7basmqVas0evRozZ07V5L00ksvKSsrS2lpacrIyGhxrJ+f33U/SvN9tXkYKSsrk7+/v1Obv7+/Kisrdf78eXl5XXnILCUlRcnJyW1dGq6zspJD6vz6cAXYappcbndz0y96Bup8C5eBdWy09Lpeu2UCyWd/+7Man3xOgXXN96lxlz7L0BWB5GrzKUnVlqfKpv+NQAJcR6dOnVJmZqYWL17cZDho7Qd5cHCwNm/erJ/+9KeOoxFeXl6y2+2aPHmyli9frp/85Cc6e/as/vrXv8qyLEnffp1y9OhRhYaGNrnuvLw8zZ4926ktLi5O27Ztu2pdkZGRqqmp0YABA/Tiiy9q+PDhrdqftnRDXk2TmJjoNMmVlZUKDg42WBFa49zpcgXYarT7/mXyCbnyMOHh6hKdP5iiZ+6JV8+OV34/efBf/9Br1R/K/k2xBurWCCMnSw4qsE76/GcDdXf/6CuWf/NFvgZs+kz2koPSd/49uNp8Vnz1uYbsnaevT5dLhBHguikqKpJlWerTp8/3Wo+bm5u6du0qyfloxOHDh1VfX68JEyYoJCREkhQREeEY5+3trfDwcLm7uze77ub+0C8rK2t2TGBgoDIyMjRkyBDV1NRo7dq1euihh7Rr1y7df//917qb10Wbh5GAgACVlzsfZi4vL3ckxKZ4enrK09OzrUtDG/EJGaAfDPq3K9prv/lSOijFDBytfnf3u3JgnqSDH7Z9gQbc3T9a/z55zhXtH/0xVdr0WYtjm5vPIknae50KBOBw6QhFWxk0aJBGjhypiIgIxcXFadSoUfrZz37m+ConOjq6Tc5JCQ8PV3h4uOPnYcOG6fDhw/rd736nN99887pvzxVtftOzmJgYZWdnO7VlZWUpJiamrTcNAIDLwsLCZLPZrhoILt159PLwUlfXwney/8fNzU1ZWVl6//331a9fP7366qsKDw/X0aNHW11jc3/ou3pVTHR0tIqKilwa0xZcDiPnzp3Tvn37tG/fPkkXL93dt2+fSkpKJF38imXKlCmO/k8++aSOHDmi5557TgcOHNDvf/97/elPf9Kzzz57ffYAAIDrqGvXroqLi1N6erqqqqquWF5RUSFJ6t69u6SLl8Zecumz8ZJLT7JtaHC+p4rNZtPw4cOVnJysgoICeXh4aOvWra2u8Xr9ob9v3z4FBga6NKYtuPw1ze7du52uU750bselM4TtdrsjmEhSr169tGPHDj377LNatWqVevbsqbVr13JZLwDc5oqON39lnentpKena/jw4YqOjtZvf/tbDRw4UPX19crKytLq1au1f/9+eXl56YEHHtDSpUvVq1cvHT9+XAsWLHBaT0hIiGw2m9599109/PDD8vLy0hdffKHs7GyNGjVKfn5+2rVrl06cOKG+fftKkvLz8zVlyhRlZ2crKCioyfpmzZqlBx98UKmpqXrkkUe0ceNG7d69W6+99pqjT2JiokpLS/W///u/kqSVK1eqV69e6t+/vy5cuKC1a9fqo48+0l/+8heX5+d6czmMPPTQQy1+n7Z+/fomxxQUFLi6KQDALcj3Tg95ubvpmbf3tds2vdzd5HunR6v79+7dW3v37tXixYs1Z84c2e12de/eXVFRUVq9erWj37p16zR9+nRFRUUpPDxcy5cv16hRoxzLg4KClJycrPnz5ys+Pl5TpkzRvHnz9Mknn2jlypWqrKxUSEiIUlNTHffwqq6uVmFhYYtf+QwbNkwbNmzQggUL9PzzzyssLEzbtm1zusfIdw8O1NbWas6cOSotLZW3t7cGDhyoDz/8sMkbobW3G/JqGgDArSvIx0sfznnwhn82TWBgoNLS0pSWltZsn759+yo3N9ep7bt/sCclJSkpKcmpLTMzs9l1Xu2P/ksmTpyoiRMnNrv8uwcHnnvuOT333HNXXa8JhBEAQLsL8vHiOTFwaPOraQAAAFpCGAEAAEYRRgAAgFGEEQAAYBRhBAAAGEUYAQAARhFGAACAUdxnBADQ/iqOSdXftN/2vO+WfILbb3twCWEEANC+Ko5J6dFSXXX7bdPdW0rIdymQlJWVafHixdqxY4dKS0vl5+enyMhIPfPMMxo5cmSr1pGTk6MRI0bo9OnT8vHxucbim/bOO+8oKSlJxcXFCgsL07Jly/Twww832/+JJ57QG2+8cUV7v3799MUXX1zX2lxFGAEAtK/qby4GkQlrpG73tf32Th6Utsy4uN1WhpHi4mINHz5cPj4+evnllxUREaG6ujp98MEHSkhI0IEDB9q46Jbl5uZq8uTJSklJ0aOPPqoNGzZo/Pjx2rt3r9PzaS63atUqLV261PFzfX29Bg0a1OIt5dsLYQQAYEa3+6QekaaraNLTTz8tm82m/Px83XnnnY72/v37a9q0aZIuBpZevXqpoKBAkZGRkqSKigr5+vrq448/VmhoqOMhdL6+vpK+fcL9pk2blJycrKKiInl7e2vw4MHavn2707ZasmrVKo0ePVpz586VJL300kvKyspSWlqaMjIymhzTpUsXdenSxfHztm3bdPr0acXHx7s2OW2AE1gBALjMqVOnlJmZqYSEhCbDQWu/bgkODtbmzZslSYWFhbLb7Vq1apXsdrsmT56sadOmaf/+/crJydGECRMcD8fLycmRzWZTcXFxs+vOy8tTbGysU1tcXJzy8vJat5OSXn/9dcXGxiokJKTVY9oKR0YAALhMUVGRLMtSnz59vtd63Nzc1LVrV0mSn5+fI8QcPnxY9fX1mjBhgiMIREREOMZ5e3srPDxc7u7uza67rKxM/v7+Tm3+/v4qKytrVW1ff/213n//fW3YsMGVXWozHBkBAOAyl45QtJVBgwZp5MiRioiI0MSJE7VmzRqdPn3asTw6OloHDhxQUFBQm9XwxhtvyMfHR+PHj2+zbbiCMAIAwGXCwsJks9muepJqhw4XP0IvDy91dXVXXb+bm5uysrL0/vvvq1+/fnr11VcVHh6uo0ePtrrGgIAAlZeXO7WVl5crICDgqmMty9K6dev0+OOPy8PDo9XbbEuEEQAALtO1a1fFxcUpPT1dVVVVVyyvqKiQJHXv3l2SZLfbHcv27dvn1PfSh31DQ4NTu81m0/Dhw5WcnKyCggJ5eHho69atra4xJiZG2dnZTm1ZWVmKiYm56tidO3eqqKhI06dPb/X22hphBACA70hPT1dDQ4Oio6O1efNmHTp0SPv379crr7zi+MD38vLSAw88oKVLl2r//v3auXOnFixY4LSekJAQ2Ww2vfvuuzpx4oTOnTunXbt2acmSJdq9e7dKSkq0ZcsWnThxQn379pUk5efnq0+fPiotLW22vlmzZikzM1Opqak6cOCAXnzxRe3evVszZ8509ElMTNSUKVOuGPv6669r6NChzV4CbAInsAIAzDh58IbdTu/evbV3714tXrxYc+bMkd1uV/fu3RUVFaXVq1c7+q1bt07Tp09XVFSUwsPDtXz5co0aNcqxPCgoSMnJyZo/f77i4+M1ZcoUzZs3T5988olWrlypyspKhYSEKDU1VWPGjJEkVVdXq7CwsMWvfIYNG6YNGzZowYIFev755xUWFqZt27Y5BQy73a6SkhKncWfOnNHmzZu1atUql+ekLRFGAADty/vui3dE3TKj/bbp7n1xuy4IDAxUWlqa0tLSmu3Tt29f5ebmOrV99wTYpKQkJSUlObVlZmY2u86HHnqoVSfRTpw4scUblq1fv/6Kti5duqi6uh3vfNtKhBEAQPvyCb54a3aeTYP/QxgBALQ/n2DCARw4gRUAABhFGAEAAEYRRgAAgFGEEQAAYBRhBAAAGEUYAQAARhFGAACAUdxnBADQ7uzn7Dpdc7rdtufr6avAToHttj24hjACAGhX9nN2jds+Tufrz7fbNr3u8NL2cdtdCiRlZWVavHixduzYodLSUvn5+SkyMlLPPPOMRo4c2ap15OTkaMSIETp9+rR8fHyusfqmvfPOO0pKSlJxcbHCwsK0bNkyPfzwwy2OSU9PV1pamoqLi3XPPffohRdeaPJheu2NMAIAaFena07rfP15pfwoRb279G7z7R05c0SJf03U6ZrTrQ4jxcXFGj58uHx8fPTyyy8rIiJCdXV1+uCDD5SQkKADBw60cdUty83N1eTJk5WSkqJHH31UGzZs0Pjx47V3795mn8a7evVqJSYmas2aNfrhD3+o/Px8zZgxQ76+vho7dmw774EzwggAwIjeXXqr3939TJfRpKefflo2m035+fm68847He39+/fXtGnTJF0MLL169VJBQYEiIyMlSRUVFfL19dXHH3+s0NBQjRgxQpLk6+srSZo6darWr1+vTZs2KTk5WUVFRfL29tbgwYO1fft2p221ZNWqVRo9erTmzp0rSXrppZeUlZWltLQ0ZWRkNDnmzTff1K9//WtNmjRJ0sUnE//jH//QsmXLjIcRTmAFAOAyp06dUmZmphISEpoMB639uiU4OFibN2+WJBUWFsput2vVqlWy2+2aPHmypk2bpv379ysnJ0cTJkxwPKk3JydHNptNxcXFza47Ly9PsbGxTm1xcXHKy8trdkxNTY06duzo1Obl5aX8/HzV1dW1ap/aCkdGAAC4TFFRkSzLUp8+fb7Xetzc3NS1a1dJkp+fnyPEHD58WPX19ZowYYJCQkIkSREREY5x3t7eCg8Pl7u7e7PrLisrk7+/v1Obv7+/ysrKmh0TFxentWvXavz48br//vu1Z88erV27VnV1dTp58qQCA82d4MuREQAALnPpCEVbGTRokEaOHKmIiAhNnDhRa9as0enT315ZFB0drQMHDigoKOi6bjcpKUljxozRAw88IHd3d40bN05Tp06VJHXoYDYOEEYAALhMWFiYbDbbVU9SvfQBfnl4ac3XHW5ubsrKytL777+vfv366dVXX1V4eLiOHj3a6hoDAgJUXl7u1FZeXq6AgIBmx3h5eWndunWqrq5WcXGxSkpKFBoaqrvuukvdu3dv9bbbAmEEAIDLdO3aVXFxcUpPT1dVVdUVyysqKiTJ8QFut9sdy/bt2+fU18PDQ5LU0NDg1G6z2TR8+HAlJyeroKBAHh4e2rp1a6trjImJUXZ2tlNbVlaWYmJirjrW3d1dPXv2lJubmzZu3KhHH32UIyMAANxo0tPT1dDQoOjoaG3evFmHDh3S/v379corrzg+8L28vPTAAw9o6dKl2r9/v3bu3KkFCxY4rSckJEQ2m03vvvuuTpw4oXPnzmnXrl1asmSJdu/erZKSEm3ZskUnTpxQ3759JUn5+fnq06ePSktLm61v1qxZyszMVGpqqg4cOKAXX3xRu3fv1syZMx19EhMTne4hcvDgQb311ls6dOiQ8vPz9fOf/1yff/65lixZcj2n7ppwAisAwIgjZ47csNvp3bu39u7dq8WLF2vOnDmy2+3q3r27oqKitHr1ake/devWafr06YqKilJ4eLiWL1+uUaNGOZYHBQUpOTlZ8+fPV3x8vKZMmaJ58+bpk08+0cqVK1VZWamQkBClpqZqzJgxkqTq6moVFha2+JXPsGHDtGHDBi1YsEDPP/+8wsLCtG3bNqd7jNjtdpWUlDh+bmhoUGpqqgoLC+Xu7q4RI0YoNzdXoaGhLs/P9UYYAQC0K19PX3nd4aXEvya22za97vCSr6evS2MCAwOVlpamtLS0Zvv07dtXubm5Tm3fPQE2KSlJSUlJTm2ZmZnNrvOhhx5q1Um0EydO1MSJE5tdvn79+itqLSgouOp6TSCMAADaVWCnQG0ft51n08CBMAIAaHeBnQIJB3DgBFYAAGAUYQQAABhFGAEAAEYRRgAAgFGEEQAAYBRhBAAAGEUYAQAARnGfEQBAu6v7+mvVn26/m57d4esr9x492m17cA1hBADQruq+/lqHH3lU1vnz7bZNm5eX7t3xrkuBpKysTIsXL9aOHTtUWloqPz8/RUZG6plnntHIkSNbtY6cnByNGDFCp0+flo+PzzVW37R33nlHSUlJKi4uVlhYmJYtW6aHH364xTHp6elKS0tTcXGx7rnnHr3wwgtOD9Orq6tTSkqK3njjDZWWlio8PFzLli3T6NGjr2vt30UYAQC0q/rTp2WdP68eLy+XR+/ebb692iNH9PXc51R/+nSrw0hxcbGGDx8uHx8fvfzyy4qIiFBdXZ0++OADJSQk6MCBA21cdctyc3M1efJkpaSk6NFHH9WGDRs0fvx47d271+lheZdbvXq1EhMTtWbNGv3whz9Ufn6+ZsyYIV9fX40dO1aStGDBAr311ltas2aN+vTpow8++EA/+clPlJubq8GDB7fZ/hBGAABGePTuLa/+/U2X0aSnn35aNptN+fn5uvPOOx3t/fv317Rp0yRdDCy9evVSQUGBIiMjJUkVFRXy9fXVxx9/rNDQUI0YMUKS5Ot78SF9U6dO1fr167Vp0yYlJyerqKhI3t7eGjx4sLZv3+60rZasWrVKo0eP1ty5cyVJL730krKyspSWlqaMjIwmx7z55pv69a9/rUmTJkm6+GTif/zjH1q2bJkjjLz55pt64YUXHEdYnnrqKX344YdKTU3VW2+95coUuoQTWAEAuMypU6eUmZmphISEJsNBa79uCQ4O1ubNmyVJhYWFstvtWrVqlex2uyZPnqxp06Zp//79ysnJ0YQJExxP6s3JyZHNZlNxcXGz687Ly1NsbKxTW1xcnPLy8podU1NTo44dOzq1eXl5KT8/X3V1dS32+fTTT1u1z9eKMAIAwGWKiopkWZb69Onzvdbj5uamrl27SpL8/PwUEBCgLl26yG63q76+XhMmTFBoaKgiIiL09NNPq1OnTpIkb29vhYeHy93dvdl1l5WVyd/f36nN399fZWVlzY6Ji4vT2rVrtWfPHlmWpd27d2vt2rWqq6vTyZMnHX1WrFihQ4cOqbGxUVlZWdqyZYvsdvv3mourIYwAAHCZS0co2sqgQYM0cuRIRUREaOLEiVqzZo1OX3ZlUXR0tA4cOKCgoKDrut2kpCSNGTNGDzzwgNzd3TVu3DhNnTpVktShw8U4sGrVKoWFhalPnz7y8PDQzJkzFR8f71jeVggjAABcJiwsTDab7aonqV76gL48vFz6uqMlbm5uysrK0vvvv69+/frp1VdfVXh4uI4ePdrqGgMCAlReXu7UVl5eroCAgGbHeHl5ad26daqurlZxcbFKSkoUGhqqu+66S927d5ckde/eXdu2bVNVVZW++uorHThwQJ06dVLvNj7RmDACAMBlunbtqri4OKWnp6uqquqK5RUVFZLk+AC//CuMffv2OfX18PCQJDU0NDi122w2DR8+XMnJySooKJCHh4e2bt3a6hpjYmKUnZ3t1JaVlaWYmJirjnV3d1fPnj3l5uamjRs36tFHH73iyEfHjh0VFBSk+vp6bd68WePGjWt1bdfimsJIenq6QkND1bFjRw0dOlT5+fkt9l+5cqXCw8Pl5eWl4OBgPfvss7pw4cI1FQwAQFtLT09XQ0ODoqOjtXnzZh06dEj79+/XK6+84vjA9/Ly0gMPPKClS5dq//792rlzpxYsWOC0npCQENlsNr377rs6ceKEzp07p127dmnJkiXavXu3SkpKtGXLFp04cUJ9+/aVJOXn56tPnz4qLS1ttr5Zs2YpMzNTqampOnDggF588UXt3r1bM2fOdPRJTEx0uofIwYMH9dZbb+nQoUPKz8/Xz3/+c33++edasmSJo8+uXbu0ZcsWHTlyRH/96181evRoNTY26rnnnrsu89ocly/tffvttzV79mxlZGRo6NChWrlypeLi4lRYWCg/P78r+m/YsEHz58/XunXrNGzYMB08eFBPPPGEbDabVqxYcV12AgBw86k9cuSG3U7v3r21d+9eLV68WHPmzJHdblf37t0VFRWl1atXO/qtW7dO06dPV1RUlMLDw7V8+XKNGjXKsTwoKEjJycmaP3++4uPjNWXKFM2bN0+ffPKJVq5cqcrKSoWEhCg1NVVjxoyRJFVXV6uwsLDFr3yGDRumDRs2aMGCBXr++ecVFhambdu2Od1jxG63q6SkxPFzQ0ODUlNTVVhYKHd3d40YMUK5ubkKDQ119Llw4YIWLFigI0eOqFOnTnr44Yf15ptvXvcbtn2Xy2FkxYoVmjFjhuLj4yVJGRkZ2rFjh9atW6f58+df0T83N1fDhw/XL37xC0lSaGioJk+erF27dn3P0gEAN6M7fH1l8/LS13Pb9q/ty9m8vHTH/93ro7UCAwOVlpamtLS0Zvv07dtXubm5Tm3fPQE2KSlJSUlJTm2ZmZnNrvOhhx5q1Um0EydO1MSJE5tdvn79+itqLSgoaHGdDz74oL788surbvt6cymM1NbWas+ePUpMTHS0dejQQbGxsc1e2zxs2DC99dZbys/PV3R0tI4cOaL33ntPjz/+eLPbqampUU1NjePnyspKV8oEANzA3Hv00L073uXZNHBwKYycPHlSDQ0NTV7b3NxZx7/4xS908uRJ/du//Zssy1J9fb2efPJJPf/8881uJyUlRcnJya6UBgC4ibj36EE4gEObX02Tk5OjJUuW6Pe//7327t2rLVu2aMeOHXrppZeaHZOYmKgzZ844XseOHWvrMgEAgCEuHRnp1q2b3NzcXLq2OSkpSY8//rh++ctfSpIiIiJUVVWlX/3qV3rhhReavJGKp6enPD09XSkNAADcpFw6MuLh4aGoqCina5sbGxuVnZ3d7LXN1dXVVwQONzc3SW1/lzsAAHDjc/lqmtmzZ2vq1KkaMmSIoqOjtXLlSlVVVTmurpkyZYqCgoKUkpIiSRo7dqxWrFihwYMHa+jQoSoqKlJSUpLGjh3rCCUAgFsbf3zeuq7H/1uXw8ikSZN04sQJLVy4UGVlZYqMjFRmZqbjpNaSkhKnIyELFiyQzWbTggULVFpaqu7du2vs2LFavHjx9y4eAHBju/Swt+rqanl5eRmuBm2hurpaklp8sN/VuBxGJGnmzJlOd3m7XE5OjvMG7rhDixYt0qJFi65lUwCAm5ibm5t8fHx0/PhxSRefSGuz2QxXhevBsixVV1fr+PHj8vHx+V7fdlxTGAEAoLUuXeBwKZDg1uLj49PiA/pagzACAGhTNptNgYGB8vPza9VTbXHzcHd3vy7nfxJGAADtws3NjQsX0KQ2v+kZAABASwgjAADAKMIIAAAwijACAACMIowAAACjCCMAAMAowggAADCKMAIAAIwijAAAAKMIIwAAwCjCCAAAMIowAgAAjCKMAAAAowgjAADAKMIIAAAwijACAACMIowAAACjCCMAAMAowggAADCKMAIAAIwijAAAAKMIIwAAwCjCCAAAMIowAgAAjCKMAAAAowgjAADAKMIIAAAwijACAACMIowAAACjCCMAAMAowggAADCKMAIAAIwijAAAAKMIIwAAwCjCCAAAMIowAgAAjCKMAAAAowgjAADAKMIIAAAwijACAACMIowAAACjCCMAAMAowggAADCKMAIAAIwijAAAAKMIIwAAwCjCCAAAMIowAgAAjCKMAAAAowgjAADAKMIIAAAwijACAACMIowAAACjCCMAAMAowggAADCKMAIAAIy6pjCSnp6u0NBQdezYUUOHDlV+fn6L/SsqKpSQkKDAwEB5enrqvvvu03vvvXdNBQMAgFvLHa4OePvttzV79mxlZGRo6NChWrlypeLi4lRYWCg/P78r+tfW1uo//uM/5Ofnp02bNikoKEhfffWVfHx8rkf9AADgJudyGFmxYoVmzJih+Ph4SVJGRoZ27NihdevWaf78+Vf0X7dunU6dOqXc3Fy5u7tLkkJDQ79f1QAA4Jbh0tc0tbW12rNnj2JjY79dQYcOio2NVV5eXpNj/t//+3+KiYlRQkKC/P39NWDAAC1ZskQNDQ3NbqempkaVlZVOLwAAcGtyKYycPHlSDQ0N8vf3d2r39/dXWVlZk2OOHDmiTZs2qaGhQe+9956SkpKUmpqq//qv/2p2OykpKerSpYvjFRwc7EqZAADgJtLmV9M0NjbKz89Pr732mqKiojRp0iS98MILysjIaHZMYmKizpw543gdO3asrcsEAACGuHTOSLdu3eTm5qby8nKn9vLycgUEBDQ5JjAwUO7u7nJzc3O09e3bV2VlZaqtrZWHh8cVYzw9PeXp6elKaQAA4Cbl0pERDw8PRUVFKTs729HW2Nio7OxsxcTENDlm+PDhKioqUmNjo6Pt4MGDCgwMbDKIAACA24vLX9PMnj1ba9as0RtvvKH9+/frqaeeUlVVlePqmilTpigxMdHR/6mnntKpU6c0a9YsHTx4UDt27NCSJUuUkJBw/fYCAADctFy+tHfSpEk6ceKEFi5cqLKyMkVGRiozM9NxUmtJSYk6dPg24wQHB+uDDz7Qs88+q4EDByooKEizZs3SvHnzrt9eAACAm5bLYUSSZs6cqZkzZza5LCcn54q2mJgY/f3vf7+WTQEAgFscz6YBAABGEUYAAIBRhBEAAGAUYQQAABhFGAEAAEYRRgAAgFGEEQAAYBRhBAAAGEUYAQAARhFGAACAUYQRAABgFGEEAAAYRRgBAABGEUYAAIBRhBEAAGAUYQQAABhFGAEAAEYRRgAAgFGEEQAAYBRhBAAAGEUYAQAARhFGAACAUYQRAABgFGEEAAAYRRgBAABGEUYAAIBRhBEAAGAUYQQAABhFGAEAAEYRRgAAgFGEEQAAYBRhBAAAGEUYAQAARhFGAACAUYQRAABgFGEEAAAYRRgBAABGEUYAAIBRhBEAAGAUYQQAABhFGAEAAEYRRgAAgFGEEQAAYBRhBAAAGEUYAQAARhFGAACAUYQRAABgFGEEAAAYRRgBAABGEUYAAIBRhBEAAGAUYQQAABhFGAEAAEYRRgAAgFGEEQAAYBRhBAAAGEUYAQAARhFGAACAUYQRAABg1DWFkfT0dIWGhqpjx44aOnSo8vPzWzVu48aNstlsGj9+/LVsFgAA3IJcDiNvv/22Zs+erUWLFmnv3r0aNGiQ4uLidPz48RbHFRcX6ze/+Y1+9KMfXXOxAADg1uNyGFmxYoVmzJih+Ph49evXTxkZGfL29ta6deuaHdPQ0KDHHntMycnJ6t279/cqGAAA3FpcCiO1tbXas2ePYmNjv11Bhw6KjY1VXl5es+N++9vfys/PT9OnT2/VdmpqalRZWen0AgAAtyaXwsjJkyfV0NAgf39/p3Z/f3+VlZU1OebTTz/V66+/rjVr1rR6OykpKerSpYvjFRwc7EqZAADgJtKmV9OcPXtWjz/+uNasWaNu3bq1elxiYqLOnDnjeB07dqwNqwQAACbd4Urnbt26yc3NTeXl5U7t5eXlCggIuKL/4cOHVVxcrLFjxzraGhsbL274jjtUWFioe++994pxnp6e8vT0dKU0AABwk3LpyIiHh4eioqKUnZ3taGtsbFR2drZiYmKu6N+nTx/985//1L59+xyv//zP/9SIESO0b98+vn4BAACuHRmRpNmzZ2vq1KkaMmSIoqOjtXLlSlVVVSk+Pl6SNGXKFAUFBSklJUUdO3bUgAEDnMb7+PhI0hXtAADg9uRyGJk0aZJOnDihhQsXqqysTJGRkcrMzHSc1FpSUqIOHbixKwAAaB2Xw4gkzZw5UzNnzmxyWU5OTotj169ffy2bBAAAtygOYQAAAKMIIwAAwCjCCAAAMIowAgAAjCKMAAAAowgjAADAKMIIAAAwijACAACMIowAAACjCCMAAMAowggAADCKMAIAAIwijAAAAKMIIwAAwCjCCAAAMIowAgAAjCKMAAAAowgjAADAKMIIAAAwijACAACMIowAAACjCCMAAMAowggAADCKMAIAAIwijAAAAKMIIwAAwCjCCAAAMIowAgAAjCKMAAAAowgjAADAKMIIAAAwijACAACMIowAAACjCCMAAMAowggAADCKMAIAAIwijAAAAKMIIwAAwCjCCAAAMIowAgAAjCKMAAAAowgjAADAKMIIAAAwijACAACMIowAAACjCCMAAMAowggAADCKMAIAAIwijAAAAKMIIwAAwCjCCAAAMIowAgAAjCKMAAAAowgjAADAKMIIAAAwijACAACMIowAAACjCCMAAMAowggAADCKMAIAAIy6pjCSnp6u0NBQdezYUUOHDlV+fn6zfdesWaMf/ehH8vX1la+vr2JjY1vsDwAAbi8uh5G3335bs2fP1qJFi7R3714NGjRIcXFxOn78eJP9c3JyNHnyZH388cfKy8tTcHCwRo0apdLS0u9dPAAAuPm5HEZWrFihGTNmKD4+Xv369VNGRoa8vb21bt26Jvv/4Q9/0NNPP63IyEj16dNHa9euVWNjo7Kzs7938QAA4ObnUhipra3Vnj17FBsb++0KOnRQbGys8vLyWrWO6upq1dXVqWvXrs32qampUWVlpdMLAADcmlwKIydPnlRDQ4P8/f2d2v39/VVWVtaqdcybN089evRwCjTflZKSoi5dujhewcHBrpQJAABuIu16Nc3SpUu1ceNGbd26VR07dmy2X2Jios6cOeN4HTt2rB2rBAAA7ekOVzp369ZNbm5uKi8vd2ovLy9XQEBAi2P/+7//W0uXLtWHH36ogQMHttjX09NTnp6erpQGAABuUi4dGfHw8FBUVJTTyaeXTkaNiYlpdtzy5cv10ksvKTMzU0OGDLn2agEAwC3HpSMjkjR79mxNnTpVQ4YMUXR0tFauXKmqqirFx8dLkqZMmaKgoCClpKRIkpYtW6aFCxdqw4YNCg0NdZxb0qlTJ3Xq1Ok67goAALgZuRxGJk2apBMnTmjhwoUqKytTZGSkMjMzHSe1lpSUqEOHbw+4rF69WrW1tfrZz37mtJ5FixbpxRdf/H7VAwCAm57LYUSSZs6cqZkzZza5LCcnx+nn4uLia9kEAAC4TfBsGgAAYBRhBAAAGEUYAQAARhFGAACAUYQRAABgFGEEAAAYRRgBAABGEUYAAIBRhBEAAGAUYQQAABhFGAEAAEYRRgAAgFGEEQAAYBRhBAAAGEUYAQAARhFGAACAUYQRAABgFGEEAAAYRRgBAABGEUYAAIBRhBEAAGAUYQQAABhFGAEAAEYRRgAAgFGEEQAAYBRhBAAAGEUYAQAARhFGAACAUYQRAABgFGEEAAAYRRgBAABGEUYAAIBRhBEAAGAUYQQAABhFGAEAAEYRRgAAgFGEEQAAYBRhBAAAGEUYAQAARhFGAACAUYQRAABgFGEEAAAYRRgBAABGEUYAAIBRhBEAAGAUYQQAABhFGAEAAEYRRgAAgFGEEQAAYBRhBAAAGEUYAQAARhFGAACAUYQRAABgFGEEAAAYRRgBAABGEUYAAIBRhBEAAGAUYQQAABhFGAEAAEZdUxhJT09XaGioOnbsqKFDhyo/P7/F/u+884769Omjjh07KiIiQu+99941FQsAAG49LoeRt99+W7Nnz9aiRYu0d+9eDRo0SHFxcTp+/HiT/XNzczV58mRNnz5dBQUFGj9+vMaPH6/PP//8excPAABufi6HkRUrVmjGjBmKj49Xv379lJGRIW9vb61bt67J/qtWrdLo0aM1d+5c9e3bVy+99JLuv/9+paWlfe/iAQDAze8OVzrX1tZqz549SkxMdLR16NBBsbGxysvLa3JMXl6eZs+e7dQWFxenbdu2Nbudmpoa1dTUOH4+c+aMJKmystKVclvlYMFO2Ys+u+7rvR2d/6ZUp056qnz3Tp06/vUVy0vPlapncb3O7f1c5Z3PXrG89suj6vmveh0pe09//vJoe5Tc5k4d2KO7GhpUVX2hyfdvVfUFnWto0LG9f9Ofqy84LbvafJaXHNKpk54q/mCD9u/e2Wb7AODWF/iDgbpv8IPXfb2X/t2zLKvljpYLSktLLUlWbm6uU/vcuXOt6OjoJse4u7tbGzZscGpLT0+3/Pz8mt3OokWLLEm8ePHixYsXr1vgdezYsRbzhUtHRtpLYmKi09GUxsZGnTp1SnfffbdsNtt1205lZaWCg4N17Ngxde7c+bqt91bGnLmG+XIdc+Ya5ss1zJfrvs+cWZals2fPqkePHi32cymMdOvWTW5ubiovL3dqLy8vV0BAQJNjAgICXOovSZ6envL09HRq8/HxcaVUl3Tu3Jk3pYuYM9cwX65jzlzDfLmG+XLdtc5Zly5drtrHpRNYPTw8FBUVpezsbEdbY2OjsrOzFRMT0+SYmJgYp/6SlJWV1Wx/AABwe3H5a5rZs2dr6tSpGjJkiKKjo7Vy5UpVVVUpPj5ekjRlyhQFBQUpJSVFkjRr1iw9+OCDSk1N1SOPPKKNGzdq9+7deu21167vngAAgJuSy2Fk0qRJOnHihBYuXKiysjJFRkYqMzNT/v7+kqSSkhJ16PDtAZdhw4Zpw4YNWrBggZ5//nmFhYVp27ZtGjBgwPXbi2vk6empRYsWXfGVEJrHnLmG+XIdc+Ya5ss1zJfr2mPObJZ1tettAAAA2g7PpgEAAEYRRgAAgFGEEQAAYBRhBAAAGHVbh5H09HSFhoaqY8eOGjp0qPLz802XdEN48cUXZbPZnF59+vRxLL9w4YISEhJ09913q1OnTvrpT396xY3tbmWffPKJxo4dqx49eshms13xnCXLsrRw4UIFBgbKy8tLsbGxOnTokFOfU6dO6bHHHlPnzp3l4+Oj6dOn69y5c+24F+3ranP2xBNPXPGeGz16tFOf22nOUlJS9MMf/lB33XWX/Pz8NH78eBUWFjr1ac3vYUlJiR555BF5e3vLz89Pc+fOVX19fXvuSrtozXw99NBDV7zHnnzySac+t8t8SdLq1as1cOBAx43MYmJi9P777zuWt/f767YNI2+//bZmz56tRYsWae/evRo0aJDi4uJ0/Phx06XdEPr37y+73e54ffrpp45lzz77rP785z/rnXfe0c6dO/X1119rwoQJBqttX1VVVRo0aJDS09ObXL58+XK98sorysjI0K5du3TnnXcqLi5OFy58+yC8xx57TF988YWysrL07rvv6pNPPtGvfvWr9tqFdne1OZOk0aNHO73n/vjHPzotv53mbOfOnUpISNDf//53ZWVlqa6uTqNGjVJVVZWjz9V+DxsaGvTII4+otrZWubm5euONN7R+/XotXLjQxC61qdbMlyTNmDHD6T22fPlyx7Lbab4kqWfPnlq6dKn27Nmj3bt369///d81btw4ffHFF5IMvL9a8Xy8W1J0dLSVkJDg+LmhocHq0aOHlZKSYrCqG8OiRYusQYMGNbmsoqLCcnd3t9555x1H2/79+y1JVl5eXjtVeOOQZG3dutXxc2NjoxUQEGC9/PLLjraKigrL09PT+uMf/2hZlmV9+eWXliTrH//4h6PP+++/b9lsNqu0tLTdajflu3NmWZY1depUa9y4cc2Oud3n7Pjx45Yka+fOnZZlte738L333rM6dOhglZWVOfqsXr3a6ty5s1VTU9O+O9DOvjtflmVZDz74oDVr1qxmx9zO83WJr6+vtXbtWiPvr9vyyEhtba327Nmj2NhYR1uHDh0UGxurvLw8g5XdOA4dOqQePXqod+/eeuyxx1RSUiJJ2rNnj+rq6pzmrk+fPrrnnnuYO0lHjx5VWVmZ0/x06dJFQ4cOdcxPXl6efHx8NGTIEEef2NhYdejQQbt27Wr3mm8UOTk58vPzU3h4uJ566il98803jmW3+5ydOXNGktS1a1dJrfs9zMvLU0REhOOGlJIUFxenyspKx1+/t6rvztclf/jDH9StWzcNGDBAiYmJqq6udiy7neeroaFBGzduVFVVlWJiYoy8v27Ip/a2tZMnT6qhocFpEiXJ399fBw4cMFTVjWPo0KFav369wsPDZbfblZycrB/96Ef6/PPPVVZWJg8PjyseXOjv76+ysjIzBd9ALs1BU++tS8vKysrk5+fntPyOO+5Q165db9s5HD16tCZMmKBevXrp8OHDev755zVmzBjl5eXJzc3ttp6zxsZGPfPMMxo+fLjjztWt+T0sKytr8n14admtqqn5kqRf/OIXCgkJUY8ePfTZZ59p3rx5Kiws1JYtWyTdnvP1z3/+UzExMbpw4YI6deqkrVu3ql+/ftq3b1+7v79uyzCClo0ZM8bx3wMHDtTQoUMVEhKiP/3pT/Ly8jJYGW5VP//5zx3/HRERoYEDB+ree+9VTk6ORo4cabAy8xISEvT55587nbeF5jU3X5efXxQREaHAwECNHDlShw8f1r333tveZd4QwsPDtW/fPp05c0abNm3S1KlTtXPnTiO13JZf03Tr1k1ubm5XnBlcXl6ugIAAQ1XduHx8fHTfffepqKhIAQEBqq2tVUVFhVMf5u6iS3PQ0nsrICDgihOl6+vrderUKebw//Tu3VvdunVTUVGRpNt3zmbOnKl3331XH3/8sXr27Olob83vYUBAQJPvw0vLbkXNzVdThg4dKklO77Hbbb48PDz0gx/8QFFRUUpJSdGgQYO0atUqI++v2zKMeHh4KCoqStnZ2Y62xsZGZWdnKyYmxmBlN6Zz587p8OHDCgwMVFRUlNzd3Z3mrrCwUCUlJcydpF69eikgIMBpfiorK7Vr1y7H/MTExKiiokJ79uxx9Pnoo4/U2Njo+Afydvevf/1L33zzjQIDAyXdfnNmWZZmzpyprVu36qOPPlKvXr2clrfm9zAmJkb//Oc/nUJcVlaWOnfurH79+rXPjrSTq81XU/bt2ydJTu+x22W+mtPY2Kiamhoz76/ve/btzWrjxo2Wp6entX79euvLL7+0fvWrX1k+Pj5OZwbfrubMmWPl5ORYR48etf72t79ZsbGxVrdu3azjx49blmVZTz75pHXPPfdYH330kbV7924rJibGiomJMVx1+zl79qxVUFBgFRQUWJKsFStWWAUFBdZXX31lWZZlLV261PLx8bG2b99uffbZZ9a4ceOsXr16WefPn3esY/To0dbgwYOtXbt2WZ9++qkVFhZmTZ482dQutbmW5uzs2bPWb37zGysvL886evSo9eGHH1r333+/FRYWZl24cMGxjttpzp566imrS5cuVk5OjmW32x2v6upqR5+r/R7W19dbAwYMsEaNGmXt27fPyszMtLp3724lJiaa2KU2dbX5Kioqsn77299au3fvto4ePWpt377d6t27t/XjH//YsY7bab4sy7Lmz59v7dy50zp69Kj12WefWfPnz7dsNpv1l7/8xbKs9n9/3bZhxLIs69VXX7Xuuecey8PDw4qOjrb+/ve/my7phjBp0iQrMDDQ8vDwsIKCgqxJkyZZRUVFjuXnz5+3nn76acvX19fy9va2fvKTn1h2u91gxe3r448/tiRd8Zo6daplWRcv701KSrL8/f0tT09Pa+TIkVZhYaHTOr755htr8uTJVqdOnazOnTtb8fHx1tmzZw3sTftoac6qq6utUaNGWd27d7fc3d2tkJAQa8aMGVf8YXA7zVlTcyXJ+p//+R9Hn9b8HhYXF1tjxoyxvLy8rG7dullz5syx6urq2nlv2t7V5qukpMT68Y9/bHXt2tXy9PS0fvCDH1hz5861zpw547Se22W+LMuypk2bZoWEhFgeHh5W9+7drZEjRzqCiGW1//vLZlmW5frxFAAAgOvjtjxnBAAA3DgIIwAAwCjCCAAAMIowAgAAjCKMAAAAowgjAADAKMIIAAAwijACAACMIowAAACjCCMAAMAowggAADCKMAIAAIz6/1geSAsbKiCYAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cut = [0.5, 0.7, 0.9, 0.99]\n",
    "fig, ax = plt.subplots()\n",
    "for c in cut:\n",
    "    ax.hist(qcd_sdmass[XbbvsQCD > c], label=f\"Cuts: {c}\", bins=np.arange(0, 300, 5), histtype='step')\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "d0f73b3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qcd_sdmass[XbbvsQCD > 0.99].size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12c0a7d2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "weaver",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

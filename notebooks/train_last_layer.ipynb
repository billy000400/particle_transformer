{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "43700d4d",
   "metadata": {},
   "source": [
    "# Notebook to train last layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "afdf04e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import zipfile\n",
    "import tarfile\n",
    "import urllib\n",
    "import requests\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import h5py as h5\n",
    "import argparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5c813369",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import awkward as ak\n",
    "import uproot\n",
    "import vector\n",
    "vector.register_awkward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ec0649fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c1e2d6a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/billyli/miniforge_x86_new/envs/weaver/lib/python3.7/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8bb1463f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PosixPath('/Users/billyli/scope/JetClass/minimal/HToCC_120.root'),\n",
       " PosixPath('/Users/billyli/scope/JetClass/minimal/WToQQ_120.root'),\n",
       " PosixPath('/Users/billyli/scope/JetClass/minimal/ZToQQ_120.root'),\n",
       " PosixPath('/Users/billyli/scope/JetClass/minimal/HToBB_120.root'),\n",
       " PosixPath('/Users/billyli/scope/JetClass/minimal/HToGG_120.root'),\n",
       " PosixPath('/Users/billyli/scope/JetClass/minimal/HToWW2Q1L_120.root'),\n",
       " PosixPath('/Users/billyli/scope/JetClass/minimal/TTBarLep_120.root'),\n",
       " PosixPath('/Users/billyli/scope/JetClass/minimal/TTBar_120.root'),\n",
       " PosixPath('/Users/billyli/scope/JetClass/minimal/ZJetsToNuNu_120.root'),\n",
       " PosixPath('/Users/billyli/scope/JetClass/minimal/HToWW4Q_120.root')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "root_dir = Path(\"/Users/billyli/scope/JetClass/minimal\")\n",
    "list(root_dir.glob('*.root'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "61b64e01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['HToCC', '120.root']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f = list(root_dir.glob('*.root'))[0]\n",
    "f.name.split('_')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b035971",
   "metadata": {},
   "source": [
    "## Get Jet label from Root Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "09836d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_size = 1e6\n",
    "data_fraction = 3e-2\n",
    "val_fraction = 1e-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c8e68549",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_to_numpy(tree, feature):\n",
    "    return tree[feature].arrays().to_numpy().astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1357ec6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "jet_classes = []\n",
    "for f in root_dir.glob('*.root'):\n",
    "    process = f.name.split('_')[0]\n",
    "    tree = uproot.open(f)['tree']\n",
    "\n",
    "    n_entries = tree.num_entries\n",
    "    n_keep = int(n_entries * data_fraction)\n",
    "\n",
    "    jet_label_f = tree.arrays(\n",
    "        filter_name=['label_*'],\n",
    "        entry_stop=n_keep,  # only read first fraction\n",
    "    )\n",
    "\n",
    "    label_list = ['label_QCD', 'label_Hbb', 'label_Hcc', 'label_Hgg', 'label_H4q', 'label_Hqql', 'label_Zqq', 'label_Wqq', 'label_Tbqq', 'label_Tbl']\n",
    "    jet_class = np.stack([jet_label_f[n].to_numpy().astype('int') for n in label_list], axis=1)\n",
    "    jet_classes.append(jet_class)\n",
    "y = np.concatenate(jet_classes, axis=0)\n",
    "y_tensor = torch.tensor(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52a888e5",
   "metadata": {},
   "source": [
    "## Load RAV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ea520fbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 1])\n"
     ]
    }
   ],
   "source": [
    "rav = h5.File('/Users/billyli/scope/particle_transformer/notebooks/rav_sdmass_frac_3e-2.h5', 'r')[\"RAV_jet_sdmass\"][:]\n",
    "rav_tensor = torch.tensor(rav, dtype=torch.float32)\n",
    "print(rav_tensor.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e8e5e2f",
   "metadata": {},
   "source": [
    "## Find the hidden state's orthogonal component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6c2e4af7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([30000, 1, 128])\n"
     ]
    }
   ],
   "source": [
    "hidden = h5.File(\"/Users/billyli/scope/weaver-core/frac_3e-2.h5\", 'r')[\"hidden_11\"][:]\n",
    "hidden_tensor = torch.tensor(hidden, dtype=torch.float32)\n",
    "print(hidden_tensor.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "429660f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def perpendicular_component(hidden_tensor, raw_tensor, eps=1e-12):\n",
    "    \"\"\"\n",
    "    hidden_tensor: (30000, 128, 1)  vectors v\n",
    "    raw_tensor:    either (30000, 128, 1) per-sample u\n",
    "                   or     (128, 1) / (128,) shared u\n",
    "    returns:       (30000, 128, 1)  v_perp\n",
    "    \"\"\"\n",
    "    v = hidden_tensor.squeeze(-1)  # (30000, 128)\n",
    "    u = raw_tensor.squeeze(-1)     # (30000,128) or (128,)\n",
    "\n",
    "    # Broadcast u to match v if needed\n",
    "    if u.dim() == 1:\n",
    "        u = u.unsqueeze(0).expand_as(v)  # (1,128) -> (30000,128)\n",
    "    elif u.shape != v.shape:\n",
    "        u = u.expand_as(v)\n",
    "\n",
    "    # Compute projection scalar coeff alpha = (v·u)/(u·u)\n",
    "    alpha = (v * u).sum(dim=-1, keepdim=True) / ((u * u).sum(dim=-1, keepdim=True) + eps)  # (30000,1)\n",
    "\n",
    "    proj = alpha * u             # (30000,128)\n",
    "    v_perp = v - proj            # (30000,128)\n",
    "    return v_perp.permute(0, 2, 1)  # (30000,128,1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "67da95b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "hidden_perp = perpendicular_component(hidden_tensor, rav_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3db5bb90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(hidden_perp.squeeze(-1) @ rav_tensor.squeeze(-1) > 1e-4).sum() # should be close to 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bcffced",
   "metadata": {},
   "source": [
    "## Define the last layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "89decf41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class PerpClassifier(nn.Module):\n",
    "    def __init__(self, in_dim=128, num_classes=10, for_inference=False):\n",
    "        super().__init__()\n",
    "        self.norm = nn.LayerNorm(in_dim)\n",
    "        self.fc = nn.Linear(in_dim, num_classes)\n",
    "        self.for_inference = for_inference\n",
    "\n",
    "    def forward(self, cls_tokens, rav_tensor=None, return_hidden=False):\n",
    "        \"\"\"\n",
    "        cls_tokens: (N, C) or (1, N, C) like your snippet\n",
    "        raw_tensor: (N, C) or (C,) direction(s) to project out\n",
    "        \"\"\"\n",
    "        # Match your snippet's first lines\n",
    "        if cls_tokens.dim() == 3:\n",
    "            # (1, N, C) -> (N, C)\n",
    "            x_cls = self.norm(cls_tokens).squeeze(0)\n",
    "        else:\n",
    "            x_cls = self.norm(cls_tokens)\n",
    "\n",
    "        # Keep track if you want\n",
    "        hiddens = [x_cls.unsqueeze(0)]\n",
    "\n",
    "        # Use perpendicular component if provided\n",
    "        if rav_tensor is not None:\n",
    "            x_used = perpendicular_component(x_cls, rav_tensor)   # (N, C)\n",
    "        else:\n",
    "            x_used = x_cls\n",
    "        \n",
    "        x_used = x_used.squeeze(-1)\n",
    "        logits = self.fc(x_used)                         # (N, 10)\n",
    "\n",
    "        if self.for_inference:\n",
    "            probs = F.softmax(logits, dim=1)\n",
    "            return (probs, hiddens) if return_hidden else probs\n",
    "        else:\n",
    "            return (logits, hiddens) if return_hidden else logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "689d4c6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PerpClassifier(\n",
       "  (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "  (fc): Linear(in_features=128, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = PerpClassifier(in_dim=128, num_classes=10, for_inference=False)\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6867b23",
   "metadata": {},
   "source": [
    "## Import Weaver modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ecd2074d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from weaver.utils.dataset import SimpleIterDataset\n",
    "from weaver.train import train_load, model_setup, optim\n",
    "from weaver.utils.logger import _logger, _configLogger"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "656e7bc8",
   "metadata": {},
   "source": [
    "## Mimic Args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "64528f96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_StoreTrueAction(option_strings=['--hidden-states'], dest='hidden_states', nargs=0, const=True, default=False, type=None, choices=None, help='let ParT output hidden states with the logits', metavar=None)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--regression-mode', action='store_true', default=False,\n",
    "                    help='run in regression mode if this flag is set; otherwise run in classification mode')\n",
    "parser.add_argument('-c', '--data-config', type=str,\n",
    "                    help='data config YAML file')\n",
    "parser.add_argument('--extra-selection', type=str, default=None,\n",
    "                    help='Additional selection requirement, will modify `selection` to `(selection) & (extra)` on-the-fly')\n",
    "parser.add_argument('--extra-test-selection', type=str, default=None,\n",
    "                    help='Additional test-time selection requirement, will modify `test_time_selection` to `(test_time_selection) & (extra)` on-the-fly')\n",
    "parser.add_argument('-i', '--data-train', nargs='*', default=[],\n",
    "                    help='training files; supported syntax:'\n",
    "                         ' (a) plain list, `--data-train /path/to/a/* /path/to/b/*`;'\n",
    "                         ' (b) (named) groups [Recommended], `--data-train a:/path/to/a/* b:/path/to/b/*`,'\n",
    "                         ' the file splitting (for each dataloader worker) will be performed per group,'\n",
    "                         ' and then mixed together, to ensure a uniform mixing from all groups for each worker.'\n",
    "                    )\n",
    "parser.add_argument('-l', '--data-val', nargs='*', default=[],\n",
    "                    help='validation files; when not set, will use training files and split by `--train-val-split`')\n",
    "parser.add_argument('-t', '--data-test', nargs='*', default=[],\n",
    "                    help='testing files; supported syntax:'\n",
    "                         ' (a) plain list, `--data-test /path/to/a/* /path/to/b/*`;'\n",
    "                         ' (b) keyword-based, `--data-test a:/path/to/a/* b:/path/to/b/*`, will produce output_a, output_b;'\n",
    "                         ' (c) split output per N input files, `--data-test a%%10:/path/to/a/*`, will split per 10 input files')\n",
    "parser.add_argument('--data-fraction', type=float, default=1,\n",
    "                    help='fraction of events to load from each file; for training, the events are randomly selected for each epoch')\n",
    "parser.add_argument('--file-fraction', type=float, default=1,\n",
    "                    help='fraction of files to load; for training, the files are randomly selected for each epoch')\n",
    "parser.add_argument('--fetch-by-files', action='store_true', default=False,\n",
    "                    help='When enabled, will load all events from a small number (set by ``--fetch-step``) of files for each data fetching. '\n",
    "                         'Otherwise (default), load a small fraction of events from all files each time, which helps reduce variations in the sample composition.')\n",
    "parser.add_argument('--fetch-step', type=float, default=0.01,\n",
    "                    help='fraction of events to load each time from every file (when ``--fetch-by-files`` is disabled); '\n",
    "                         'Or: number of files to load each time (when ``--fetch-by-files`` is enabled). Shuffling & sampling is done within these events, so set a large enough value.')\n",
    "parser.add_argument('--in-memory', action='store_true', default=False,\n",
    "                    help='load the whole dataset (and perform the preprocessing) only once and keep it in memory for the entire run')\n",
    "parser.add_argument('--train-val-split', type=float, default=0.8,\n",
    "                    help='training/validation split fraction')\n",
    "parser.add_argument('--no-remake-weights', action='store_true', default=False,\n",
    "                    help='do not remake weights for sampling (reweighting), use existing ones in the previous auto-generated data config YAML file')\n",
    "parser.add_argument('--demo', action='store_true', default=False,\n",
    "                    help='quickly test the setup by running over only a small number of events')\n",
    "parser.add_argument('--lr-finder', type=str, default=None,\n",
    "                    help='run learning rate finder instead of the actual training; format: ``start_lr, end_lr, num_iters``')\n",
    "parser.add_argument('--tensorboard', type=str, default=None,\n",
    "                    help='create a tensorboard summary writer with the given comment')\n",
    "parser.add_argument('--tensorboard-custom-fn', type=str, default=None,\n",
    "                    help='the path of the python script containing a user-specified function `get_tensorboard_custom_fn`, '\n",
    "                         'to display custom information per mini-batch or per epoch, during the training, validation or test.')\n",
    "parser.add_argument('-n', '--network-config', type=str,\n",
    "                    help='network architecture configuration file; the path must be relative to the current dir')\n",
    "parser.add_argument('-o', '--network-option', nargs=2, action='append', default=[],\n",
    "                    help='options to pass to the model class constructor, e.g., `--network-option use_counts False`')\n",
    "parser.add_argument('-m', '--model-prefix', type=str, default='models/{auto}/network',\n",
    "                    help='path to save or load the model; for training, this will be used as a prefix, so model snapshots '\n",
    "                         'will saved to `{model_prefix}_epoch-%%d_state.pt` after each epoch, and the one with the best '\n",
    "                         'validation metric to `{model_prefix}_best_epoch_state.pt`; for testing, this should be the full path '\n",
    "                         'including the suffix, otherwise the one with the best validation metric will be used; '\n",
    "                         'for training, `{auto}` can be used as part of the path to auto-generate a name, '\n",
    "                         'based on the timestamp and network configuration')\n",
    "parser.add_argument('--load-model-weights', type=str, default=None,\n",
    "                    help='initialize model with pre-trained weights')\n",
    "parser.add_argument('--exclude-model-weights', type=str, default=None,\n",
    "                    help='comma-separated regex to exclude matched weights from being loaded, e.g., `a.fc..+,b.fc..+`')\n",
    "parser.add_argument('--freeze-model-weights', type=str, default=None,\n",
    "                    help='comma-separated regex to freeze matched weights from being updated in the training, e.g., `a.fc..+,b.fc..+`')\n",
    "parser.add_argument('--num-epochs', type=int, default=20,\n",
    "                    help='number of epochs')\n",
    "parser.add_argument('--steps-per-epoch', type=int, default=None,\n",
    "                    help='number of steps (iterations) per epochs; '\n",
    "                         'if neither of `--steps-per-epoch` or `--samples-per-epoch` is set, each epoch will run over all loaded samples')\n",
    "parser.add_argument('--steps-per-epoch-val', type=int, default=None,\n",
    "                    help='number of steps (iterations) per epochs for validation; '\n",
    "                         'if neither of `--steps-per-epoch-val` or `--samples-per-epoch-val` is set, each epoch will run over all loaded samples')\n",
    "parser.add_argument('--samples-per-epoch', type=int, default=None,\n",
    "                    help='number of samples per epochs; '\n",
    "                         'if neither of `--steps-per-epoch` or `--samples-per-epoch` is set, each epoch will run over all loaded samples')\n",
    "parser.add_argument('--samples-per-epoch-val', type=int, default=None,\n",
    "                    help='number of samples per epochs for validation; '\n",
    "                         'if neither of `--steps-per-epoch-val` or `--samples-per-epoch-val` is set, each epoch will run over all loaded samples')\n",
    "parser.add_argument('--optimizer', type=str, default='ranger', choices=['adam', 'adamW', 'radam', 'ranger'],  # TODO: add more\n",
    "                    help='optimizer for the training')\n",
    "parser.add_argument('--optimizer-option', nargs=2, action='append', default=[],\n",
    "                    help='options to pass to the optimizer class constructor, e.g., `--optimizer-option weight_decay 1e-4`')\n",
    "parser.add_argument('--lr-scheduler', type=str, default='flat+decay',\n",
    "                    choices=['none', 'steps', 'flat+decay', 'flat+linear', 'flat+cos', 'one-cycle'],\n",
    "                    help='learning rate scheduler')\n",
    "parser.add_argument('--warmup-steps', type=int, default=0,\n",
    "                    help='number of warm-up steps, only valid for `flat+linear` and `flat+cos` lr schedulers')\n",
    "parser.add_argument('--load-epoch', type=int, default=None,\n",
    "                    help='used to resume interrupted training, load model and optimizer state saved in the `epoch-%%d_state.pt` and `epoch-%%d_optimizer.pt` files')\n",
    "parser.add_argument('--start-lr', type=float, default=5e-3,\n",
    "                    help='start learning rate')\n",
    "parser.add_argument('--batch-size', type=int, default=128,\n",
    "                    help='batch size')\n",
    "parser.add_argument('--use-amp', action='store_true', default=False,\n",
    "                    help='use mixed precision training (fp16)')\n",
    "parser.add_argument('--gpus', type=str, default='0',\n",
    "                    help='device for the training/testing; to use CPU, set to empty string (\"\"); to use multiple gpu, set it as a comma separated list, e.g., `1,2,3,4`')\n",
    "parser.add_argument('--predict-gpus', type=str, default=None,\n",
    "                    help='device for the testing; to use CPU, set to empty string (\"\"); to use multiple gpu, set it as a comma separated list, e.g., `1,2,3,4`; if not set, use the same as `--gpus`')\n",
    "parser.add_argument('--num-workers', type=int, default=1,\n",
    "                    help='number of threads to load the dataset; memory consumption and disk access load increases (~linearly) with this numbers')\n",
    "parser.add_argument('--predict', action='store_true', default=False,\n",
    "                    help='run prediction instead of training')\n",
    "parser.add_argument('--predict-output', type=str,\n",
    "                    help='path to save the prediction output, support `.root` and `.parquet` format')\n",
    "parser.add_argument('--export-onnx', type=str, default=None,\n",
    "                    help='export the PyTorch model to ONNX model and save it at the given path (path must ends w/ .onnx); '\n",
    "                         'needs to set `--data-config`, `--network-config`, and `--model-prefix` (requires the full model path)')\n",
    "parser.add_argument('--onnx-opset', type=int, default=15,\n",
    "                    help='ONNX opset version.')\n",
    "parser.add_argument('--io-test', action='store_true', default=False,\n",
    "                    help='test throughput of the dataloader')\n",
    "parser.add_argument('--copy-inputs', action='store_true', default=False,\n",
    "                    help='copy input files to the current dir (can help to speed up dataloading when running over remote files, e.g., from EOS)')\n",
    "parser.add_argument('--log', type=str, default='',\n",
    "                    help='path to the log file; `{auto}` can be used as part of the path to auto-generate a name, based on the timestamp and network configuration')\n",
    "parser.add_argument('--print', action='store_true', default=False,\n",
    "                    help='do not run training/prediction but only print model information, e.g., FLOPs and number of parameters of a model')\n",
    "parser.add_argument('--profile', action='store_true', default=False,\n",
    "                    help='run the profiler')\n",
    "parser.add_argument('--backend', type=str, choices=['gloo', 'nccl', 'mpi'], default=None,\n",
    "                    help='backend for distributed training')\n",
    "parser.add_argument('--cross-validation', type=str, default=None,\n",
    "                    help='enable k-fold cross validation; input format: `variable_name%%k`')\n",
    "parser.add_argument('--disable-mps', action='store_true', default=False,\n",
    "                    help='disable using mps device if it does not work for you')\n",
    "parser.add_argument('--hidden-states-out', type=str, default=\"hidden_states_out.h5\",\n",
    "                    help='path to save hidden states as h5 file')\n",
    "parser.add_argument('--hidden-states', action='store_true', default=False,\n",
    "                    help='let ParT output hidden states with the logits')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "89d06666",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = parser.parse_args([\n",
    "    \"--predict\",\n",
    "    \"--data-test\", \"/Users/billyli/scope/JetClass/minimal/*.root\",\n",
    "    \"--data-config\", \"/Users/billyli/scope/particle_transformer/data/JetClass/JetClass_full.yaml\",\n",
    "    \"--data-fraction\", f\"{data_fraction}\",\n",
    "    \"--network-config\", \"/Users/billyli/scope/particle_transformer/networks/ParT_w_hidden_states.py\",\n",
    "    \"--use-amp\",\n",
    "    \"--model-prefix\", \"/Users/billyli/scope/particle_transformer/models/ParT_full.pt\",\n",
    "    \"--batch-size\", \"64\",\n",
    "    \"--predict-output\", \"/Users/billyli/scope/particle_transformer/tmp/test_output.root\",\n",
    "    \"--gpus\", \"\",\n",
    "    \"--num-workers\", \"0\",\n",
    "    \"--disable-mps\",\n",
    "    \"--hidden-states\"\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75c5a8f6",
   "metadata": {},
   "source": [
    "## Deivice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c2c8ad88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# device\n",
    "if args.gpus:\n",
    "    # distributed training\n",
    "    if args.backend is not None:\n",
    "        local_rank = args.local_rank\n",
    "        torch.cuda.set_device(local_rank)\n",
    "        gpus = [local_rank]\n",
    "        dev = torch.device(local_rank)\n",
    "        torch.distributed.init_process_group(backend=args.backend)\n",
    "        _logger.info(f'Using distributed PyTorch with {args.backend} backend')\n",
    "    else:\n",
    "        gpus = [int(i) for i in args.gpus.split(',')]\n",
    "        dev = torch.device(gpus[0])\n",
    "else:\n",
    "    gpus = None\n",
    "    dev = torch.device('cpu')\n",
    "    if not args.disable_mps:\n",
    "        try:\n",
    "            if torch.backends.mps.is_available():\n",
    "                dev = torch.device('mps')\n",
    "        except AttributeError:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1ccd79f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "args.local_rank = None if args.backend is None else int(os.environ.get(\"LOCAL_RANK\", \"0\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8e5367b",
   "metadata": {},
   "source": [
    "## Prepare Dataloader for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6129ad01",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader, random_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "9f587edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: X: (N, 128), y_onehot: (N, 10)\n",
    "y_idx = y_tensor.argmax(dim=1).long()  # (N,)\n",
    "\n",
    "# Wrap into a dataset\n",
    "dataset = TensorDataset(hidden_tensor, y_idx)\n",
    "\n",
    "# Split 9:1\n",
    "N = len(dataset)\n",
    "n_train = int(0.9 * N)\n",
    "n_val   = N - n_train\n",
    "train_set, val_set = random_split(dataset, [n_train, n_val])\n",
    "\n",
    "# Build DataLoaders\n",
    "train_loader = DataLoader(train_set, batch_size=args.batch_size, shuffle=True)\n",
    "val_loader   = DataLoader(val_set,   batch_size=args.batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe79f6d4",
   "metadata": {},
   "source": [
    "## Config Optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "befe3b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39abeeca",
   "metadata": {},
   "source": [
    "## Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "27b376ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02a756dc",
   "metadata": {},
   "source": [
    "## Train the last layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ba3904f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "1da4d33b",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def eval_epoch(loader, rav_tensor, device='cpu'):\n",
    "    model.eval()\n",
    "    total, correct, total_loss = 0, 0, 0.0\n",
    "    for xb, yb in loader:\n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "        logits = model(xb, rav_tensor=rav_tensor)\n",
    "        loss = criterion(logits, yb)\n",
    "        preds = logits.argmax(dim=1)\n",
    "        correct += (preds == yb).sum().item()\n",
    "        total += yb.size(0)\n",
    "        total_loss += loss.item() * yb.size(0)\n",
    "    return total_loss/total, correct/total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d13f8917",
   "metadata": {},
   "outputs": [],
   "source": [
    "eppoch = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e055adee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01 | train loss 0.4058 acc 0.8568 | val loss 0.4053 acc 0.8573\n",
      "Epoch 02 | train loss 0.4058 acc 0.8571 | val loss 0.4048 acc 0.8576\n",
      "Epoch 03 | train loss 0.4059 acc 0.8571 | val loss 0.4047 acc 0.8575\n",
      "Epoch 04 | train loss 0.4059 acc 0.8574 | val loss 0.4048 acc 0.8567\n",
      "Epoch 05 | train loss 0.4058 acc 0.8564 | val loss 0.4048 acc 0.8576\n",
      "Epoch 06 | train loss 0.4058 acc 0.8571 | val loss 0.4048 acc 0.8578\n",
      "Epoch 07 | train loss 0.4056 acc 0.8571 | val loss 0.4048 acc 0.8572\n",
      "Epoch 08 | train loss 0.4058 acc 0.8571 | val loss 0.4048 acc 0.8569\n",
      "Epoch 09 | train loss 0.4058 acc 0.8564 | val loss 0.4049 acc 0.8579\n",
      "Epoch 10 | train loss 0.4057 acc 0.8576 | val loss 0.4048 acc 0.8572\n",
      "Epoch 11 | train loss 0.4058 acc 0.8570 | val loss 0.4048 acc 0.8571\n",
      "Epoch 12 | train loss 0.4057 acc 0.8570 | val loss 0.4051 acc 0.8576\n",
      "Epoch 13 | train loss 0.4057 acc 0.8566 | val loss 0.4049 acc 0.8571\n",
      "Epoch 14 | train loss 0.4057 acc 0.8570 | val loss 0.4048 acc 0.8575\n",
      "Epoch 15 | train loss 0.4057 acc 0.8577 | val loss 0.4049 acc 0.8573\n",
      "Epoch 16 | train loss 0.4059 acc 0.8571 | val loss 0.4046 acc 0.8579\n",
      "Epoch 17 | train loss 0.4056 acc 0.8573 | val loss 0.4048 acc 0.8576\n",
      "Epoch 18 | train loss 0.4057 acc 0.8577 | val loss 0.4050 acc 0.8571\n",
      "Epoch 19 | train loss 0.4057 acc 0.8568 | val loss 0.4048 acc 0.8573\n",
      "Epoch 20 | train loss 0.4058 acc 0.8570 | val loss 0.4046 acc 0.8578\n",
      "Epoch 21 | train loss 0.4056 acc 0.8575 | val loss 0.4052 acc 0.8564\n",
      "Epoch 22 | train loss 0.4057 acc 0.8578 | val loss 0.4046 acc 0.8571\n",
      "Epoch 23 | train loss 0.4057 acc 0.8572 | val loss 0.4048 acc 0.8574\n",
      "Epoch 24 | train loss 0.4056 acc 0.8565 | val loss 0.4051 acc 0.8571\n",
      "Epoch 25 | train loss 0.4057 acc 0.8567 | val loss 0.4048 acc 0.8575\n",
      "Epoch 26 | train loss 0.4056 acc 0.8567 | val loss 0.4049 acc 0.8574\n",
      "Epoch 27 | train loss 0.4057 acc 0.8572 | val loss 0.4046 acc 0.8575\n",
      "Epoch 28 | train loss 0.4058 acc 0.8570 | val loss 0.4045 acc 0.8575\n",
      "Epoch 29 | train loss 0.4055 acc 0.8570 | val loss 0.4048 acc 0.8579\n",
      "Epoch 30 | train loss 0.4056 acc 0.8574 | val loss 0.4045 acc 0.8579\n",
      "Epoch 31 | train loss 0.4057 acc 0.8568 | val loss 0.4045 acc 0.8577\n",
      "Epoch 32 | train loss 0.4056 acc 0.8574 | val loss 0.4047 acc 0.8580\n",
      "Epoch 33 | train loss 0.4056 acc 0.8573 | val loss 0.4045 acc 0.8569\n",
      "Epoch 34 | train loss 0.4056 acc 0.8571 | val loss 0.4046 acc 0.8574\n",
      "Epoch 35 | train loss 0.4056 acc 0.8571 | val loss 0.4045 acc 0.8573\n",
      "Epoch 36 | train loss 0.4056 acc 0.8574 | val loss 0.4049 acc 0.8570\n",
      "Epoch 37 | train loss 0.4055 acc 0.8568 | val loss 0.4047 acc 0.8570\n",
      "Epoch 38 | train loss 0.4055 acc 0.8572 | val loss 0.4047 acc 0.8568\n",
      "Epoch 39 | train loss 0.4056 acc 0.8571 | val loss 0.4046 acc 0.8576\n",
      "Epoch 40 | train loss 0.4057 acc 0.8571 | val loss 0.4044 acc 0.8576\n",
      "Epoch 41 | train loss 0.4055 acc 0.8564 | val loss 0.4046 acc 0.8575\n",
      "Epoch 42 | train loss 0.4056 acc 0.8569 | val loss 0.4046 acc 0.8574\n",
      "Epoch 43 | train loss 0.4054 acc 0.8579 | val loss 0.4051 acc 0.8573\n",
      "Epoch 44 | train loss 0.4055 acc 0.8571 | val loss 0.4047 acc 0.8583\n",
      "Epoch 45 | train loss 0.4054 acc 0.8577 | val loss 0.4048 acc 0.8579\n",
      "Epoch 46 | train loss 0.4056 acc 0.8572 | val loss 0.4050 acc 0.8570\n",
      "Epoch 47 | train loss 0.4054 acc 0.8571 | val loss 0.4049 acc 0.8575\n",
      "Epoch 48 | train loss 0.4054 acc 0.8567 | val loss 0.4047 acc 0.8574\n",
      "Epoch 49 | train loss 0.4054 acc 0.8573 | val loss 0.4054 acc 0.8570\n",
      "Epoch 50 | train loss 0.4054 acc 0.8569 | val loss 0.4052 acc 0.8563\n",
      "Epoch 51 | train loss 0.4056 acc 0.8571 | val loss 0.4046 acc 0.8575\n",
      "Epoch 52 | train loss 0.4054 acc 0.8572 | val loss 0.4046 acc 0.8567\n",
      "Epoch 53 | train loss 0.4055 acc 0.8576 | val loss 0.4046 acc 0.8573\n",
      "Epoch 54 | train loss 0.4054 acc 0.8568 | val loss 0.4046 acc 0.8577\n",
      "Epoch 55 | train loss 0.4055 acc 0.8570 | val loss 0.4047 acc 0.8578\n",
      "Epoch 56 | train loss 0.4055 acc 0.8567 | val loss 0.4045 acc 0.8580\n",
      "Epoch 57 | train loss 0.4054 acc 0.8569 | val loss 0.4047 acc 0.8575\n",
      "Epoch 58 | train loss 0.4054 acc 0.8575 | val loss 0.4051 acc 0.8578\n",
      "Epoch 59 | train loss 0.4055 acc 0.8569 | val loss 0.4044 acc 0.8579\n",
      "Epoch 60 | train loss 0.4055 acc 0.8570 | val loss 0.4045 acc 0.8575\n",
      "Epoch 61 | train loss 0.4053 acc 0.8579 | val loss 0.4045 acc 0.8580\n",
      "Epoch 62 | train loss 0.4055 acc 0.8573 | val loss 0.4046 acc 0.8566\n",
      "Epoch 63 | train loss 0.4054 acc 0.8573 | val loss 0.4045 acc 0.8571\n",
      "Epoch 64 | train loss 0.4055 acc 0.8577 | val loss 0.4045 acc 0.8576\n",
      "Epoch 65 | train loss 0.4054 acc 0.8574 | val loss 0.4045 acc 0.8570\n",
      "Epoch 66 | train loss 0.4054 acc 0.8566 | val loss 0.4047 acc 0.8575\n",
      "Epoch 67 | train loss 0.4054 acc 0.8571 | val loss 0.4044 acc 0.8576\n",
      "Epoch 68 | train loss 0.4054 acc 0.8579 | val loss 0.4052 acc 0.8567\n",
      "Epoch 69 | train loss 0.4054 acc 0.8561 | val loss 0.4044 acc 0.8583\n",
      "Epoch 70 | train loss 0.4053 acc 0.8579 | val loss 0.4047 acc 0.8572\n",
      "Epoch 71 | train loss 0.4055 acc 0.8574 | val loss 0.4050 acc 0.8571\n",
      "Epoch 72 | train loss 0.4053 acc 0.8567 | val loss 0.4044 acc 0.8576\n",
      "Epoch 73 | train loss 0.4053 acc 0.8569 | val loss 0.4045 acc 0.8574\n",
      "Epoch 74 | train loss 0.4053 acc 0.8569 | val loss 0.4044 acc 0.8574\n",
      "Epoch 75 | train loss 0.4053 acc 0.8576 | val loss 0.4046 acc 0.8582\n",
      "Epoch 76 | train loss 0.4052 acc 0.8574 | val loss 0.4048 acc 0.8570\n",
      "Epoch 77 | train loss 0.4052 acc 0.8570 | val loss 0.4046 acc 0.8576\n",
      "Epoch 78 | train loss 0.4055 acc 0.8577 | val loss 0.4043 acc 0.8571\n",
      "Epoch 79 | train loss 0.4053 acc 0.8573 | val loss 0.4045 acc 0.8573\n",
      "Epoch 80 | train loss 0.4053 acc 0.8576 | val loss 0.4044 acc 0.8575\n",
      "Epoch 81 | train loss 0.4052 acc 0.8569 | val loss 0.4048 acc 0.8580\n",
      "Epoch 82 | train loss 0.4054 acc 0.8574 | val loss 0.4045 acc 0.8577\n",
      "Epoch 83 | train loss 0.4054 acc 0.8572 | val loss 0.4044 acc 0.8573\n",
      "Epoch 84 | train loss 0.4053 acc 0.8571 | val loss 0.4044 acc 0.8573\n",
      "Epoch 85 | train loss 0.4051 acc 0.8577 | val loss 0.4048 acc 0.8576\n",
      "Epoch 86 | train loss 0.4054 acc 0.8573 | val loss 0.4044 acc 0.8580\n",
      "Epoch 87 | train loss 0.4053 acc 0.8571 | val loss 0.4044 acc 0.8574\n",
      "Epoch 88 | train loss 0.4053 acc 0.8574 | val loss 0.4045 acc 0.8579\n",
      "Epoch 89 | train loss 0.4053 acc 0.8573 | val loss 0.4044 acc 0.8574\n",
      "Epoch 90 | train loss 0.4054 acc 0.8576 | val loss 0.4043 acc 0.8574\n",
      "Epoch 91 | train loss 0.4052 acc 0.8572 | val loss 0.4046 acc 0.8575\n",
      "Epoch 92 | train loss 0.4053 acc 0.8570 | val loss 0.4045 acc 0.8575\n",
      "Epoch 93 | train loss 0.4052 acc 0.8576 | val loss 0.4044 acc 0.8579\n",
      "Epoch 94 | train loss 0.4053 acc 0.8573 | val loss 0.4047 acc 0.8577\n",
      "Epoch 95 | train loss 0.4051 acc 0.8574 | val loss 0.4047 acc 0.8571\n",
      "Epoch 96 | train loss 0.4053 acc 0.8571 | val loss 0.4045 acc 0.8570\n",
      "Epoch 97 | train loss 0.4052 acc 0.8574 | val loss 0.4043 acc 0.8573\n",
      "Epoch 98 | train loss 0.4055 acc 0.8567 | val loss 0.4043 acc 0.8578\n",
      "Epoch 99 | train loss 0.4054 acc 0.8576 | val loss 0.4042 acc 0.8573\n",
      "Epoch 100 | train loss 0.4053 acc 0.8568 | val loss 0.4044 acc 0.8579\n",
      "Epoch 101 | train loss 0.4053 acc 0.8570 | val loss 0.4043 acc 0.8571\n",
      "Epoch 102 | train loss 0.4053 acc 0.8574 | val loss 0.4042 acc 0.8580\n",
      "Epoch 103 | train loss 0.4052 acc 0.8574 | val loss 0.4045 acc 0.8574\n",
      "Epoch 104 | train loss 0.4052 acc 0.8568 | val loss 0.4047 acc 0.8577\n",
      "Epoch 105 | train loss 0.4054 acc 0.8573 | val loss 0.4045 acc 0.8579\n",
      "Epoch 106 | train loss 0.4052 acc 0.8571 | val loss 0.4046 acc 0.8584\n",
      "Epoch 107 | train loss 0.4052 acc 0.8574 | val loss 0.4047 acc 0.8572\n",
      "Epoch 108 | train loss 0.4054 acc 0.8580 | val loss 0.4044 acc 0.8571\n",
      "Epoch 109 | train loss 0.4052 acc 0.8563 | val loss 0.4045 acc 0.8573\n",
      "Epoch 110 | train loss 0.4053 acc 0.8573 | val loss 0.4044 acc 0.8573\n",
      "Epoch 111 | train loss 0.4053 acc 0.8566 | val loss 0.4043 acc 0.8576\n",
      "Epoch 112 | train loss 0.4054 acc 0.8573 | val loss 0.4044 acc 0.8574\n",
      "Epoch 113 | train loss 0.4051 acc 0.8573 | val loss 0.4046 acc 0.8574\n",
      "Epoch 114 | train loss 0.4052 acc 0.8580 | val loss 0.4045 acc 0.8576\n",
      "Epoch 115 | train loss 0.4052 acc 0.8573 | val loss 0.4044 acc 0.8571\n",
      "Epoch 116 | train loss 0.4051 acc 0.8570 | val loss 0.4048 acc 0.8573\n",
      "Epoch 117 | train loss 0.4052 acc 0.8573 | val loss 0.4044 acc 0.8574\n",
      "Epoch 118 | train loss 0.4051 acc 0.8577 | val loss 0.4047 acc 0.8580\n",
      "Epoch 119 | train loss 0.4054 acc 0.8570 | val loss 0.4041 acc 0.8577\n",
      "Epoch 120 | train loss 0.4051 acc 0.8578 | val loss 0.4044 acc 0.8569\n",
      "Epoch 121 | train loss 0.4054 acc 0.8574 | val loss 0.4045 acc 0.8572\n",
      "Epoch 122 | train loss 0.4051 acc 0.8571 | val loss 0.4044 acc 0.8577\n",
      "Epoch 123 | train loss 0.4053 acc 0.8575 | val loss 0.4042 acc 0.8581\n",
      "Epoch 124 | train loss 0.4051 acc 0.8580 | val loss 0.4050 acc 0.8567\n",
      "Epoch 125 | train loss 0.4051 acc 0.8569 | val loss 0.4043 acc 0.8572\n",
      "Epoch 126 | train loss 0.4053 acc 0.8570 | val loss 0.4042 acc 0.8575\n",
      "Epoch 127 | train loss 0.4052 acc 0.8573 | val loss 0.4044 acc 0.8577\n",
      "Epoch 128 | train loss 0.4052 acc 0.8571 | val loss 0.4041 acc 0.8578\n",
      "Epoch 129 | train loss 0.4051 acc 0.8578 | val loss 0.4043 acc 0.8582\n",
      "Epoch 130 | train loss 0.4052 acc 0.8576 | val loss 0.4043 acc 0.8575\n",
      "Epoch 131 | train loss 0.4051 acc 0.8571 | val loss 0.4044 acc 0.8574\n",
      "Epoch 132 | train loss 0.4053 acc 0.8577 | val loss 0.4042 acc 0.8574\n",
      "Epoch 133 | train loss 0.4051 acc 0.8571 | val loss 0.4044 acc 0.8570\n",
      "Epoch 134 | train loss 0.4053 acc 0.8570 | val loss 0.4043 acc 0.8576\n",
      "Epoch 135 | train loss 0.4053 acc 0.8573 | val loss 0.4041 acc 0.8574\n",
      "Epoch 136 | train loss 0.4051 acc 0.8568 | val loss 0.4043 acc 0.8577\n",
      "Epoch 137 | train loss 0.4053 acc 0.8577 | val loss 0.4045 acc 0.8578\n",
      "Epoch 138 | train loss 0.4051 acc 0.8580 | val loss 0.4044 acc 0.8573\n",
      "Epoch 139 | train loss 0.4051 acc 0.8574 | val loss 0.4041 acc 0.8579\n",
      "Epoch 140 | train loss 0.4052 acc 0.8578 | val loss 0.4042 acc 0.8576\n",
      "Epoch 141 | train loss 0.4051 acc 0.8569 | val loss 0.4041 acc 0.8580\n",
      "Epoch 142 | train loss 0.4051 acc 0.8576 | val loss 0.4045 acc 0.8571\n",
      "Epoch 143 | train loss 0.4052 acc 0.8569 | val loss 0.4042 acc 0.8579\n",
      "Epoch 144 | train loss 0.4051 acc 0.8571 | val loss 0.4044 acc 0.8572\n",
      "Epoch 145 | train loss 0.4051 acc 0.8576 | val loss 0.4046 acc 0.8578\n",
      "Epoch 146 | train loss 0.4052 acc 0.8577 | val loss 0.4042 acc 0.8574\n",
      "Epoch 147 | train loss 0.4051 acc 0.8570 | val loss 0.4045 acc 0.8580\n",
      "Epoch 148 | train loss 0.4051 acc 0.8576 | val loss 0.4043 acc 0.8573\n",
      "Epoch 149 | train loss 0.4051 acc 0.8570 | val loss 0.4043 acc 0.8573\n",
      "Epoch 150 | train loss 0.4051 acc 0.8572 | val loss 0.4044 acc 0.8573\n",
      "Epoch 151 | train loss 0.4053 acc 0.8571 | val loss 0.4044 acc 0.8580\n",
      "Epoch 152 | train loss 0.4052 acc 0.8572 | val loss 0.4043 acc 0.8575\n",
      "Epoch 153 | train loss 0.4051 acc 0.8575 | val loss 0.4041 acc 0.8578\n",
      "Epoch 154 | train loss 0.4051 acc 0.8574 | val loss 0.4041 acc 0.8571\n",
      "Epoch 155 | train loss 0.4051 acc 0.8571 | val loss 0.4046 acc 0.8573\n",
      "Epoch 156 | train loss 0.4052 acc 0.8569 | val loss 0.4041 acc 0.8574\n",
      "Epoch 157 | train loss 0.4050 acc 0.8576 | val loss 0.4042 acc 0.8579\n",
      "Epoch 158 | train loss 0.4051 acc 0.8572 | val loss 0.4044 acc 0.8578\n",
      "Epoch 159 | train loss 0.4051 acc 0.8573 | val loss 0.4044 acc 0.8568\n",
      "Epoch 160 | train loss 0.4051 acc 0.8573 | val loss 0.4042 acc 0.8575\n",
      "Epoch 161 | train loss 0.4051 acc 0.8571 | val loss 0.4044 acc 0.8569\n",
      "Epoch 162 | train loss 0.4049 acc 0.8576 | val loss 0.4046 acc 0.8581\n",
      "Epoch 163 | train loss 0.4050 acc 0.8571 | val loss 0.4047 acc 0.8574\n",
      "Epoch 164 | train loss 0.4052 acc 0.8577 | val loss 0.4042 acc 0.8580\n",
      "Epoch 165 | train loss 0.4050 acc 0.8577 | val loss 0.4042 acc 0.8574\n",
      "Epoch 166 | train loss 0.4051 acc 0.8573 | val loss 0.4042 acc 0.8574\n",
      "Epoch 167 | train loss 0.4050 acc 0.8574 | val loss 0.4041 acc 0.8576\n",
      "Epoch 168 | train loss 0.4051 acc 0.8577 | val loss 0.4044 acc 0.8573\n",
      "Epoch 169 | train loss 0.4051 acc 0.8573 | val loss 0.4042 acc 0.8578\n",
      "Epoch 170 | train loss 0.4051 acc 0.8571 | val loss 0.4042 acc 0.8572\n",
      "Epoch 171 | train loss 0.4051 acc 0.8575 | val loss 0.4042 acc 0.8579\n",
      "Epoch 172 | train loss 0.4051 acc 0.8579 | val loss 0.4041 acc 0.8578\n",
      "Epoch 173 | train loss 0.4051 acc 0.8574 | val loss 0.4042 acc 0.8579\n",
      "Epoch 174 | train loss 0.4051 acc 0.8569 | val loss 0.4044 acc 0.8581\n",
      "Epoch 175 | train loss 0.4050 acc 0.8573 | val loss 0.4042 acc 0.8571\n",
      "Epoch 176 | train loss 0.4051 acc 0.8579 | val loss 0.4042 acc 0.8573\n",
      "Epoch 177 | train loss 0.4051 acc 0.8561 | val loss 0.4042 acc 0.8579\n",
      "Epoch 178 | train loss 0.4050 acc 0.8576 | val loss 0.4045 acc 0.8578\n",
      "Epoch 179 | train loss 0.4051 acc 0.8577 | val loss 0.4042 acc 0.8579\n",
      "Epoch 180 | train loss 0.4052 acc 0.8577 | val loss 0.4040 acc 0.8575\n",
      "Epoch 181 | train loss 0.4050 acc 0.8573 | val loss 0.4044 acc 0.8577\n",
      "Epoch 182 | train loss 0.4050 acc 0.8577 | val loss 0.4041 acc 0.8573\n",
      "Epoch 183 | train loss 0.4051 acc 0.8574 | val loss 0.4040 acc 0.8580\n",
      "Epoch 184 | train loss 0.4051 acc 0.8571 | val loss 0.4041 acc 0.8573\n",
      "Epoch 185 | train loss 0.4050 acc 0.8564 | val loss 0.4042 acc 0.8574\n",
      "Epoch 186 | train loss 0.4050 acc 0.8572 | val loss 0.4046 acc 0.8571\n",
      "Epoch 187 | train loss 0.4052 acc 0.8571 | val loss 0.4042 acc 0.8569\n",
      "Epoch 188 | train loss 0.4051 acc 0.8574 | val loss 0.4042 acc 0.8570\n",
      "Epoch 189 | train loss 0.4049 acc 0.8573 | val loss 0.4043 acc 0.8573\n",
      "Epoch 190 | train loss 0.4050 acc 0.8576 | val loss 0.4044 acc 0.8578\n",
      "Epoch 191 | train loss 0.4051 acc 0.8569 | val loss 0.4041 acc 0.8580\n",
      "Epoch 192 | train loss 0.4050 acc 0.8576 | val loss 0.4041 acc 0.8573\n",
      "Epoch 193 | train loss 0.4050 acc 0.8569 | val loss 0.4042 acc 0.8576\n",
      "Epoch 194 | train loss 0.4051 acc 0.8574 | val loss 0.4042 acc 0.8577\n",
      "Epoch 195 | train loss 0.4051 acc 0.8576 | val loss 0.4042 acc 0.8579\n",
      "Epoch 196 | train loss 0.4050 acc 0.8571 | val loss 0.4042 acc 0.8573\n",
      "Epoch 197 | train loss 0.4049 acc 0.8571 | val loss 0.4043 acc 0.8573\n",
      "Epoch 198 | train loss 0.4050 acc 0.8578 | val loss 0.4041 acc 0.8573\n",
      "Epoch 199 | train loss 0.4050 acc 0.8566 | val loss 0.4043 acc 0.8576\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, 200):\n",
    "    model.train()\n",
    "    total, correct, total_loss = 0, 0, 0.0\n",
    "    for xb, yb in train_loader:\n",
    "        xb, yb = xb.to(dev), yb.to(dev)\n",
    "        xb.permute(1, 0, 2)\n",
    "        opt.zero_grad(set_to_none=True)\n",
    "        logits = model(xb, rav_tensor=rav_tensor)   # logits from perp(x)\n",
    "        loss = criterion(logits, yb)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "        preds = logits.argmax(dim=1)\n",
    "        correct += (preds == yb).sum().item()\n",
    "        total += yb.size(0)\n",
    "        total_loss += loss.item() * yb.size(0)\n",
    "\n",
    "    train_loss = total_loss/total\n",
    "    train_acc = correct/total\n",
    "    val_loss, val_acc = eval_epoch(train_loader, rav_tensor)\n",
    "\n",
    "    print(f\"Epoch {epoch:02d} | \"\n",
    "          f\"train loss {train_loss:.4f} acc {train_acc:.4f} | \"\n",
    "          f\"val loss {val_loss:.4f} acc {val_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "24150e91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([48, 1, 128])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xb.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3340c74c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "weaver",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
